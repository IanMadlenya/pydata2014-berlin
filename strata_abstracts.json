{"2011": ["\nThe new data centricity drives that we have to rethink how we collect, store, manage, analyze and share our data, as all these processes  now require limitless resources. This talk will focus on the changes in infrastructure requirements to support  the new world and how innovations are removing barriers for companies to be successful.\n", "\nOur healthcare system isn\u2019t broken.  Despite the flood of statistics on rising costs and mediocre outcomes, it does what all systems do \u2013 produces precisely what it\u2019s capable of producing.   The shock of McAllen, TX \u2013 where a Medicare person\u2019s annual cost of care exceeds the average citizen\u2019s income \u2013 wasn\u2019t that it exposed a system run amok, but one that was functioning quite naturally, and similarly to every other major city in the US.\nSuch consistency, especially in the face of sometimes draconian efforts, implies that our issues are less the work of \u2018malfunction\u2019 and more the result of design; today\u2019s healthcare system simply isn\u2019t capable of producing anything fundamentally different.  If we want something different, we must not \u201cfix\u201d, we must design.  We must learn to see our problems anew, including what is missing.\nAmong its missing pieces is something all true systems have:  robust feedback loops.  Since healthcare\u2019s only systemic feedback loop is financial, we get a kind of myopic fiscal opportunism in a system with no real brakes (other than regulatory), no power of observation, and thus no built-in ability to learn from the myriad things being experienced every day.  Crossing the Institutes of Medicine\u2019s chasm means addressing healthcare\u2019s learning disability, and BIG DATA may be just what the doctor ordered.\n", "\nWindows Azure Marketplace includes data, imagery, and real-time web services from leading commercial data providers and authoritative public data sources. Customers have access to datasets such as demographic, environmental, financial, retail, weather and sports. Developers can build applications for various platforms like PC, Servers, Azure, Windows Phone, IPhone, IPAD etc using data from DataMarket. Developers can access the data as a service through an industry standard ODATA API. information workers can use the data to perform analysis using tools like Excel, PowerPivot and 3rd party applications. DataMarket also includes visualizations and analytics to enable insight on top of data.\n", "\nFor the first time, Forbes this year included information on political contributions with its Forbes 400 list of the richest people in America. Gathering this data was not easy; the Federal Election Commission publishes contribution records as they\u2019re submitted by campaigns, including typos, misspellings and attempts by campaigns and donors to obscure contributions.\nForbes overcame this difficulty by developing a data-cleaning wizard for the FEC\u2019s data that fit easily into our researchers\u2019 workflow.  We downloaded all 6 million post-2006 contribution records in the FEC database and used our wizard to help researchers find and select the identities under which billionaires donate to political organizations. After these identities are selected, the wizard automates the importing of corresponding donation records to the massive MySQL database that contains all of Forbes\u2019s data on billionaires.\nThe resulting database contains more than 20,000 contributions by 400 billionaires to 1,500 political committees. This data is largely self-updating (we import more donation records automatically every time the FEC updates its database) and will be used through the year to produce articles that examine the influence of money on politics.\nThe problem that we addressed is one faced by many data users: an important field\u2014in this case, donor name\u2014was not identified by any kind of individual ID; instead, these had to be developed by identifying combinations of fields that correspond to individuals. Our method, combining some hand identification with automated database operations, yields extremely clean data (compared, in particular, with results compiled by other sites that process FEC data) with only moderate human action.\nI\u2019ll discuss the shortcomings in the FEC\u2019s data that led us to create the data-cleaning wizard, demonstrate the functionality of the wizard and describe its mechanism, and execute sample queries of the sort that will lead to close coverage of billionaires\u2019 political activity.\n", "\nMost monitoring systems use a time series database to store historical data. RRD and traditional relational databases such as MySQL are among the most common storage backends used in popular monitoring systems such as MRTG, Cacti, Ganglia, Munin, Nagios, and Opsview. With the advent of the \u201cNoSQL\u201d movement, scalable and distributed data stores have become readily available in large clusters of commodity machines. This presentation introduces OpenTSDB, an open-source, horizontally scalable, general purpose time series database built on top of HBase. We show how its design can be used to monitor large clusters at an unprecedented level of granularity. With such a system, it becomes possible to track orders of magnitude more time series from thousands of hosts and applications, with a resolution of a few seconds to provide accurate real-time monitoring as well as long term trending.\nWhen dealing with increasingly complex distributed systems and applications,\nengineers are faced with the growing challenge of understanding the complex\nstate of the systems they run.  All modern network equipment, operating\nsystems, and applications export a wealth of metrics about their state and\ninteractions with other services.  In a large cluster, collecting, indexing\nand storing all the monitoring data becomes a daunting task due to the sheer\nvolume of information and high rate of change.  Metrics are typically\ncollected by running an agent on the hosts.  Data points are then persisted in\na chronological fashion in a time series database. Being able to plot the data\nis of utmost importance, and staying on top of the trends is critical for\ncapacity planning and performance monitoring.  Being able to correlate\ndifferent time series is tremendously helpful when trying to understand the\nbehavior of a service or conduct postmortem analyses.\nOpenTSDB is a master-less, horizontally scalable system that uses HBase to\nstore time series data.  HBase is an open-source, distributed, non-relational\ndatabase modeled after Google\u2019s Bigtable.  It features\nlow-latency, high throughput, consistent operations that are atomic at the row\nlevel, fault tolerance, and load balancing.  Thanks to those key features, it\nbecomes possible to easily store significant amounts of time series data.\nBy choosing an appropriate schema and using efficient algorithms, millions of\ndata points from arbitrary time series can be retrieved and graphed quickly.\nOpenTSDB offers a simple yet powerful query interface that allows custom\ngraphs to be generated over arbitrary time periods and with an unprecedented\ngranularity.\nOpenTSDB has been in use at StumbleUpon for almost a year and has played a key role in helping operation and engineering teams to understand the behavior and performance of our systems, troubleshoot production issues, provide significant supporting material for postmortems, do capacity planning and trend analysis.  We constantly collect many hundred metrics and hundred to thousands of data points per second.\n", "\nMyths provide explanations for underlying patterns and help make sense of the world. Big data and analytics have developed a mythology rooted in underlying assumptions. Myths can also obscure reality and confuse rather than enlighten. They impede progress and misdirect our efforts.\nWe need to ignore these myths and think clearly about how organizations use data, which means understanding how people use information and make decisions. Analytics is not a wave that washes away everything that came before. It\u2019s a wave that leaves new stuff alongside the old when the tide recedes. We need to think about how our efforts fit into that seemingly disordered larger picture.\n", "\nWe are a new era of big behavioral data. Unprecedented business model experimentation is rapidly eroding individual privacy despite rising consumer concerns. Yet, successfully managing privacy will be a key differentiator for services providers. In the business-to-business space, the stakes to get privacy right are even higher.\nFollowing recent high profile missteps by service providers, there is an increasing public awareness of online consumer products that leverage behavioral data and their threat to individual privacy. Will this rising tide of public awareness drive towards self-regulation or a stronger government response? Is this the end of the wild west of consumer product use of behavioral data?\nThe B2B space is different from the consumer space. What are the implications of privacy for service providers in the B2B space? This talk will identify and discuss the implications of privacy in order to successfully deliver information services in the B2B space.\n", "\nPossibly the biggest news at Strataconf is the launch of the $3m predictive modeling prize by the Health Provider Network (HPN) \u2013 the biggest data mining competition ever. In fact, predictive modeling competitions are shaping up to be the biggest thing in data science in 2011. On the way are other huge prizes and vital scientific projects that are being thrown open to a competition for the first time. Come and hear from Kaggle, the hosts of the HPN competition and get inside knowledge on creating and entering competitions.\nKaggle is a brand new project that hosts predictive modeling competitions, allowing companies to post their problems and have the best in the world compete to offer the best solution. Although less than a year old,  Kaggle has already used competitions to help improve the state of the art in HIV research, chess ratings and motorway travel time forecasting.\nData science competitions are becoming exciting, as companies and researchers are realizing that they are the only way to really get the most out of a dataset. Participants are finding that they can use competitions to further their knowledge and skills, test their ideas, meet the most inspiring people and develop a reputation as a world-leading performer. Kaggle is bringing these two groups together, providing new opportunities in data science.\n", "\nSocial media websites are producing ginormous amounts of data and creating a massive demand for insight related to users, how they engage with features, where they are coming from, why they are visiting, what excites them, and so forth. These are questions that, when answered in real-time, can make the difference between success and failure for an online business. Over the past few years Tagged has transformed itself into one of the most engaging sites on the internet, achieving some of the longest session times of any property.  Learn why Tagged now puts analytics at the core of its strategy and how it uses Greenplum\u2019s Data Computing Appliance (DCA) to build a platform for advanced analytics to support product development and marketing efforts that contribute to the company\u2019s long term growth.\n", "\nVirtual worlds are a goldmine of untapped behavioral data with insights that can be applied to many online social systems, as well as to the physical world.\nBut unlike the physical world (where it is obtrusive and cost-prohibitive to follow distributed users around with video cameras and sensors), virtual worlds come readily instrumented. Anything a user says or does \u2013 including how often and in what ways they interact with other users and objects in their virtual environment \u2013 can be tracked over an extended period of time.\nIn this presentation, PARC social scientists will share findings and methods (e.g., customized scripts) they developed to extract behavioral data from online games. While we will use the example of World of Warcraft, a massively popular online multiplayer game that appeals to a broad demographic (and has an average user age of 30), our data collection/analysis methods have been applied to other virtual environments as well.\nMore importantly, we will discuss how we converted and processed raw behavioral metrics into meaningful psychological variables that can be applied to a broad spectrum of business applications and segments. Other questions we will address include: What are some of the unique data collection challenges in virtual environments? What are the pitfalls and advantages of large-scale data sets, real-time data monitoring, and more? How can one extrapolate insights from low-incident events to broader samples or domains?\nMeanwhile, our other goals in this work (which is partially funded by the U.S. government) were to examine: whether behaviors in virtual worlds can be used to predict a user\u2019s demographic and personality; which cues are most predictive; and how well can these variables predict real world behaviors? The process we developed and our findings can be used to create practical, actionable tools for automated segmentation, targeted marketing, and other business intelligence.\n", "\nThis tutorial offers a basic introduction to practicing data science. We\u2019ll walk through several typical projects that range from conceptualization to acquiring data, to analyzing and visualizing it, to drawing conclusions.\n", "\nFrom overflowing email inboxes to DNA analysis, people are often budding up against massive amounts of data.  As this data streams into the lives of consumers, researchers, and big business, designers are often at a loss when it comes to making these systems manageable, not to mention usable.\nWhen dealing with ever-increasing amounts of information, the following tasks can become incredibly inefficient: content creation, information retrieval, information organization, and maintaining information.  Techniques such as distributed computing, information augmentation, and contextual awareness play a large role in building usable human/computer interfaces.  This presentation will cover these proven design techniques and more, showcasing a great number of true-life examples to demonstrate their implementation and success. Attendees will receive a variety of actionable tips and ideas they can use when they return home.\n", "\nChallenges around open data come in various angles, from licensing to provenance to change management. We focus on one in particular: how to create Web APIs for open data that encourage new creative uses of data sets and do not get in the way.\nIn this session we\u2019ll discuss key aspects of data-centric service interfaces, such as the huge power of simply applying HTTP and URLs to address parts of data sets with the appropriate granularity, the tough balance between allowing clients to ask very specific questions and ensuring that servers can still take the load, and how uniformity in data service interfaces across the industry can bring a huge opportunity to the table.\nDuring the session we\u2019ll share our various experiences designing the Open Data Protocol (odata.org), where we have many examples of ideas that worked great, of ideas that did not work at all, and of live services that have been using the protocol for a while to share data on the Web.\n", "\nGoogle is a Data business: over the past few years, many of the tools Google created to store, query, analyze, visualize its data, have been exposed to developers as services.\n\n\nThis talk will give you an overview of Google services for Data Crunchers:\n\t\nGoogle Storage for developers\nBigQuery, fast interactive queries on Terabytes of data\nMachine Learning API: Machine Learning made easy\nGoogle App Engine, exposing Data APIs is a very common use case for App Engine\nVisualization API: many cool visualization components\nServices that have not been announced as of the writing of this proposal but may be available when the conference happens:-)\n\n", "\nAs storage costs have dropped, organizations can now afford to save the vast majority of data that passes through them.  Systems like Hadoop\u2019s MapReduce permit such data to be easily analyzed and mined to improve businesses.  However classic data formats like CSV, XML and gzipped archives serve such uses poorly.  Some have weak data models.  Others support rich datastructures but are inefficient.  Most integrate poorly with MapReduce.\nApache Avro data files define an expressive, efficient standard for representing large data collections.  Avro supports rich, recursive datatypes and includes facilities for datatype evolution.  In Avro,  new datatypes may be processed and defined on the fly, useful from dynamic scripting and query languages.  Avro data is compact and fast to process.  Avro data files are compressed and MapReduce-friendly.\nThis talk will describe how Avro achieves these capabilities and how applications can start incorporating Avro data today.\n", "\nAs the world becomes more heavily instrumented, we are collecting massive amounts of raw data which often sits unused in log files or data warehouses.  At the same time, statistical techniques, cloud computing, and software frameworks have matured to a point where a small team or even a single person can rapidly extract insights and build products on top of this data.\nAs a case study, this talk will combine datasets from Twitter, LinkedIn, Wikipedia, Mechanical Turk and other sources to extract insights about the O\u2019Reilly Strata Conference and its attendees.\nDuring the process, we will walk you through the nuts & bolts of building data products using these tools.  We will cover coming up with ideas, tracking down or creating data, using visualizations during development, wiring together a prototype, and   show some algorithmic tricks that can get you out of a jam.\n", "\nBeginning in the 1960s with the explosion of post war science, new methods for understanding the relative value of a researcher\u2019s or institution\u2019s scholarly output were developed.  Significant among these was the development of the citation index by Dr. Eugene Garfield \u2013 which ties an ongoing community based valuation to a researcher\u2019s peer-reviewed published papers.  Metrics based on citation indexing now contribute to the assessment of scholarly journal quality, institutional ranking, and even an individual researcher\u2019s tenure and funding opportunities.\nWith the coming of the digital age, data-driven science is changing the research landscape.  With advances in computational methods and computer technologies, the opportunity to collect and share the information that underlies research is increasingly easy and cheap.  This trend increases both the opportunity for global collaboration and provides new avenues and challenges to the evaluation of scholarly work.  Original work needs to be identified and similar work needs to be differentiated, from among an exponentially growing set of digital objects.  There is a need to be able to computationally filter, validate, and analyze this work, while integrating the input and direct evaluation of the scientific expert community.\nWe use two projects, the Thomson Reuters Global Institutional Profile Project and the Citation Laureate Nobel Prize Predictions to illustrate the changing landscape for both discovery and evaluation.  Our discussion will include some interesting visualizations and statistics about the current evolution of scientific knowledge.  Finally, we will discuss how traditional measures are evolving to showcase research strengths, identify experts, track growth trends, and strategically manage investment funding.\n", "\nIn an ideal world, developers would be able to write one software application that can span multiple devices (i.e. smart phones, tablets, netbooks) and platforms.  This would allow developers to save hours of development time while expanding their revenue streams by repurposing one program or application for multiple devices.  By having a completely open source platform, it would be available to any OEM, device manufacturer and network operator who wished to use it.  For developers, this would create an open source ecosystem for computing devices, yet maintain the freedom for manufacturer and developer innovation.  Because an open source platform would work across different platforms, it would help decrease market fragmentation and complexity, accelerating the time-to-market for applications.  Additionally, one platform would give consumers a universal software experience for multiple devices.  During this session, Peddibhotla will discuss how open source platforms can run on multiple hardware devices including tablets, smart phones and any number of upcoming Internet-connected devices such as TVs and in-car dashboard systems.  He will also examine how this open source ecosystem can foster innovation and how to identify new market opportunities for developers and OEMs.\n", "\nAs Hal Varian famously predicted back in 2009, data scientists have seen explosive job-demand in recent years.  Even in the face of a global recession, in the last two years, the average salary for data scientists and Hadoop engineers is spiking well over $100k all around the country.\nSo, how do you build a crack team of data scientists and engineers on a shoestring budget?  At Infochimps, we\u2019ve been able to craft a team of data scientists by drawing upon smart, enthusiastic hires from our nearby university in untraditional areas such as non-linear dynamics and statistical physics, and equipping them with the process and tools that accelerate their transformation into bona fide data scientists.  Drawing from both his experiences as a teacher and his vast programming and data experience, Flip developed our methodology that embraces failure and constantly pushes people outside of their comfort zone, ultimately resulting in better, smarter scientists.\nOur unique approach to data science is well suited for our small, nimble organization.  Emphasizing program simplicity and programmer fun over runtime efficiency lets us run the minimum viable experiments that a lean startup requires.  Cloud computing and dev ops let us achieve massive horizontally scaling with minimal distraction.\nIn this 40-minute presentation from the co-founder of Infochimps, Flip Kromer will walk you through our process of recruitment and data science methodology within a fast-paced and pivotal startup. In the end, you\u2019ll learn how to make data scientists out of just about any (smart, driven) simian.\n", "\nIn this session we will talk about the instrumentation of the Reconnoiter open source monitoring systems and specifically the \u201cintelligent event processing\u201d system within it built atop the open source Esper platform.  Processing several thousand metrics per second and making sense of them all can sound like a formidable challenge, moreso when you aren\u2019t sure of the questions you\u2019d like to ask.  Esper allows for an SQL-like language to be run against live, high frequency data streams to extract complicated analytics and correlations in real-time.  I\u2019ll take you on a tour of how we apply this to high-volume global data streams and you will leave understanding what it is, how it\u2019s done and how you might use it to extract information from your data that you never thought possible.\n", "\nThis talk demonstrates how an eclectic blend of storage, analysis, and visualization techniques can be used to gain a lot of serious insight from Twitter data, but also to answer fun quesions such as \u201cWhat does Justin Bieber and the Tea Party have (and not have) in common?\u201d\n\nOverview \u2013 Why you should or shouldn\u2019t stick around for the rest of the talk\n\n\nTrends, Tweets, and Retweet Visualizations \u2013 A very quick overview of how to get your wheels spinning with Twitter data. Getting it. Analyzing it. Visualizing it.\n\n\nFriends, Followers, and Setwise Operations \u2013 A look at some of the things you can compute with friendship data and setwise operations, making the case for Redis as an data store for this type of analysis, and presenting techniques for analyzing potential influence and interests of Twitterers. This block wraps up with a look at what you can learn by computing the largest friendship clique in someone\u2019s network (and how you might stand to gain from it.)\n\n\nThe Tweet, the Whole Tweet, and Nothing but the Tweet \u2013 Focuses in on tweet data (as opposed to friendship data), and presents techniques for answering questions such as how often particular Twitters are mentioning one another, whether or not users are spammy with their hashtags, and who\u2019s getting retweeted the most often (and what this might say about trust or influence.) This block makes the case for CouchDB as an ideal data store for this type of analysis, and wraps up by juxtaposing and visualizing tweet data for #JustinBieber and #TeaParty to answer the question \u201cWhat does Justin Bieber and the Tea Party have (and not have) in common?\u201d\n\n", "\nJoin us in the Sponsor Pavilion immediately following sessions on Wednesday, February 2. Have a drink and some delectable nibbles, network with other Strata attendees, and visit our Sponsors who are at the leading edge of the data conversation.\n", "\nDetails coming soon.\n", "\nBirds of a Feather (BoF) sessions provide face to face exposure to those interested in the same projects and concepts. BoFs can be organized for individual projects or broader topics (best practices, open data, standards). BoF topics are entirely up to you.\nBoFs at Strata will happen during lunch on Wednesday, February 2 and Thursday, February 3, where lunch is served on the Mezzanine level of the hotel.\nVisit the BoF signup board near registration to claim a reserved table and schedule your BoF. Then, at lunch, look for the designated BoF area and your reserved table, marked with a table tent bearing your BoF session title.\n", "\nIn a technological age in which information saturation is prolific and the omnipresent existence of media flying exists all about us, the visualization of data to engage and effectively couple the data consumer with the data provider has increased importance and merit.  Artistic visualizations and infographics tell the stories of rich data in unique, compelling ways and synthesize datasets in ways that allow them to be interpreted, absorbed, and experienced in ways beyond the spreadsheet, pie chart, and bar graph.\nThese are not new concepts.  Edward Tufte has been advocating for data visualizations to transmit message and meaning for years.  Mr. Tufted even presents his visualization of data as museum pieces.  The Web 2.0 era brought us numerous data mashups  using real time sources such as Twitter and Geographic Information Systems to create engagement of conversational social media in creative ways.  Infographics have been translating data into rich pictures that share the story of data with mass audiences in publications such as USA Today and the New York Times.  \nAs technology continues to evolve, the tools for creating artistic visualizations have become increasingly accessible.  Tools like Adobe Flash and the Processing langauage merge visual displays of art with motion graphics to create moving pictures of Art data.\n", "\nOften overlooked in treating data as either highly structured (databases) or completely  unstructured (free text) is the fact that much useful business data is in \u201csemi-structured\u201d form: government filings, insurance claims, customer comment forms, etc. Semi-structured data are often documents broken up into free text fields. Individually, the content of these fields is highly variable, but given the context of the document, much more predictable than unstructured text. Without knowledge of that context, most search tools treat semi-structured data as free text, and are far less useful than they could be.   But a little structure goes a long way: this talk will describe how, and show how semi-structured data can be interpreted, summarized, and applied to produce business value in several real-life examples.\n", "\nI will present a few case studies in Applied Complexity using using agent-based modeling and data visualization. I will demonstrate new forms of human computer through ambient computing or what is other times known as spatial augmented reality.\n", "\nThis session will introduce OpenStack and the different components that currently make up the project. There will be a focus on Swift, the object storage project, describing how it works today and what data use cases it is appropriate for. There will also be some discussion on how the open source community is structured around it and what folks can do to get involved to help build a better cloud computing platform.\n", "\nThe \u201cinformation economy\u201d is greedy. The growth in our ability to store, retrieve and analyze data has resulted in a virtually insatiable appetite for more and better information. Everyone\u2014from the corporate CEO looking for new market strategies, to the public policy expert designing job creation programs, to the civic application developer\u2014wants more.  The advent of new technologies potentially allows far more data to be brought to bear on their questions\u2014more data can be collected, prepared quickly, distributed widely, and analyzed in extraordinarily complex ways.  One major focus has been on data available from government agencies \u2013 the Gov 2.0 movement. But is there really good data to be gotten from government agencies?  The short answer is yes, but the long answer is that I\u2019ve found that new entrants into the world of government information tend to believe more data are available than really are; or that the data are in better shape than they are; or that there is no difference between operational data, administrative data, and statistical data. To a certain extent, it is true that there is potential to access more data in new ways, but the ubiquitous presence of technology and visualization has also lulled us into a false sense of security.  It is as though technology lends the data an aura of respectability that the data don\u2019t deserve. The reality is that data emerge through a complex socio-operational context, and understanding that context is crucial for building good apps, good visualizations, and making good decisions.\nThis talk will introduce the machinations that lead to the collection of local and federal government data.  What data do government agencies collect, and why?  What IS the difference between operational, administrative and statistical?  Gov 2.0 types are interested in operational and administrative data for both local and federal government for the most part.  Legacy institutions have made it their business to understand and use data from the federal statistical system.   What\u2019s the difference, why should we care, what\u2019s it mean for civic application developers?  For example, on a Friday you\u2019ll hear that the economy lost 100,000 jobs but that the unemployment rate declined by .5%.  The former data point comes from administrative data (data collected as part of the administration of unemployment insurance), while the latter is collected through a statistical survey known as the Current Population Survey.   Knowing the difference is crucial to good decision-making.\n", "\nThis presentation is part of the Executive Summit.\nWhile the idea of supporting decision making with computerized data stretches back to the earliest days of computing, the first formally defined architecture for decision support dates to the mid-1980s (Devlin & Murphy, 1988) with the definition of the data warehouse.  The primary driver was the need to integrate data from diverse sources to provide executives and other decision makers with a \u201csingle version of the truth\u201d.  And among the first challenges it encountered was big data, at least in the context of the computing power and storage technology of the day.\nToday, over two decades later, the architectural concepts and methods continue to form the basis for modern implementations, despite radically expanded business expectations and enormous advances in the available technology.  The fundamental questions remain:\n\nHow can business leaders derive valid knowledge from electronically generated, stored or mediated information?  \nHow far can data, in all its now diverse forms and from its disparate sources, truly inform modern decision making?  \nWhat new concepts for and approaches to decision making and information usage are required to drive return on investment in mega-volumes of data?\nWhere are the real challenges in organization, management and technology?\n\nBased on almost 25 years of consulting, architectural and implementation experience, Barry will share his views on where we\u2019ve come from in information management, the lessons we can hope to learn, the implications of where we come from and an overview of a new conceptual architecture we need to create a data-driven business.\nDevlin, B. A. and Murphy, P. T., \u201cAn architecture for a business and information system,\u201d IBM Systems Journal, Volume 27, Number 1, Page 60 (1988)  http://bit.ly/EBIS1988\n", "\nThis presentation is part of the Executive Summit.\nThere has been an explosion in database technology designed to handle big data and deep analytics from both established vendors and startups. This session will provide a quick tour of the primary technology innovations and systems powering the analytic database landscape\u2014from data warehousing appliances and columnar databases to massively parallel processing and in-memory technology. The goal is to help you understand the strengths and limitations of these alternatives and how they are evolving so you can select technology that is best suited to your organization and needs.\n", "\nThis presentation is part of the Executive Summit.\nMuch of the world\u2019s most valuable information is trapped in digital sand, siloed in servers scattered around the globe. These vast expanses of data \u2013 streaming from our smart phones, DVRs, and GPS-enabled cars \u2013 can yield information to predict traffic jams, entertainment trends, even flu outbreaks.  In this talk I\u2019ll discuss the promise of big data, which will come to pass in the coming decade, driven by advances in three principle areas: sensor networks, cloud computing, and machine learning.\n", "\nAs massive data acquisition and storage becomes increasingly affordable, a wide variety of enterprises are employing statisticians to engage in sophisticated data analysis. In this talk we highlight the emerging practice of Magnetic, Agile, Deep (MAD) data analysis as a radical departure from traditional Enterprise Data Warehouses and Business Intelligence. We present architecture, techniques and experience in the field providing MAD analytics for businesses and researchers confronted with ever-expanding data sets.\nWe describe design methodologies that support the agile working style of analysts in these settings. We present data-parallel algorithms for statistical techniques, with a focus on density methods. Finally, we reflect on system features that enable agile design and flexible algorithm development, with lessons for both SQL and MapReduce programming styles.\n", "\nIntroduction to Cassandra \nProject history, background, and motivation\n * High-level description and comparison to other NoSQL systems\n\nCassandra data model \nUsing ColumnFamilies like tables\n * Using ColumnFamilies like materialized views\n * Secondary indexes\n * Insert vs update\n * Conflict resolution\n\nTwissandra: A Simple Twitter Clone \nTwissandra data model\n * pycassa-shell\n * Exercises\n\n", "\nThis presentation is part of the Executive Summit.\nThis Clay-Shirky-Meets-Will Wright-style presentation, heavy on images and brain-popping visual narratives, lays bare the dark underbelly of analytics in the enterprise. Drawing on darkly humorous experiences (both personal and from colleagues in other analytics companies), the speaker will unpack the unacknowledged blind spots of senior executives, who are hired on the basis of their leadership ability, when it comes to analytics (either from their own in-house quants or from outside vendors).\nFor a variety of reasons, a lot of high level Deciders who confidently grapple with strategic issues from technology development to partnerships and marketing, are stymied by the prospect of building analytic competence within their organizations. They know that analytics can be very powerful. But partly because of their own checkered history in math, and partly because of the black box mystique cultivated by vendors, they don\u2019t understand how analytics work.\nWhen something you don\u2019t understand is mysterious and very powerful, there are two responses. The first is to understand how the thing works, so you can evaluate its applicability. The second is to assume that because it involves arcane knowledge beyond your ken (Greek letters! Formulas! Ancient Runes!), that this very powerful thing is an occult phenomenon. Make sure the masters of it are treated with great respect, and accept what they say is revealed truth gleaned from an inaccessible plane. Analytics have the veneer of logic and rationality. But in practice, they are treated like the reading of entrails.\nI have personally been in situations where an analytics team offered to explain what we were doing and how it worked, and been told, \u201cNo, don\u2019t tell me how it works. I don\u2019t care, and besides, I probably wouldn\u2019t understand it.\u201d All we needed to do, it was explained, was find a magic nugget of truth and forecast the future. He may as well have said, \u201cI don\u2019t need to know how those throwing bones work! Just throw them and tell me the future!\u201d A colleague at another company was once told by a CEO, \u201cBuild me an analytic engine!\u201d What, she asked, did he want this analytic engine to do? Much hand-waving ensued, but no answers beyond \u201canalytic stuff!\u201d and \u201ccompetitive advantage!\u201dThere are very few analytics people who\u2019ve been in the business for any length of time who have not had this experience. We are, for all intents and purposes, witch doctors in the enterprise. And to some degree, we are invested in the mystique of that role.\nThis talk will bring executives into the Light. It will go to the core assumptions of why a company should do analytics in the first place. The talk will split out the main categories of business problems that are suited to quantitative analysis, and the process of determining which of these problems are strategic and which are grad school exercises that are intellectually interesting but will not affect the bottom line (except for the bottom line of solutions providers). The talk will also list out some important questions and litmus tests for the witch doctors \u2013 questions that prove they\u2019re in the position to deliver business value.\n", "\nThis presentation is part of the Executive Summit.\nSpeaker Marc Parrish, Vice President of Direct Marketing, Barnes & Noble will address the business challenges encountered and solutions being implemented to make an established brick-and-mortar business (Barnes & Noble bookstores) relevant as the business and its customers evolve to different products and mediums. Issues he will touch upon in a 40-minute session will include:\nHow implementation of MapReduce/SQL has driven great efficiency within the organization\n\nEfficiency gains with automation, faster analysis, and reduced efforts\n\n\nAbility to receive query responses in seconds vs. hours and days has resulted in faster marketing campaign creation and deployment to the end user/customer\n\nIntegrating customer data from multiple channels for a single, \u201csmart\u201d view and consolidating nine Oracle database silos onto a single system with advance analytic capabilities\n\nMining customer data across three separate channels \u2013 retail stores, BN.com & Nook/iPad sales \u2013 to provide a comprehensive customer profile for the most targeted outreach\n\n\nLeveraging applications within the database to provide qualitative and quantitative customer data for the most targeted offers and accurate measurement\n\nMapReduce/SQL Ecosystem \u2013 how data findings are impacting the overall business model\n\nThe effect of cafes within bookstores on AOV\n\n\nImpact of membership/loyalty program in different sales channels\n\n\nIntroduction of non-core items: i.e. toys in brick and mortar stores\n\n", "\nA number of open source databases have taken inspiration from Amazon\u2019s\nDynamo system.  Some prominent examples include Riak, Voldemort, and\nCassandra.  However, there is one essential difference that sets Riak\napart from the rest: Riak Core.  In the model realized by Riak Core,\nthe fundamental distributed systems elements have been separated from\nthe database-domain-specific elements in order to create a framework\nfor building other systems with similar properties.\nJustin will discuss:\n\nRiak KV, the database that Riak Core sprang from\nRiak Core, the engine for distributed systems\nRiak Search, the first concrete use of Riak Core after its separation from Riak KV\n\nAttendees will leave the talk with a good understanding of the\ndistributed systems fundamentals that Riak Core embodies, and how Riak\nCore enables the development of new systems with those properties.\nThis is a major shift from the historical situation where every such\nsystem had to start from scratch, which meant that only very few\nenterprises found it worthwhile to build their own robust distributed\nsystems.  With a general implementation of core capabilities, far less\nfoundational work is necessary\u2014making it practical to produce a\ncustomized, scalable, highly-available distributed system without\nenormous investment.\n", "\n\nAs part of Strata, we\u2019ll be holding a Science Fair. It\u2019s a place to demonstrate cutting-edge technologies and cool toys \u2014 the more hands-on, the better. Whether it\u2019s software that breaks the rules of computing, a compelling new interface, or a prototype that pushes the envelope, we want to see it.\nJoin us in the Mission City Ballroom Foyer, immediately following the Sponsor Pavilion Reception on Wednesday, February 2.\n\n\nHere is the lineup of projects showcasing at the Strata Science Fair. View All Science Fair project descriptions.\n\n\nA visual programming language for information processes\nAccelerating the pace of research with Mendeley\nAmbient Computing: Interactive Sandtable\nBehavioral Visualization Front End\nData mining your own DNA with SNPedia and Promethease\nDemandEstimator\nFind Your Best Airline with Data Visualization\nSoftware for exploratory analysis of systems biology data\nSynthesys On-Demand \u2013 an emerging platform for text mining in the cloud\nThe synthesis of data is where the money is\nUsing Collaborative Simulations to Predict the Future\nUsing data visualization to quickly find patterns and anomalies in large data sets\nWith medication sensors and mobile apps Asthmapolis captures information on the day-to-day experience of asthma\n\n", "\nThis tutorial begins by reviewing human perception and our ability to decode graphical information.  It continues by:\n\nRanking elementary graphical perception tasks to identify those that we do the best. \nShowing the limitations of many common graphical constructions. \nDemonstrating newer, more effective graphical forms developed on the basis of the ranking.\nProviding general principles for creating effective graphs, as well as metrics on the quality of graphs with a checklist of possible defects. \nCommenting on software packages that produce graphs. \nComparing the same data using different graph forms so the audience can see how understanding depends on the graphical construction used.\nDiscussing Trellis Display (a framework for the visualization of multivariate data) and other innovative methods for presenting more than two variables.\n\n\nPresenting Mosaic Plots and other graphical methods for categorical data.\n\nSince scales (the rulers along which we graph the data) have a profound effect on our interpretation of graphs, the section on general principles contains a detailed discussion of scales including:\n\nTo include or not to include zero?\nWhen do logarithmic scales improve clarity?\nWhat are breaks in scales and how should they be used?\nAre two scales better than one?  How can we distinguish between informative and deceptive double axes?\nCan a scale \u201chide\u201d data?  How can this be avoided?\n\nThe tutorial concludes with additional topics appropriate for the expected audience.  Possible topics include deceptive and misleading graphs, graphs that have affected history, how to decorate if appropriate, resolving conflicting advice on data displays, and special requests from the customer.\nParticipants will learn to:\n\nPresent data more effectively in documents and presentations.\nDisplay data so that their structure is more apparent.\nUnderstand principles of effective simple graphs to build upon when creating interactive or dynamic displays.\nBecome more critical and analytical when viewing graphs.\n\n", "\nData competitions come of age: from movie recommendations to life and death. Possibly the biggest news at Strataconf is Heritage Provider Network\u2019s $3 million predictive modeling prize \u2013 the biggest data mining competition ever. It requires data scientists to build algorithms that predict who will go to hospital in the next year, so that preventive action can be taken.\n", "\nMost analytics systems rely on large offline computations, which means results come in hours or days behind. Twitter is a realtime system, and it\u2019s critical that our analytics be realtime as well.  But with over 160 million users producing over 90 million tweets per day, we needed infrastructure that scaled horizontally in addition to being realtime.  In this talk we\u2019ll discuss the high-volume realtime analytics system we have built, as well as the benefits and challenges of this model over standard offline models.  We\u2019ll look at some products that we are building on top of this infrastructure, and discuss where we\u2019re hoping to take this system in the future.\n", "\nWhile the majority of charts were designed to handle a variety of data without regard for implementation, there is a certain efficiency and novelty of presenting data in a very succinct way. By designing a presentation method restricted to specific data points, we can realize an economy of space and interface. It\u2019s often practical to use the standard charts that are tried and true, but by eschewing the norms we can create visualizations that maximize interface real estate and provide a succinct view of our data.\nThere are many cases when addressing data visualization for a small space is beneficial \u2013 mobile accessibility, data journalism, and high risk/situational awareness applications to name a few. By considering the goals of the visualization and finely honing our designs, we can create highly useful interfaces that maximize comprehension and elevate judgment.\n\n\nThe following aspects of data-specific visualizations will be covered: \n\t\nTelling smaller stories with the data\nEncapsulating the design by choosing related data\nDesigning for multiple encodings\nAbstracting visual context for brevity\nWorking within constraints\nGoal-based visualization\nSmall multiples\n\nI will use personal and industry examples to demonstrate the points above. By using these methods, attendees gain insight into how to create succinct views of their data.\n", "\nOverview\nInteractive visualizations have become the new media for telling stories online. With myriad new programs for creating these visualizations, it has thankfully become common to see useful interactive data. Unfortunately, confusing visuals have also been proliferating. This session will focus on going from a good visualization to a great visualization by focusing on organization, user interface, and formatting. You should expect to leave this session confident in your ability to consistently create excellent interactive visuals.\nSub-topics\n1. Organization \u2013 It is all to easy to use charts and graphs that don\u2019t make sense for your data. In this section, we will discuss the most effective chart types for certain data, and how to arrange them for greatest effect in interactive dashboards.\n2. Interactivity \u2013 Interactivity can be the difference between a horribly confusing visualization and an all-star analysis. However, needless interactivity can take an all-star analysis and make it useless. This section will discuss applying useful and engaging interactivity to online visualizations.\n3. Formatting \u2013 Colors, fonts and borders matter. This section will discuss bringing good design principles into every visualization to optimize beauty and analytical value.\nConclusion\nMaking amazing interactive content is a complex process, but by following the best practices outlined in this session you will be able to simplify the process and improve the quality of your visualizations.\n", "\nFor more than 20 years now, data warehousing has put manners on unruly enterprise data. Yet, physics tells us that disorder inexorably increases unless we endlessly fight it. As information volumes and types explode into chaos, is it time to declare the warehouse dead? Or we could move from classical to quantum physics and create a new information architecture. It\u2019s time to make some new choices\u2026\n", "\nComing Soon\u2026\n", "\nOrganizations today possess massive data \u2013 in tera- and petabytes \u2013 that\nneeds to be effectively collected, stored and processed. Hadoop is a cost\neffective option that helps manage this big data. To derive real returns\nfrom these big data systems, one needs to extract useful insights and\nbusiness intelligence. This session will highlight: Challenges in managing\nlarge data; Solutions available for the same; Using Hadoop and Hive to\nmanage and structure data; Applying analytics on large data; Workable\napproaches for flexible, end-user-driven ad hoc analysis.\n", "\nOur talk summarizes some recent thinking in the field of data-driven vertical search and illustrates it in the context of a new version of Westlaw, called WestlawNext.  In particular, we examine the paradox that it is hard to gain user acceptance for more sophisticated search algorithms, even if the results are demonstrably better than keyword searching.  There needs to be an explanatory model of how relevant documents are retrieved and ranked that is well grounded in the actual performance of the search engine.  Our approach is based upon a relatively simple story about how our algorithms leverage the expertise of both editors and users to generate meta-data that improves search.  Improved recall depends upon editorial enhancements interpreted by natural language processing to find the long tail, while voluminous online user behavior feeds machine learning programs that improve precision in the top ranks.  We argue that getting the right allocation of function between person and machine is the key to making specialist content more findable and search results more understandable.\n", "\nA defining characteristic of modern life is the incredible proliferation of digital information. The Economist estimates that the amount of information created each year is growing at a 60% compounded rate. According to the Harvard Business Review, we humans generated more data last year than in all of previous human history. Big Data management and advanced analytic processing are highly relevant to both our personal and professional lives. Now is the time to get involved, learn how to unlock the power of Big Data and ride the crest of an incredible wave of innovation and influence on every aspect of our lives.\n", "\nBirds of a Feather (BoF) sessions provide face to face exposure to those interested in the same projects and concepts. BoFs can be organized for individual projects or broader topics (best practices, open data, standards). BoF topics are entirely up to you.\nBoFs at Strata will happen during lunch on Wednesday, February 2 and Thursday, February 3, where lunch is served on the Mezzanine level of the hotel.\nVisit the BoF signup board near registration to claim a reserved table and schedule your BoF. Then, at lunch, look for the designated BoF area and your reserved table, marked with a table tent bearing your BoF session title.\n", "\nThe tools we use play a key role in how we use and respond to big data. Hear about the changes being led by key architects of future big data systems.\n", "\nThis presentation is part of the Executive Summit.\nRetailers and their suppliers are the most aggressive civilian users of analytics and data science in the world. Their roots run deep: evolving from internal paper inventory records to mainframe EDI networks, to EDI over the internet, to RFID, to multinational collaborative forecasting, to real time inventory decisions based on weather forecasts. Retail\u2019s exponential data growth, its past success with analytics, and the relentless competition by its suppliers for shelf space as well as its own for consumer attention, ensures that the retail industry will continue to drive much of the innovation in the data science space.\nThe introduction of what many of us now consider \u201cmainstream\u201d devices, processes, and practices\u2014such as VPNs, RFID, environmental sensors, location tracking, real time logistics and supply chain management\u2014were driven by the needs of the retail sector. Why is retail at the forefront of the data capture, management, and analysis movement? The answer is quite simple. Retail is one of the most data rich industries with some of the smallest margins.\nNo retailer or supplier can survive without being able to track, analyze, and predict the effects of the 4 Ps\u2014Price, Product, Place, and Promotion\u2014across their entire business.  Now, with new issues such as track and trace, brand protection, cold chain management, and stricter government regulations across the globe, their ability to execute on the 4Ps becomes far more complicated. In retail, data science excellence is a pre-requisite for survival.\nThe scope of this problem is large and growing at an exponential rate: a domestic mid-size manufacturer will produce on average 655 billion data points a year. Consider how much data must be tracked by a large retailer who deals with thousands of manufacturers. It\u2019s easy to understand why Wal-Mart is broadly considered to have the largest civilian data warehouse in the world.  And with the addition of loyalty data and robust e-commerce channels, the amount of data and associated challenges continues to grow.\nThe retail industry, with its extensive use of data, has also been at the forefront of dealing with new challenges, such as the myriad of cultural and privacy issues that accompanies any accumulation of data this large.  It also continues to struggle with the issue of collaboration and sharing of data: when does it make sense and when is it a threat to overall business? Moving forward, these challenges need to be addressed.\nWhere is retail heading? There are a number of trends coming into play:\n\nBarriers to widespread adoption are disappearing. More cost effective and ROI conscious technology solutions, a laser-like focus on analytics as both a science and a business enabler, as well as the increasing availability of analytics expertise ensures an even more aggressive data science approach by companies of all sizes. Well known giants like Wal-Mart will no longer be the only data science \u201cplayers\u201d in the retail sector.\n\n\nNew approaches make all data more valuable. More data and data sources are driving advances in real time track and trace, forecasting, in store behavior tracking, and RFID \u201cusefulness.\u201d\n\n\nMobile devices will cause a major upheaval for traditional aggregators like AC Neilsen. These devices will reshape how we all \u201cdo our jobs\u201d and the disintermediation effect of this technology will impact traditional data aggregators.\n\nThese trends, in turn, will change the very landscape in which retail does business. What does a more \u201cperfect market\u201d mean for retailers and manufacturers when improved logistics and more consumer information come into play?  What happens when consumers can compare prices immediately on their mobile phones and order from the cheapest source? What effect will companies like Amazon, who offer sophisticated logistics and marketing services to tiny companies which enables them to operate as their much larger counterparts, have on the retail landscape? How will being able to access the opinions of thousands in real time on a particular product or a particular store affect what consumers buy?  Will retail consolidation continue and how will data science affect the outcome?\nThe Internet has completely changed the game for brick and mortar businesses of all sizes. Advances in data science will do the same, impacting not only the retail sector but the lives of consumers everywhere.\n", "\n\nHands On Instructions\nAttendees are invited to participate in the hands-on section of this tutorial using Karmasphere software and free Amazon Web Services credits which will be distributed on a USB memory stick at the tutorial. If you\u2019re interested in this, we recommend you install the VMware VMPlayer (for Windows and Linux) or VMWare Fusion (trial or full version for Mac) on your laptop in advance.\n\nDistributed applications running on Hadoop clusters can deliver powerful insights and results from the biggest data sets ever generated. But do you have to be a rocket scientist to use it? Fortunately, the answer is no. This tutorial will explain the theory of MapReduce and how to develop big data applications in Java and higher level languages such as Pig and Hive SQL. Using practical, real-world examples such as weblog processing, analytics, and text summarization, it will cover how to prototype, debug, monitor, test and optimize big data applications for Hadoop\u2019s distributed processing platform. Attendees will get hands-on instruction and will leave with a solid understanding of how to analyze data on Hadoop clusters and practical examples they can use and build on after the tutorial.\nThe tutorial will be in 5 parts:\nPart 1 (30 mins): What you need to know about MapReduce and Hadoop\nPart 2 (45 mins): Rapid prototyping and ad hoc analytics\nPart 3 (15 mins): Real world case study\nPart 4 (60 mins): Hands on instruction with practical examples\nPart 5 (30 mins): Your questions answered\n", "\nThe ability to collect, crunch, act upon, and share huge amounts of data disrupts nearly every industry, tearing down barriers to entry and creating entirely new businesses. This panel of investors will discuss where they see the opportunities in the Big Data industry, and how they think about the value of new ventures in the space.\n", "\nAfter Kennedy, you couldn\u2019t win an election without TV. After Obama, it was social media. But tomorrow\u2019s citizen gets their information from visualizations.\nIn this panel, three acclaimed designers show how they apply visualization to big data, making complex, controversial topics easy to understand and explore.\n", "\nThe state of open data today is a real mess: it\u2019s like Yahoo! circa 1995, before search saved us from lists of lists. It\u2019s very difficult to find the data you need and be confident that it\u2019s timely and accurate. There is a growing list of companies now vying to become the key destinations for people to gather around new datasets and be excited together. What projects, partnerships and even ventures would be created if there was a marketplace for data?\nBut fundamental questions arise: What does this marketplace look like? Is it market-driven like the iTunes app store, or open to collaboration similar to GitHub? And where is the data sourced from? Finally, what options are there for real business to be transacted via this data marketplace?\nIn actuality, real-time data distribution is changing the business models behind data. Licensing entire data sets and shipping CDs is rapidly becoming a thing of the past, and real-time, unfettered access to fresh, relevant data is squarely in our future. A new class of data providers is emerging to satisfy the needs of a new class of data consumers \u2013 and, in some cases, neither the data provider nor the data consumer are who you might anticipate they would be!\nIn addition to the network-effect distribution opportunity created by a collaborative data marketplace, this session will also explore the dynamic pricing opportunities being brought about by real-time data access, and will focus on data-as-a-service (DaaS) delivery with a variety of pricing models. Data security and privacy issues will also be addressed, time permitting.\n", "\nAt 18% of GDP, the healthcare industry is one of the most important sectors of the U.S. economy, yet from an IT perspective, it has been stuck in the 1980s.  A recent influx of $18 billion dollars from a Federal stimulus package designed to help the industry go digital, as well as a $7 Billion package to help it go wireless, has sparked a gold rush across the health IT space.  Nearly all of this investment is going to create systems to collect, share, and analyze data to improve health and lower cost.\nFrom the study of epidemics, to machine learning that can improve diagnosis and long-term disease management, to the sequencing of the human genome, we\u2019re doing the math of life itself.  This panel of practitioners will show us how this explosion of data will change healthcare forever.\n", "\nJoin practitioners from a range of industries to learn how they\u2019re putting new tools and massive data sets to work. We\u2019ll hear how music, geophysics, and the legal system are all changing by putting huge, rich information into the hands of business.\n", "\n\u201cMany hands make light work\u201d, as the saying goes. That\u2019s true when thousands of people can collaborate on a data set. In this session, we\u2019ll look at collective interfaces that allow many distributed users to examine and share data with one another, and how that\u2019s changing traditional desktop visualization tools.\n", "\nCan machines help us make better decisions? In this panel, real-world practitioners from the travel, finance, and energy industry give us an inside look at how they\u2019re applying machine learning to their industries, oprimizing the use of resources and helping with decision support.\n", "\nWhether you\u2019re a new startup looking for investment, or a team at a large company who wants the green light for a new product, nothing convinces like real running code. But how do you solve the chicken-and-egg problem of filling your early prototype with real data?\nWe propose an end to Lorem Ipsum in product mockups. By identifying the right public open datasets and web APIs, coders and designers working together can create surprisingly high levels of product fidelity in their prototypes. Feasibility issues can be tested against real-world data early in the product development cycle, instead of hoping for the best post-launch.\nThis talk will present recommended techniques and good sources for data-driven prototyping, and discuss how best to integrate these practices into larger teams.\n", "\n\nSponsored by:\n\n\nOpenStack Meetup & Open Bar\nSponsored by Rackspace Hosting\n6:00 \u2013 9:00 pm\nHyatt Santa Clara Convention Center\nSecond Level Mezzanine\nJoin OpenStack contributors, users, and backers to celebrate the second release of the fastest-growing open source cloud platform, code-named Bexar.  There will be a community Meetup with speakers from 6:00 \u2013 7:00 pm, followed by an open bar from 7:00 \u2013 9:00 pm.\nAt the Meetup, Jim Curry, VP of OpenStack for Rackspace, and Jonathan Bryce, chair of the OpenStack Project Oversight Committee, will kick things off with a quick progress report and overview of key features in the Bexar release, scheduled to be available February 3.  Then we\u2019ll mix things up with a series of five-minute lightning talks from various community members.  Confirmed speakers include:\n\nJesse Andrews, co-founder and CEO, Anso Labs\nVictoria Livschitz, founder and CEO, Grid Dynamics\nAnne Gentle, Content Stacker, OpenStack\nRob Hirschfeld, Principle Cloud Solution Architect, Dell\nChiradeep Vittal, Chief Architect, Cloud.com\nCloudscaling\n\nRSVP for this event\nAbout OpenStack\nOpenStack is a collection of open source technologies delivering a massively scalable cloud operating system. Founded with source code from Rackspace and NASA, and with more than 40 corporate sponsors, OpenStack is currently developing two interrelated projects: OpenStack Compute and OpenStack Object Storage. OpenStack Compute is software to provision and manage large groups of virtual private servers, and OpenStack Object Storage is software for creating redundant, scalable object storage using clusters of servers to store terabytes or even petabytes of data.\n", "\nLearn how to speed and simplify development and deployment of analytic applications that unlock the power of Hadoop with faster, cheaper, greener performance.  Rapidly build analytic applications that maximize performance within each machine in a cluster and across the cluster to deliver astonishing performance gains.  This session explores how to get more done, faster with high-performance Map/Reduce and expand the universe of Hadoop possibilities.\n", "\nThe world\u2019s largest telecoms, social networking companies, retailers and banks rely on real-time insight from their data to move their businesses forward. \nWeb analytics drive new business offerings. Click-stream analysis and user behaviors impact product design. Fraud detection and abuse monitoring protect the business. Trending reports and data drilldown help optimize IT infrastructure performance.\nLeading businesses need real-time visibility into the activities occurring across the infrastructure which can affect service, security, employee productivity and customer retention.\nFrom customer behaviors and usage statistics to security postures and operational analytics, Splunk has the unique ability to make sense of all types of machine data, structured or unstructured, and mash it up with other business data for complete real-time visibility and operational intelligence. This session will demonstrate a new approach for analyzing your organization\u2019s data and provide a tutorial for deriving real-time insights from terabytes to petabytes of data. These simple Splunk searches can be used as a data decoder ring by anyone in your organization.\n", "\nThe world\u2019s available scientific and factual data is growing at an alarming pace, but how do we use all this information?  How do we incorporate it into our decision making process?  Wolfram|Alpha is unique on the web as a computational knowledge engine, intended to assimilate the world\u2019s factual data and use it to answer your questions. Joshua Martell, content manager and all around data wrangler for Wolfram|Alpha, will give an inside look into how Wolfram|Alpha works, what it takes to make data \u201ccomputable\u201d, understand user input, and present meaningful results.  Topics will include curation of raw data, storage and retrieval, and the unique technologies that make it all possible.\n", "\nAt the inaugural edition of Strata, we\u2019re hosting our first ever Startup Showcase. Highlighting the startup ecosystem\u2019s creativity and variety, the Showcase will give you a chance to get your company in front of a global community of leaders in the technology industry-\u2014as well as potential investors.\nOn Tuesday evening, February 1, we\u2019ll have approximately 15 data related startups demoing in one large room. If your company is chosen to participate, we\u2019ll provide you with a small table and room for two people to demo\u2014-you\u2019ll bring a laptop (or two) and a founder (or two).\nStrata attendees and a panel of judges from the investor community will have an hour to visit the demos and listen to your pitches-\u2014we\u2019ll sound a chime every five minutes or so, letting people know it\u2019s time to circulate. As they walk around, attendees will vote on their favorite demos. At the end of the hour, the judges will announce their top picks along with the audience favorite. These winning startups will then each give a pitch and have an on-stage conversation with the judges.\nThe Judges\n\nTim Guileri (Sierra Ventures)\nAlistair Croll (Strata Co-Chair, Bitcurrent)\n\nThe Winners\n\nTim: Billguard\nAlistair: Sing.ly\nPeople\u2019s Choice: Drawn to Scale\n\nParticipating Companies\n\nawe.sm\nBayes Informatics\nBillGuard\nCerrio\nChart.io\nData-Publica\nDataSift\nParstream\nQwerly\nSaaspire\nSing.ly\nDrawn to Scale\nWeatherTrends360\nWordnik\nYourSports\n\n", "\nThe massive increase in the amount of data generated from the sensors that surround us, in our cell phones, our computers, our hospitals, our power network, and the enormous pressure to produce results in real-time are beyond the capabilities of existing databases.  To meet these emerging needs, we have built a platform for web-scale OLAP on sensor network data.  In this talk, I will describe the challenges faced, the many false starts, and the approaches we ultimately took in building our storage and query infrastructure capable of real-time update and query performance over hundreds of terabytes of multidimensional data.\n", "\n90,000 items on Afghanistan, 291,000 on Iraq \u2013 and another 251,000 cables. Managing the Wikileaks release is just one of the huge data journalism projects the Guardian\u2019s data team has embarked on. This talk will look at how journalists can make sense of data, get stories out of it and our role in supplying open data to the world.\n", "\nRichard Hamming, in his well-known book Numerical Methods for Scientists and Engineers, stated, _\u201cThe purpose of computing is insight not numbers.\u201d _  That was in 1962.  Today, as the Information Age plows forward, computers touch all aspects of our lives.  Far from achieving \u201cinsight\u201d, however, there is often a feeling that we are rapidly sinking under the enormous volumes of data.  The amount of technical information in the world is increasing geometrically, currently doubling about every two years.  While advancing information technology can provide tools to attempt to alleviate this problem, it must be acknowledged that this technology is also a significant contributor to the problem itself.  The fact that there is still heated debate on the \u2018Productivity Paradox\u2019 associated with massive utilization of computing technology underscores the apparent dichotomy of the situation.\nInformation processing, long considered the sole province of the computer, is not equivalent to comprehension, real world problem solving, or accurate decision-making.  Data mining, for example, is capable of finding hidden relationships and connections in complex data sets.  However it is often unable to determine why a correlation exists or whether the correlation is related to some other, unrecognized factor.  Faster processors, larger mass storage capacity, ultra high speed bandwidths, and more sophisticated database software cannot, by themselves, the solve the business, scientific, cultural and personal\nThe difficulty and time required for individuals to comprehend and understand implications buried in massive computer-based information has become the crucial factor in effectively utilizing the information and drawing functional conclusions.  Assisting people to explore, question and understand complex information is an essential for future computational environments.  Highly interactive human-computer environments can yield insight and practical solutions to some problems far more rapidly than either can by operating independently.  The human mind can process and correlate non-linear, multi-dimensional information at rates in excess of 100 gigabytes/sec.  Synthetic environments that can successfully exploit this capability may well provide the most effective approach for accelerating human understanding of computational information.\nThe presentation will outline reasons for the current \u2018information bottleneck\u2019 and examine how new perception-based software and hardware environments can accelerate individual comprehension and understanding of complex multi-dimensional information by 2 to 3 orders of magnitude.  The effectiveness of these techniques will be illustrated by presenting real-world problem solving, from both business and science, to which they have been applied.  Enhancing a individual\u2019s ability to rapidly find and understand the critical relationships contained in complex information may be the most important computational challenge of the 21st Century.\n", "\nCrowdsourcing democratizes the data-collection process, cutting researchers\u2019 reliance on stagnant, overused datasets. Now anyone can gather data overnight, rather than waiting years. Some of the data collection may be sloppy, but it\u2019s possible to build robust quality-control mechanisms in order to standardize the results that come back from the crowd. The important thing to remember is that crowdsourcing provides channels that allow researchers, businesses, or even armchair social scientists to gather data.\nTopics for any discipline that focuses on quantitative or technical data have always depended on the datasets that were available at the time. For example, the Brown Corpus is a dataset compiled in the 1960s that has served as the basis for thousands of linguistics studies. Graduate students would center entire research plans on the availability of previously collected data. As a result, generations of papers on word disambiguation were tailored to the constraints of old data. By contrast, almost every grad student in the Stanford linguistics department is using crowdsourcing platforms for their studies.\nUsing crowdsourcing tools, it\u2019s possible to conduct experiments that replicate the World Color Survey, test Benford\u2019s Law (a.k.a., the \u201cfirst digit law\u201d), explore age and gender stereotypes, or even pose philosophical problems \u2014 in a fraction of the time that such experiments used to take, while yielding tons of data.\n", "\nIf you are a leading enterprise or web company, then two things are almost certainly true. Data is the lifeblood of your business. And you face an ever-increasing need to scale your applications and data services.\nThese realities have pushed traditional database and application server technologies to breaking point, and have led companies like Google, Amazon and Facebook to build next-generation data infrastructures designed for massive scale. Following their work, technologies like Apache Cassandra have made it possible for even the smallest startup to build for Internet scale.\nAs the commercial leader in Apache Cassandra\u2122, DataStax (formerly Riptano) is taking Cassandra to the next frontier \u2013 using Cassandra as a scalability \u2018scaffold\u2019 into which other technologies (e.g. Java applications, enterprise text search, analytical processing, and many others) can be \u2018injected\u2019. Despite the limited scaling properties of these technologies, they are infused with the elastic massive scalability and fault tolerance of Cassandra, and without requiring developers to become NoSQL experts or change their development practices.\n", "\nTo many people, Big Data means Open Data: social graphs, voting records, weather patterns, and more. But who owns data? Most of our laws were written for atoms, not bits; they\u2019re woefully out of date in an information age. When you share data, does it become more or less valuable? If someone adds to your data, is it still yours? This panel will tackle the gray area of data ownership.\n", "\nOpen access to information promises to connect citizens to their representatives, improving government transparency and helping educators transform the classroom.\nIn this real-world panel, practitioners in government and the public sector will give us a glimpse into how data and new interfaces are transforming how we teach and govern.\n", "\nToday\u2019s web analyst has moved far beyond funnels and visitors. Automated systems decide who gets what content, and language parsing tries to distill sentiment from millions of online interactions.\n", "\nData doesn\u2019t just show us the past\u2014it can help predict the future. Several new firms harvest massive amounts of open data, trying to anticipate everything the right ad placement to the next terrorist attack. In this session, we bring together the founders of these firms to discuss the technology\u2014and ethics\u2014of looking into the future.\n", "\nDoes information really want to be free? While the Internet is full of open data, there\u2019s plenty of data companies are willing to pay handsomely for\u2014particularly if it\u2019s timely and well aggregated.\nAs a result, data marketplaces are a burgeoning business. Some companies crawl the web, structuring information so it\u2019s easy to use; others re-sell proprietary feeds; still others act as a clearing house for data.\nThis panel discussion will look at the role of data marketplaces in information-based business, comparing the value of ready-made data sets to the cost of collecting information yourself.\n", "\nDiscover how the industrial revolution in data will affect your business. Learn about the new opportunities and challenges that big data and analytics provide, hear from successful data-driven businesses, and plan for the impact on your organization\u2019s infrastructure and personnel.\n", "\nAlistair Croll and Edd Dumbill welcome you back to Strata.\n", "\nEdd Dumbill and Alistair Croll welcome you to Strata.\n", "\nStrata looks at three things: Big, open data; ubiquitous computing and the democratization of IT; and new interfaces that make complex, multidimensional data accessible and usable.\nPut those three things together, and you\u2019ve created an entirely new way of thinking. No longer do we learn by rote; we phone a friend. Compelling infographics have taken the place of political discourse. And any device that can send an SMS is tied to the most powerful computers in the world.\nHow will this change us? Will it make us shallow and distracted, as Nicholas Carr suggests? Will it prepare us for a truly connected society, where antiquated ideas like broadcast marketing and representative democracies give way to one-to-one buying and direct voting?\nThe change is slow, but inexorable. A few short years ago, we were warned not to put our real names on the Web. Today, every site has a share button, and we freely track our own activities for all to see. Each of those acts leaves a crumb of data; take away our smartphones, and we feel like we\u2019ve had a digital stroke, leaving us without faculties we otherwise take for granted.\nIt\u2019s maybe an understatement to say that connected access to the sum of all knowledge changes us. It\u2019ll shape how we work and play. It\u2019ll change how we learn. It may even alter who\u2014and what\u2014we love.\nIn this panel discussion, we\u2019ll look beyond the bytes and algorithms to think about humanity awash in a sea of information.\n", "\nWhether in financial services, legal investigation, or government intelligence, understanding the social network fabric is critical to making better, faster and more actionable decisions. These networks are typically determined by static user-defined criteria such as \u201clike\u201d, \u201cknows\u201d, \u201cconnection request\u201d, however, these critical connections are far more dynamic that these user-defined entries.  Furthermore, the true \u201cSocial Network\u201d can be among people, places, organizations and many other categories.\nIn his presentation, Tim Estes will discuss how the Entity Oriented Analytics in Synthesys dynamically discovers, refines and presents these and many other critical connections derived directly from the data \u2013 especially when the source is very large scale unstructured data.  The ability to discover and present these connections, classifications and rankings dynamically is critical to expanding our understanding of underlying facts and reducing the reading burden of overwhelmed data analysts.\nEntity Oriented Analytics is uniquely is positioned to verify relationship assumptions to meet today\u2019s real-time big data demands.\n", "\nThe amount of digital data available online has been exploding in recent years: Users generate content on blogs and micro-blogs, shopping sites make product reviews and detailed descriptions available. With such amounts of data at their fingertips software developers are more than ever in need for a scalable, easy to use framework for extracting knowledge from the data. Apache Mahout offers scalable implementations of algorithms for data mining and machine learning.\nScalable here means \u201cscalable community\u201d as in the project is based on a sustainable community. The number of possible use cases is scalable in that the library is available under a commercially friendly license . Of course scalable also means scalable in terms of amount of data to process: Apache Mahout is easy to start with but scales to increasing data volumn due to its use of Apache Hadoop.\nAfter motivating the need for machine learning the talk gives an overview of Apache Mahout including a deep dive to one of its algorithms. It shows the tremendous improvements that have been implemented in recent past \u2013 including the addition of several algorithms, performance improvements. Last but not least Apache Mahout graduated to a top level project this year.\n", "\nMore than ever before, organizations must chose to spend their money and time on the right software initiatives.  With exploding volumes of critical data, getting new insight and mastery over business operations demands new investments in BI at multiple levels (people, process, and software.)  So where are you going to find the extra money needed to do it?   For big and fast impact, stop paying exorbitant fees on the underlying database software and put it to better use in areas like BI where you can realize a stronger ROI.   We will show you the proven path for how to shift the spend and budget allocation away from your rich (and getting much richer!) database vendors and move it back to your P&L and new mission critical BI initiatives.\nHistorically, the most impactful economic shifts in IT have happened around re-platforming. We saw it when the IT masses moved from the mainframe into client server architectures and again when it went from big iron UNIX to X86 commodity hardware.  Those shifts paved the way for Linux and other open source software to become mainstream in corporate datacenters around the world.   Today, the move to cloud infrastructure and the need to handle big data have created the perfect catalysts for organizations to introduce new infrastructure software and break ties from their expensive incumbent vendors. Linux supplanted Unix and gave control and budget dollars back to IT and the same has happened with middleware (Apache, JBoss, Spring.) The biggest category of infrastructure spend, your relational database, is now undergoing a massive transformation.  In this session, Ed draws on experiences and case studies from EnterpriseDB and Red Hat and will share a detailed strategy on how to leverage open source database solutions like PostgreSQL to contain database cost and free budget for other, more valuable initiatives.   Learn how global organizations such as Sony, the FAA and NTT have significantly reduced their Oracle database costs across a range of applications, including: New LOB applications, Replication for Reporting and BI, and Migration of both Mission Critical and Non-Mission Critical Applications.\n", "\nMore than ever before, organizations must choose to spend their money and time on the right software initiatives.  With exploding volumes of critical data, getting new insight and mastery over business operations demands new investments in BI at multiple levels (people, process, and software.)  So where are you going to find the extra money needed to do it?   For big and fast impact, stop paying exorbitant fees on the underlying database software and put it to better use in areas like BI where you can realize a stronger ROI.   We will show you the proven path for how to shift the spend and budget allocation away from your rich (and getting much richer!) database vendors and move it back to your P&L and new mission critical BI initiatives.\n", "\nDiscover how the industrial revolution in data will affect your business. Learn about the new opportunities and challenges that big data and analytics provide, hear from successful data-driven businesses, and plan for the impact on your organization\u2019s infrastructure and personnel.\nRegister now for the Executive Summit, or the Executive Summit plus the full O\u2019Reilly Strata Conference (February 2-3).\nSchedule\n9:00am \u2013 9:45amIntroduction: Mining the Tar Sands of Big Data\nMichael Driscoll\nMuch of the world\u2019s most valuable information is trapped in digital sand, siloed in servers scattered around the globe. In this talk I\u2019ll discuss the promise of big data, which will come to pass in the coming decade, driven by advances in three principle areas: sensor networks, cloud computing, and machine learning.\n9:45am \u2013 10:30am The Data-driven Business and Other Lessons from History\nBarry Devlin\nWith Big Data comes Big Promises. Mine the blogsphere and discover the secret of eternal wealth. Feast on the Twitter feeds for the wisdom of the ages. We have visited this land in the past, naming it data warehousing and business intelligence. Will we learn the lessons of history? Can we do it differently today? Let\u2019s take this present moment to review the past and imagine the future.\n10:30am \u2013 11:00am Break\n11:00am \u2013 11:45am Building the Data-Driven Organization\nBob Page\nThere\u2019s never just one way to do things, but for big bets like Big Data, it helps to learn about paths that others have taken. In this presentation, Bob Page, VP Data & Analytics Platforms for eBay, gives a \u201cbehinds the scenes\u201d look at the systems and procedures in that power decision-making at the world\u2019s largest online marketplace.\n11:45am \u2013 12:30pm Determine the Right Analytic Database: A Survey of Data Technologies and Products\nMark Madsen\nThere has been an explosion in database technology designed to handle big data and deep analytics from both established vendors and startups. This session will provide a quick tour of the primary technology innovations and systems powering the analytic database landscape\u2014from data warehousing appliances and columnar databases to massively parallel processing and in-memory technology. The goal is to help you understand the strengths and limitations of these alternatives and how they are evolving so you can select technology that is best suited to your organization and needs.\n12:30pm \u2013 1:30pm Lunch\n1:30pm \u2013 2:15pm  Retail: Lessons Learned from the First Data-Driven Business and Future Directions\nMarilyn Craig\nRetailers and their suppliers have always operated on the cutting edge of data science. In fact, this industry is responsible for many of the technology advances that have contributed to the exponential growth of data, analytics, and related technology. This session covers the history of data science in retail, current trends, and explores future directions in the \u201cbig\u201d data age.\n2:15pm \u2013 3:00pm Barnes & Noble: Big Data Analytics Across \u201cBricks\u201d and \u201cClicks\u201d for Driving Competitive Advantage\nMark Parrish\nThis presentation will focus on how businesses can maximize big data analytics for deeper customer insights.\n3:00pm \u2013 3:30pm Break\n3:30pm \u2013 4:15pmAnalytics and Witch-Doctoring: Why Executives Succumb to the Black Box Mentality\nJ.C. Hertz\nThis presentation lays bare the dark underbelly of analytics in the enterprise. Drawing on darkly humorous experiences, the speaker will explain why executives treat analytics as an occult phenomenon. The talk will give executives the mental tools to separate strategically valuable analytics projects from fishing expeditions, and provide litmus tests to keep the witch doctors honest.\n4:15pm \u2013 5:00pm With Big Data Comes Big Responsibility\nJohn Fritz\nBig Data and predictive analytics can deliver incredible insight that can be used for purposes both good, and not so good. Drawing on real world examples, this session will examine the fine line between competitive advantage and bad behavior, and implications to a complex cast of stakeholders. Let\u2019s begin a dialog on ethics now instead of waiting for our first major crisis.\n7:30pm \u2013 8:30pmStartup Showcase\nRegister Now for Strata 2010\n", "\nHadoop and HBase make it easy to store terabytes of data, but how do you scale your search mechanism to sift through these mountains of bits and retrieve large result sets in a matter of milliseconds?\nThe Solr search server, based on Lucene, provides a scalable querying capability that nicely complements HBase.  In this session, we\u2019ll use OpenLogic\u2019s production Solr and Hadoop environment as a case study on how you can handle rapid fire queries against terabytes of data, primarily through a combination of index sharding and fault-tolerant load balancing.\nWe\u2019ll also learn how to avoid pitfalls when initially loading your data, pros and cons of putting Big Data in a public cloud, and tips on how to make your implementation go more smoothly.\nCome to this session to learn how you can put scalable, high-performance search to work on your own Big Data.\n", "\nThis tutorial offers a basic introduction to practicing data science.\nWe\u2019ll walk through several typical projects that range from\nconceptualization to acquiring data, to analyzing and visualizing it,\nto drawing conclusions.\nWe assume familiarity with the command line and the ability to use\nlibraries and code.\nTopics covered include:\n\nData acquisition and cleaning\nBuilding practical data storage, analysis, and production systems\nVisualizing data for exploration and presentation\nLearning from data\nBuilding a data science team\nPrivacy and security issues\n\nTutorial outline\n\nIntroductions and admin \n\t\nWho we are, why we love data\nMotivations for why now is the time to learn \u201cdata science\u201d \n\n\nWorking with image data\n\t\nWe will discuss examples of how to classify and cluster images based on color values and intensities, introduce the K-nearest neighbor approach to this classification, and show visualizations of image data histograms.\nPrimary concepts\n\t\nAcquiring data from APIs\nFeature space representations\nSupervised learning: K-nearest neighbors classification\nUnsupervised learning: K-means clustering\n\n\nExamples\nVisualization\n\t\nColor intensity histograms\nk-nearest neighbor plots\n\n\n\n\nWorking with text data\n\t\nDuring this section we will cover acquiring and cleaning semi-structured e-mail and LinkedIn data (via CSV export).  We will explore both data sets at the command-line, merge the data and test some basic machine learning concepts on the data.  Throughout, various visualization techniques will be used to explore the data; particularly the social graph that arises from e-mail.\nPrimary concepts\n\t\nAcquisition and cleaning of text data\nMerges and joins\nClassification via Naive Bayes, and why k-nearest does not work for text\n\n\nData (email and LinkedIn contacts)\nVisualizations\n\t\nTime-series of emails (periodicity)\nFrequent correspondents\nHeat map for LinkedIn contacts in U.S.\nE-mail graph\nGeo->IP->Map plots\n\n\n\n\nBig Data\n\t\nData Storage\n\t\nFlat files\nRelational databases (SQL)\nOther databases (NoSQL)\n\n\nProcessing\n\t\nSerial\nParallel (Hadoop, MPI, etc)\n\n\nWhere to keep your data\n\t\nMachines you own/control\nThe cloud\n\n\nPrivacy and Security Considerations\n\t\nWhy you should care\nQuick overview of applicable laws\n\n\n\n\nData mashups\nConcluding panel discussion\n\t\nBuilding a data science team\nGeneral Q&A\n\n\nSoftware Requirements\nFor those Data Bootcamp participants that wish to follow along with the instructors there are several software tools that you will need to have pre-installed.  If you do not wish to practice during the session then it is not necessary to have these tools installed prior to bootcamp, but you will need them to replicate the methods described on your own.\n\n\tFor those running a UNIX distribution or Mac OS X all of the base tools (bash, Python, and R) are already installed, so you will only need to make sure that you have the supporting packages listed below.  For Windows users you will need to install the tools separately from binaries, which you can download at the following sites:\n- To use these command-line tools we recommend you install Cygwin to emulate a UNIX-like environment.\n- Python: http://www.python.org/download/windows/\n- R: http://cran.r-project.org/bin/windows/base/\n\nUNIX bash ###\n\nA large part of analyzing data is dealing with structured and unstructured text.  As such, there are several command-line tools that allow for \u201cquick and dirty\u201d handling of this data.  For this tutorial we will rely on the following set, which come with any UNIX-like distribution:\n- sed\n- awk\n- grep\n\nPython ###\n\nPython is a powerful high-level scripting language that is well suited for manipulating and analyzing data of all kinds.  There are a number of Python libraries for analyzing data, but for this tutorial we will focus on the following:\n- email: For parsing email data\n- Natural Language Toolkit (NLTK):  Powerful set of tools for performing natural language processing on text\n- NumPy, SciPy, matplotlib: A trio of scientific computing libraries in Python that provide data types and functions for numeric and statistical analysis, as well as visualization\n- Python Image Library (PIL): For the statistical analysis of image data\n- NetworkX: For the creation, manipulation, and study of the structure, dynamics, and functions of complex networks\nThere are a few ways to install Python packages, but we recommend either of the following.  In you Python setuptools installed you can download and install all of the above libraries with the following command:\n$ easy_instal {package_name}\nFor example, to install NetworkX simply type:\n$ easy_install networkx\nYou can also install packages from source by downloading the source files at the sites referenced above.  Simply unarchive the source code, navigate to the folder where the source code is located, and use the following command:\n$ python setup.py install\n\nR ###\n\nThe R statistical programming language has become the de facto lingua franca for statistical analysis.  There are thousands of R packages available on CRAN to perform any number of analyses.  For the purposes of this tutorial we will use the extremely powerful ggplot2 package by Hadley Wickham for data visualization.\nTo install packages in R we use the ``install.packages`` command:\n\ninstall.packages(\u201cggplot2\u201d, dependencies=TRUE)\n\nNote, ggplot2 requires several other packages, so if you are running a new R installation this may take a few minutes.\n\nAdditional Software ###\n\nDuring the tutorial there will be opportunity to visualize network relationships.  A very useful tool for visualizing networks in Gephi, which is a standalone application.  If you wish to follow along with this portion of the tutorial please download and install Gephi.\n", "\nDiscover how the industrial revolution in data will affect your business. Learn about the new opportunities and challenges that big data and analytics provide, hear from successful data-driven businesses, and plan for the impact on your organization\u2019s infrastructure and personnel.\nRegister now for the Executive Summit, or the Executive Summit plus the full O\u2019Reilly Strata Conference (February 2-3).\nSchedule\n9:00am \u2013 9:45amIntroduction: Mining the Tar Sands of Big Data\nMichael Driscoll\nMuch of the world\u2019s most valuable information is trapped in digital sand, siloed in servers scattered around the globe. In this talk I\u2019ll discuss the promise of big data, which will come to pass in the coming decade, driven by advances in three principle areas: sensor networks, cloud computing, and machine learning.\n9:45am \u2013 10:30am The Data-driven Business and Other Lessons from History\nBarry Devlin\nWith Big Data comes Big Promises. Mine the blogsphere and discover the secret of eternal wealth. Feast on the Twitter feeds for the wisdom of the ages. We have visited this land in the past, naming it data warehousing and business intelligence. Will we learn the lessons of history? Can we do it differently today? Let\u2019s take this present moment to review the past and imagine the future.\n10:30am \u2013 11:00am Break\n11:00am \u2013 11:45am Building the Data-Driven Organization\nBob Page\nThere\u2019s never just one way to do things, but for big bets like Big Data, it helps to learn about paths that others have taken. In this presentation, Bob Page, VP Data & Analytics Platforms for eBay, gives a \u201cbehinds the scenes\u201d look at the systems and procedures in that power decision-making at the world\u2019s largest online marketplace.\n11:45am \u2013 12:30pm Determine the Right Analytic Database: A Survey of Data Technologies and Products\nMark Madsen\nThere has been an explosion in database technology designed to handle big data and deep analytics from both established vendors and startups. This session will provide a quick tour of the primary technology innovations and systems powering the analytic database landscape\u2014from data warehousing appliances and columnar databases to massively parallel processing and in-memory technology. The goal is to help you understand the strengths and limitations of these alternatives and how they are evolving so you can select technology that is best suited to your organization and needs.\n12:30pm \u2013 1:30pm Lunch\n1:30pm \u2013 2:15pm  Retail: Lessons Learned from the First Data-Driven Business and Future Directions\nMarilyn Craig\nRetailers and their suppliers have always operated on the cutting edge of data science. In fact, this industry is responsible for many of the technology advances that have contributed to the exponential growth of data, analytics, and related technology. This session covers the history of data science in retail, current trends, and explores future directions in the \u201cbig\u201d data age.\n2:15pm \u2013 3:00pm Barnes & Noble: Big Data Analytics Across \u201cBricks\u201d and \u201cClicks\u201d for Driving Competitive Advantage\nMark Parrish\nThis presentation will focus on how businesses can maximize big data analytics for deeper customer insights.\n3:00pm \u2013 3:30pm Break\n3:30pm \u2013 4:15pmAnalytics and Witch-Doctoring: Why Executives Succumb to the Black Box Mentality\nJ.C. Hertz\nThis presentation lays bare the dark underbelly of analytics in the enterprise. Drawing on darkly humorous experiences, the speaker will explain why executives treat analytics as an occult phenomenon. The talk will give executives the mental tools to separate strategically valuable analytics projects from fishing expeditions, and provide litmus tests to keep the witch doctors honest.\n4:15pm \u2013 5:00pm With Big Data Comes Big Responsibility\nJohn Fritz\nBig Data and predictive analytics can deliver incredible insight that can be used for purposes both good, and not so good. Drawing on real world examples, this session will examine the fine line between competitive advantage and bad behavior, and implications to a complex cast of stakeholders. Let\u2019s begin a dialog on ethics now instead of waiting for our first major crisis.\n7:30pm \u2013 8:30pmStartup Showcase\nRegister Now for Strata 2010\n", "\nThis presentation outlines several new academic developments in large data that you haven\u2019t heard of yet but that have immediate applications in industry. We discuss industry applications, like search, question-answering, and distributed computing, that could be improved immensely using these techniques.\nThese techniques include:\n\ndeep learning + semantic hashing\ngraphlab, a new parallelism abstraction\nunsupervised semantic parsing\n\nI will discuss each techniques for about five to eight minutes each.\nSemantic hashing (Salakhutdinov + Hinton, 2007)\nKeyword search and its varients, like that done by Google, can easily scale to billions of documents, but can often miss relevant results.\nWhat if your search is missing relevant results, because simple\nkeyword matching misses documents that don\u2019t contain that exact\nkeywords? This issue is especially acute for short text, like tweets. Tweets about the MTV music awards, for example, rarely contain the term VMA or the hash tag #vma. But wouldn\u2019t it be useful to retrieve all relevant results?\nSemantic hashing allows you to do search just as fast as\nkeyword matching, but it does semantic search and find relevant documents that don\u2019t necessarily contain the search keywords. It also is completely automatic, and doesn\u2019t require ontologies or other human\nannotation. And it can scale to billions of documents, like keyword\nsearch.\nGraphlab, a new parallelism abstraction (Low et al, 2010)\nThere are two ways to achieve significant improvements in predictive analytics and ML tasks like recommendation, sentiment analysis, credit risk assessment, financial forecasting, etc: You can throw more data at the problem or you can use more sophisticated learning algorithms.\nMapReduce, and its implementation Hadoop, have been highly successful at promoting distributed computing. MapReduce is good for single-iteration and embarassingly parallel distributed tasks like feature processing, which means that a lot more data can be processed. However, Map-Reduce is too high-level to implement sophisticated learning algorithms.\nWhat kind of gains could you see if you could have the best of both worlds? Large data AND sophisticated learning algorithms? GraphLab might offer those gains.\nGraphLab is only slightly lower-level than MapReduce, but significantly more powerful. It is good for iterative algorithms with computational dependencies or complex asynchronous schedules, and has been tested on a variety of sophisticated machine learning algorithms.\nSource code is available that implements GraphLab.\nUnsupervised Semantic Parsing (Poon + Domingos, 2009+2010)\nA lot of work has gone into building natural language search engines, and question-answering systems. However, these works have only been moderately successful. In particular, previous approaches (like that of Powerset and Wolfram Alpha) have required sophisticated linguistic expertise, and extensive ontology and knowledge-base construction. Essentially, there have been a lot of human engineering in the loop, and these techniques still don\u2019t work so well.\nUnsupervised semantic parsing is a highly ambitious and successful technique that attacks the problem of reading text and understanding its meaning. It requires no human annotation, and just learns by reading text. It has been applied to question-answering and is far more successful that competing academic baselines. By combining this automatic technique with current human-engineered tricks, one could significantly improve deployed NL search and question-answering systems.\nSource code is available that implements this technique.\nConclusion and question period\nI conclude by summarizing the techniques and the applications that they address. During the question period, I will specific solicit audience questions about more technical applications and problems, that are important to them. I will note more academic developments that are relevant to these audience questions, which didn\u2019t make it into the main talk.\n", "\nHadoop is responsible for computing a varying array of data\nproducts at LinkedIn, including People You May Know (LinkedIn\u2019s\npeople recommendation service), People Who Viewed This Also\nViewed (LinkedIn\u2019s collaborative filtering), Who\u2019s Viewed My\nProfile?, Career Center, LinkedIn\u2019s job recommendations, and\nmore. These products are immensely successful and extremely data\nintensive: People You May Know, for example, generates a\nsignificant portion of the invitations on LinkedIn, churning\nthrough over 50 TB of data every day.\nIn this talk, I will detail the pieces of infrastructure that\nallow us to make this happen (all open sourced), which will allow\nan attendee to build their own data products. I will also give\ntips & tricks that we have learned, sometimes painfully, along\nthe way. This talk is geared towards the intermediate Hadoop user\nwho perhaps has a few jobs that compute some data, but wants to\nlearn how to put this into a productionized process. There will\nalso be some nuggets for advanced users on how LinkedIn deals\nwith big data.\nThe talk will be subdivided into 4 \u201cproverbs,\u201d as follows.\n\n\u201cTall oaks grow from little acorns.\u201d I will explain Azkaban,\nour Hadoop scheduler, which takes individual jobs and constructs\nthem into flows that can be scheduled, restarted, and monitored.\n\u201cDon\u2019t put the cart before the horse.\u201d Once data is computed in\nbatch, we need to serve and update this data at scale. At\nLinkedIn, we use Voldemort, which serves the computed data on our\nwebsite. A common pattern is to have data re-computed in Hadoop\nand pushed periodically, but the computed data to be read-only.\nThe \u201cread-only\u201d extensions of Voldemort allow building stores\noffline in Hadoop and serving read-only traffic with low latency\nand high throughput.\n\u201cA stitch in time saves nine.\u201d The talk will also touch on how\nwe are able to quickly iterate on our data models, and push new\nmodels to production. I\u2019ll also discuss the necessities of data\nverification, and of being careful with how your data pipeline is\nconstructed. I will present examples of how we perfunctorily\nconstructed some jobs to our later detriment.\n\u201cHalf a loaf is better than none.\u201d Now we have this process,\nhow do we make it faster? The common performance bottleneck, and\nthe one faced by most of LinkedIn\u2019s data products, is\nintermediate data I/O. I will discuss the various measures we\nemploy to deal with this, such as using Bloom filters for inexact\njoins, normalization of large keys, ``the curse of the last\nreducer\u2019\u2019, increasing map locality, etc.\n\n", "\nData science is evolving rapidly. I\u2019ll talk about our current and slightly future technical and philosophical challenges, including realtime vs non-realtime analysis, streams of data vs traditional databases, and some of the opportunities we have to learn amazing things about the world through our data and what this means for those of us who are immersed in working with it.\n", "\nThis presentation is part of the Executive Summit.\nWhen are data marts justified?  How do you use your data warehouse for more than BI?  Should your data org be centralized or decentralized?  How does your warehouse and Hadoop fit together?  When is it appropriate to leverage open source? How do you combine behavioral and transactional data?  How do you create a culture of analytics \u2013 discovery, sharing, and ultimately action?  These are common concerns \u2013 questions eBay has grappled with and answered. In this presentation, Bob Page, VP Data & Analytics Platforms for eBay, will walk through the \u201cbehind the scenes\u201d decisions and outcomes in use at the world\u2019s largest online marketplace.\n", "\nPart 1 \u2013 Visualization Principles and Best Practices Overview\nPeople think visually. However, most of the tools that today\u2019s information workers use actually confuse our natural visual interpretation skills. In the first part of our tutorial, we will share practical guidance that makes data presentation more effective, engaging, and valuable. Some of the topics we\u2019ll be discussing include information layout and workflow, information visualization, chart selection, and styling. We\u2019ll show real-life examples to demonstrate how applying these best practices enables charting laymen to work like visualization ninjas.\nPart 2 \u2013 Design Principles of Dashboards and Interactive Applications\nThe second part of the tutorial will focus on best practices for applying visualization principles when designing web-based dashboards and other analytical tools. We will describe how to gradually reveal information as users express interest, how to make the tough decisions about what information should be shown, and offer guidance on creating workflows that support users\u2019 needs. Information-rich applications share many common features\u2014we will discuss when to include these features and proven approaches for making them valuable for your users.\nPart 3 \u2013 Practical Application: Case Study\nThe final section of the tutorial will bring together the lessons on design to show how they are applied in a real-world example. We will demonstrate the process for translating client requirements into a full interface design. We will discuss topics including: 1) understanding specific end-user needs and data requirements; 2) determining key features and functionality for the interface; 3) styling choices to create the final UI composites.\n", "\nThis presentation is part of the Executive Summit.\nNothing is as alluring to a scientist as \u201cdiscovery\u201d \u2013 the thrill of looking at something that nobody has seen before; of deciphering a seemingly unsolvable problem.  Likewise, nothing is as seductive to a business person as the promise of untold profit and the ability to knee cap one\u2019s competition.  And governments\u2026please, don\u2019t get me started.\nThe Big Data revolution promises myriad exciting discoveries about the behavior of genus Homo, unlocking individual and group dynamics that can offer incredible customer service, product development and social advances. But let\u2019s face it \u2013 the point of understanding human behavior, at least for business and governments, is to intervene in order to encourage, stop, or redirect behavior.  The temptation to manipulate behavior will be immense, and it will be fueled by hyper competitive markets.\nMost all of us have a point where the squirm factor will kick in and make us think \u201cYeah I can do this, but should I\u201d?  Drawing from experiences from an earlier stage of predictive analytics, we challenge the audiences \u201csquirm factor\u201d, and begin a dialog on how data scientists and business  people can begin developing an ethical framework to accompany the revolution.\n"], "2012": ["\nOrganizations today are generating data at an ever-increasing velocity but how can\nthey leverage this Big Data? In this session, Expedia, one of the world\u2019s leading online\ntravel companies, describes how they tapped into their massive machine data to\ndeliver unprecedented insights across key IT and business areas \u2013 from ad metrics and\nrisk analysis, to capacity planning, security, and availability analysis.By using Splunk to\nharness their data, Expedia saved tens of millions of dollars andfreed up key resources\nwho now can focus on innovation instead of just operations.\n", "\nHadoop is not an island.  To deliver a complete Big Data solution, a data pipeline needs to be developed that incorporates and orchestrates many diverse technologies.\nA Hadoop focused data pipeline not only needs to coordinate the running of multiple Hadoop jobs (MapReduce, Hive, or Pig), but also encompass real-time data acquisition and the analysis of reduced data sets extracted into relational/NoSQL databases or dedicated analytical engines.\nUsing an example of real-time weblog processing, in this session we will demonstrate how the open source Spring Batch and Spring Integration projects can be used to build manageable and robust pipeline solutions around Hadoop.\n", "\nBig data science and cloud computing is changing how engineering driven companies develop highly complex products.  Utilizing a novel cloud platform based on hadoop, big data analytics, and applied mathematics tools, the traditional product development cycle can be drastically sped up and used to provide new unique insights into highly complex products improving their final designs.  Data science on the cloud can be utilized as a platform to collaborate between disciplinary silo\u2019s within engineering organizations providing new opportunities for applications of advanced machine learning and optimization tools.  These tools are demonstrating drastic improvements in aerospace, automotive, and other high-tech industries.\nAn airplane wing case study will be shown to illustrate the ideas and methods presented.  The case study will show how complex engineering disciplines such as aerodynamics and structural analysis can be simultaneously run on the cloud and coupled to not only increase the speed of product development but also used to develop better final product designs.  Several tools described in the case study will be shown through a live demonstration.\n", "\nWhat does it really take to build a data product?  Recall and relevancy are only parts of the challenge.  In fact, an entire new approach is required to build consistently great data products.  This includes new paradigms of design, web development, engineering, and testing that allow a team to prototype for 1x, build for 10x, and engineer for 100x.  I\u2019ll explain the Data Jujitsu approach which is an agile approach that supports all these scales.\n", "\n\nAttendees:\nThis is a hands-on tutorial. Please download the slides and exercise materials (28MB).  It will also contain instructions for installing Hive so you\u2019ll be ready to go Tuesday morning. If you have any problems or questions, send email to training AT thinkbiganalytics DOT com beforehand. \nIn this hands-on tutorial, you\u2019ll learn how to install and use Hive for Hadoop-based data warehousing. You\u2019ll also learn some tricks of the trade and how to handle known issues.\n\nUsing the Hive Tutorial Tools\n\nWe\u2019ll email instructions to you before the tutorial so you can come prepared with the necessary tools installed and ready to go. This prior preparation will let us use the whole tutorial time to learn Hive\u2019s query language and other important topics. At the beginning of the tutorial we\u2019ll show you how to use these tools.\n\nWriting Hive Queries\n\nWe\u2019ll spend most of the tutorial using a series of hands-on exercises with actual Hive queries, so you can learn by doing. We\u2019ll go over all the main features of Hive\u2019s query language, HiveQL, and how Hive works with data in Hadoop.\n\nAdvanced Techniques\n\nHive is very flexible about the formats of data files, the \u201cschema\u201d of records and so forth. We\u2019ll discuss options for customizing these and other aspects of your Hive and data cluster setup. We\u2019ll briefly examine how you can write Java user defined functions (UDFs) and other plugins that extend Hive for data formats that aren\u2019t supported natively.\n\nHive in the Hadoop Ecosystem\n\nWe\u2019ll conclude with a discussion of Hive\u2019s place in the Hadoop ecosystem, such as how it compares to other available tools. We\u2019ll discuss installation and configuration issues that ensure the best performance and ease of use in a real production cluster. In particular, we\u2019ll discuss how to create Hive\u2019s separate \u201cmetadata\u201d store in a traditional relational database, such as MySQL. We\u2019ll offer tips on data formats and layouts that improve performance in various scenarios.\n", "\nMobile devices are ideal data capture and presentation points. They offer boundless opportunities for data collection and the presentation of temporally- and spatially-relevant data. The most compelling mobile applications will require aggregation, analysis and transformation of data from many devices and users. But intermittent network connectivity and constrained processing, storage, bandwidth and battery resources present significant obstacles. Highlighted with real-world applications, this session will cover challenges and approaches to device data collection; device-device and device-cloud data synchronization; and cloud-based data aggregation, analysis and transformation.\n", "\nThe era of big geodata has arrived. Federal transparency initiatives have spawned millions of rows of data, state and local programs engage developers and wonks with APIs, contests and data galore. Sub meter imagery ensures unparalleled accuracy and collection efforts mean timely updates. Private industry offers attribute-laden device exhaust, forming a geo-footprint of who is going where, when, how and (maybe) for what.\nWith opportunity comes challenge\u2014the expertise in sourcing, identifying, collecting, normalizing and maintaining geographic data is often overlooked in the mad rush to analyze. Curation, or the human side of extract, transform and load (ETL) has increased in scope, scale and importance as data proliferation translates to a deluge of non-standardized data types, lacking sufficient documentation or validation, questioning underlying value. Big Data calls for expertise in curating. Acquiring, validating and arranging data in collections that are relevant to the right audience at the right time.\nThe CEO of Urban Mapping, Ian White, will demonstrate why your maps are only as good as your data, the issues around big data curating and illustrate how data acquisition can be addressed from the get-go of any geospatial intelligence project or program planning\n", "\nIn a research environment, under the current operating system, most data and figures collected or generated during your work is lost, intentionally tossed aside or classified as \u201cjunk\u201d, or at worst trapped in silos or locked behind embargo periods. This stifles and limits scientific research at its core, making it much more difficult to validate experiments, reproduce experiments or even stumble upon new breakthroughs that may be buried in your null results.\nChanging this reality not only takes the right tools and technology to store, sift and publish data, but also a shift in the way we think of and value data as a scientific contribution in the research process. In the digital age, we\u2019re not bound by the physical limitations of analog medium such as the traditional scientific journal or research paper, nor should our data be locked into understandings based off that medium.\nThis session will look at the socio-cultural context of data science in the research environment, specifically at the importance of publishing negative results through tools like FigShare \u2013 an open data project that fosters data publication, not only for supplementary information tied to publication, but all of the back end information needed to reproduce and validate the work, as well as the negative results. We\u2019ll hear about the broader cultural shift needed in how we incentivise better practices in the lab and how companies like Digital Science are working to use technology to push those levers to address the social issue. The session will also include a look at the real-world implications in clinical research and medicine from Ben Goldacre, an epidemiologist who has been looking at not only the ethical consequences but issues in efficacy and validation.\n", "\nThe challenge of unstructured data is a top priority for organizations that are looking for ways to search, sort, analyze and extract knowledge from masses of documents they store and create daily. Text mining uses knowledge-driven algorithms to make sense of documents in a similar way a person would do by reading them. Lately, text mining and analytics tools became available via APIs, meaning that organizations can take immediate advantage these tools. We discuss three examples of how such APIs were utilized to solve key business challenges.\nMost organizations dream of paperless office, but still generate and receive millions of print documents. Digitizing these documents and intelligently sharing them is a universal enterprise challenge. Major scanning providers offer solutions that analyze scanned and OCR\u2019d documents and then store detected information in document management systems. This works well with pre-defined forms, but human interaction is required when scanning unstructured text. We describe a prototype build for the legal vertical that scans stacks of paper documents and on the fly categorizes and generates meaningful metadata.\nIn the area of forensics, intelligence and security, manual monitoring of masses of unstructured data is not feasible.  The ability of automatically identify people\u2019s names, addresses, credit card and bank account numbers and other entities is the key. We will briefly describe a case study of how a major international financial institution is taking advantage of text mining APIs in order to comply with a recent legislation act.\nIn healthcare, although Electronic Health Records (EHRs) have been increasingly becoming available over the past two decades, patient confidentiality and privacy concerns have been acting as obstacles from utilizing the incredibly valuable information they contain to further medical research. Several approaches have been reported in assigning unique encrypted identifiers to patients\u2019 ID but each comes with drawbacks. For a number of medical studies consistent uniform ID mapping is not necessary and automated text sanitization can serve as a solution. We will demonstrate how sanitization has practical use in a medical study.\n", "\nDuring the last 12 months, Apache Hadoop has received an enormous amount of attention for its ability to transform the way organizations capitalize on their data in a cost effective manner. The technology has evolved to a point where organizations of all sizes and industries are testing its power as a potential solution to their own data management challenges.\nHowever, there are still technology and knowledge gaps hindering adoption of Apache Hadoop as an enterprise standard.  Among these gaps are the complexity of the system, the lack of technical content that exists to assist with its usage, and that it requires intensive developer and data scientist skills to be used properly.  With virtually every Fortune 500 company constructing their Hadoop strategy today, many in the IT community are wondering what the future of Hadoop will look like.\nIn this session, Hortonworks CEO Eric Baldeschwieler will look at the current state of Apache Hadoop, how the ecosystem is evolving by working together to close the existing technological  and knowledge gaps, and present a roadmap for the future of the project.\n", "\nTools for attacking big data problems originated at consumer internet companies, but the number and variety of big data problems have spread across industries and around the world. I\u2019ll present a brief summary of some of the critical social and business problems that we\u2019re attacking with the open source Apache Hadoop platform.\n", "\nData Scientists deal with a complex world of Big Data \u2013 increasing volume, velocity and variety of data \u2013 demanding an evolution in the solutions for analytics.   Analytics today are not just about statics but really understanding the meaning of content regardless of the source or the structure. This is even more the case with unstructured data. While unstructured data has been a major issue in the area of Intelligence and National Security, its now a mainstream problem with the overwhelming amount of information that users and business most face every day from Social Media and Online content. We can\u2019t just search or count anymore- it is vital to create and make sense of the valuable interconnections of entities and relationships that are key to our daily decisions. Tim will introduce Automated Understanding for Big Data and explain how this new evolution is the fundamental step in the next wave of software.  Tim will show the power of this new capability on a large and valuable dataset that has never been deeply understood by software before.\nThis session is sponsored by Digital Reasoning\n", "\nMeasuring productivity remains a notoriously difficult problem, nowhere more so perhaps than in innovation. Feedback on the progress of projects and the performance of workers is scant, highly uncertain, and collected either too infrequently or too slowly. Yet such information is indispensable to the efficient allocation of resources to innovation projects. These challenges are all the more acute for companies involved in complex product development, where performance hinges critically on an organization\u2019s capacity to constantly and consistently innovate. At the same time, information captured by enterprises has generally gone from scarce to superabundant, affording them an unprecedented opportunity to monitor information flows, observe worker interactions and organizational structures, and estimate individual and organizational performance.\nWe will discuss how companies are using data to obtain sharper, more timely insights. Specifically, we will present how real-time information about engineering collaborations are being leveraged to measure, model, and ultimately forecast organizational productivity and project performance with a level of accuracy and timeliness heretofore impossible. Over the past couple of years, QuantumBlack has developed and deployed an analytics tool to help companies in a variety of industries, from aerospace and automotive to software and semiconductor manufacturing, improve the yield of their project investments. The software tracks and analyses real-time communication and collaboration data, as well as data on performance metrics related to tasks and projects under assessment, to forecast organizational productivity, predict the success or failure of projects, identify performance bottlenecks and drivers, and ultimately help optimize resource and work allocation strategies.\nThe talk will center on case studies involving successful deployments at several Formula One (F1) teams. We will show how we were able to forecast the productivity of innovation teams, improve investment yields by as much as 15%, and raise productivity by nearly 20%. Certainly, this is no free lunch and we will dwell on some of the more important difficulties: the technological and computing challenges associated with machine-learning and real-time analysis of a transient data set that can grow at the rate of several terabytes per day, some of the privacy issues associated with trawling employee communications even if by machine-only readers, and finally some of the cultural and management challenges that we and our clients faced in deploying a capability that forecasts individual and organizational performance. By the same token, there is a great deal that enterprises can do to help build and facilitate the adoption of analytical capabilities within their ranks. After all, and as we will show, the returns certainly warrant the effort.\n", "\nSo you\u2019ve hoarded the world\u2019s data within your enterprise. Now what? Author and digital marketing evangelist Avinash Kaushik shares lessons from the nascent world of Web Analytics on how multiplicity, scale and outsourcing powers a data democracy, and how that in turn drives business action.\n", "\nSo few of the old marketing rules still apply, it\u2019s almost comical. Gone are the 18-month implementations, Big Bang marketing campaigns, fastidious adherence to 100% data quality, and the avoidance of failure. Today\u2019s analyst lives in a world of infinite data, available instantly. That means they\u2019ve learned vital lessons about thinking smart and moving fast\u2014or rather, thinking fast and moving smartly.\nAuthor and digital marketing evangelist Avinash Kaushik shares his perspective, drawing from experience with some of the world\u2019s largest online marketers, and looks at how an analyst mentality is quickly permeating all aspects of business and marketing.\n", "\nOpening remarks by Program Chair, Alistair Croll, Founder, Bitcurrent\n", "\nThere is a lot of talk today about the power of data. Data Science and Big Data related articles seem to appear almost weekly in places like the New York Times, the Economist and the Harvard Business Review.  To be clear, this is not just a new technology trend, but a business trend as well.  Big Data, Hadoop, Agile Analytics and a new class of data driven applications hold the promise of unlocking incredible economic value and smart CEOs are moving quickly to take advantage of this opportunity.\nSo what are the fundamental skills that a CEO needs to become \u201cData Driven\u201d? In this session we will discuss the 3 essential skills that will enable CEOs to effectively lead their organizations into the Data Revolution. These organizations will harness the power of data to innovate, grow profits and beat the competition. If you are a CEO, or work for one, you will not want to miss this session.\n", "\nThe founder and CEO of CrowdControl Software, Max Yankelevich is going to explore new ways to solve big data problems involving crowdsourcing.\nHe will define crowdsourcing and the common barriers to applying it to Big Data. Everyone knows managing the crowd can be a nightmare given the complexity involved and the quality issues that arise.  Many companies focus on the quantity of data when often times it\u2019s the quality that really matters.  Through years of research at MIT and in collaboration with Amazon Mechanical Turk, Yankelevich has created an artificial intelligence application that maximizes the quality of crowdsourced work at scale.\nHe will cover specifc company use cases for collecting, controlling, correcting and enriching data.  From startups to Fortune 500 companies, this new methodology is transforming data driven businesses.  Its applications range from human sentiment analysis to keeping business listings up to date.\nThis session is sponsored by CrowdControl Software\n", "\nWhy data can tell us only so much about food, flavor, and our preferences.\n", "\r\n\t\t  The effect of big data on all business models cannot be denied. This panel of SCM experts looks at how business are using, or should be using, big data to drive supply chain management issues focusing on the broader manufacturing issues that must be addressed as well as practical tips that can be applied in dealing with supply chains that now span the globe and include multiple suppliers, distribution channels, and production environments. Added to that, technology has made the capturing of data from anywhere and anything simple, while mature and new SCM solutions are leveraging that data to inform decisions at every point in the supply chain.\nEach panelist brings a unique perspective to the discussion and all have been at the forefront of the big data explosion, addressing the opportunities and challenges of big data on supply chain processes and operations. Join us, as the panelists consider:\n\t\nHow manufacturing has evolved over the past few decades.\nWhether big data is a disruptive force that has changed the rules for successful SCM.\nHow big data can and should be leveraged by companies to address key SCM problems.\n\n", "\nLearn first hand from award-winning Guardian journalists how they mix data, journalism and visualization to break and tell compelling stories: all at newsroom speeds.\n", "\nData visualization is often where people realize the real value in underlying data. Good data visualization tools are therefore vital for many data projects to reach their full potential.\nMany companies have realized this and are looking for the best solutions to address their data visualization needs. There is plenty of tools to choose from, but even for relatively simple charting, many have found themselves with limited options. As the requirements pile up, options become limited: Cross-browser compatibility, server-side rendering, iOS support, interactivity, full control of branding, look and feel \u2026 and you\u2019ll find yourself compromising, or \u2013 worse yet \u2013 building your own visualization library!\nBuilding our data publishing platform \u2013 DataMarket.com \u2013 we\u2019ve certainly been faced with the aforementioned challenges. In this session we\u2019ll share our findings and approach for others to avoid our mistakes and learn from our \u2013 sometimes hard \u2013 lessons learned.\nWe\u2019ll also share what we see the future of online data visualization holding: the technologies we\u2019re betting on and how things will become easier, visualizations more effective, code easier to maintain and applications more user friendly as these technologies mature and develop.\n", "\nDemands for real-time analytics to derive information, patterns, and revenue from Big Data have left legacy DBMS technologies in the dust.  Two foundational technologies have proven to be critical to handle today\u2019s data scale problems \u2013 1) the tremendous parallelism delivered by today\u2019s multi-core/distributed server, and 2) column storage to solve the I/O bottleneck when analyzing large data sets.\nIn this session, Jim Tommaney will provide an overview of column store databases, the benefits and where companies are implementing them in their organizations.  He will also discuss specifics of the InfiniDB Map Reduce style distribution framework and how it helps provide linear scalability for SQL operations.  Together, these have tremendous synergies to provide companies a new level of performance to attack big data analytics in a simplistic and scalable manner.\nThis session is sponsored by Calpont Corporation\n", "\nBusiness and operational management of data content has become a top priority for the most vital American financial institutions. From big banks to mortgage lenders, there is an effort currently sweeping the industry to overhaul and \u2013 just as importantly \u2013 align data standards in a way that is efficient and understandable nationwide. For the first time, the financial industry is being transparent regarding data quality, both internally and externally. Firms are developing a 360 degree view of issues such as customer data, loan life cycle and security data, allowing business partners to develop stronger products, maintain better customer relationships and have a deeper knowledge of overall risk position. Reduction in overall data interfaces and databases as well as the complexity of the overall environment opens the door for tens of millions in cost savings for financial firms embracing data management as an agent of change. Simply put, improved data management standards and techniques are mitigating risk for financial firms and increasing stability for an industry that is obviously critical to our nation\u2019s economical structure.\n", "\nBeautiful, useful and scalable techniques for analysing and displaying spatial information are key to unlocking important trends in geospatial and geotemporal data. Recent developments in HTML 5 enable rendering of complex visualisations within the browser, facilitating fast, dynamic user interfaces built around web maps. Client-side visualization allows developers to forgo expensive server-side tasks to render visualisations. These new interfaces have enabled a new class of application, empowering any user to explore large, enterprise-scale spatial data without requiring specialised geographic information technology software. This session will examine existing enterprise-scale, server-side visualization technologies and demonstrate how cutting edge technologies can supplement and replace them while enable additional capabilities.\n", "\nPretty Simple Data Privacy isn\u2019t a company or a project. Rather, it is the idea that we\u2019ve made personal data privacy too complicated and granular. Rather than get deeper and deeper into algorithmic approaches, we should be providing users a very simple set of choices about their data and a easy interface to mark their data as usable, off limits, or negotiable.\nMost privacy choices come down to Yes, No, and Maybe. Many users are willing to let their personal data be used in a research context, or if they get something back in return for their data. Many want the right to say no if they don\u2019t like or understand the terms. And many are willing to negotiate for the vast majority of questions that fall in between. PSDP ties together privacy and policy issues explored in existing projects to standardize informed consent, create iconic representations of privacy policies, and move towards a world where users manage the way their vendors use their data.\nThe session specifically builds on the experience of the Consent to Research project in personalized genomics, quantified self, and other personally identifiable data projects.\n", "\nSocial data is growing, Twitter produces 250+ million tweets per day and 27 million links to news and media. Big Data can give insights into these large datasets but first the data must be curated, cleaned and quantified before it has value. We will cover how we move from unstructured to structured and how we take simple data and apply complex processes to give context to the data.\nWe will cover how we developed a platform that can deal with billions of items per day and perform complex analysis before handing the data onto thousands of customers in real-time. We will also walk through our platform architecture looking at our use of Hadoop, HBase, 0MQ, Kafka and many other cutting edge technologies. You will learn some of the pitfalls of running a production Hadoop cluster and the value when you make it work.\n", "\nData isn\u2019t just for supporting decisions and creating actionable interfaces.  Data can create nuance, giving new understandings that lead to further questioning\u2014rather than just actionable decisions.  In particular, curiosity, and creative thinking can be driven by combining different data sets and techniques to develop a narrative around a set of data sets that tells the story of a place\u2014the emotions, history, and change embedded in the experience of the place.\nIn this session, we\u2019ll see how far we can go in exploring one street in San Francisco, Haight Street, and see how well we can understand it\u2019s geography, ebbs and flows, and behavior by combining as many data sources as possible.  We\u2019ll integrate basic public data from the city, street and mapping data from Open Street Maps, real estate and rental listings data, data from social services like Foursquare, Yelp and Instagram, and analyze photographs of streets from mapping services to create a holistic view of one street and see what we can understand from this. We\u2019ll show how you can summarize this data numerically, textually, and visually, using a number of simple techniques.\nWe\u2019ll cover how traditional data analysis tools like R and NumPy can be combined with tools more often associated with robotics like OpenCV (computer-vision) to create a more complete data set.  We\u2019ll also cover how traditional data visualization techniques can be combined with mapping and augmented reality to present a more complete picture of any place, including Haight Street.\n", "\nThe Orbitz family of travel sites receives hundreds of thousands of searches each day from consumers looking for hotels.  A single consumer initiated request may spawn dozens of individual hotel rate look-ups resulting in many millions of such requests per day.  Most hotel inventory is managed by suppliers, often in more antiquated systems not capable of handling a large number of requests.  In order to minimize the impact of high consumer traffic volume on these suppliers, Orbitz caches rate information locally.  Such caching also helps Orbitz maintain look-to-book ratios with their suppliers and reduces latency experienced by the consumer.\nHotel rate and availability information can change over time causing cached information to go stale.  This is not desired since it causes consumers to experience discrepancies between cached and real-time rates.  Therefore, each piece of information stored in the cache is given a time-to-live (TTL).  Historically, the TTL values have been determined by business intuition and have not been generated via a data-driven approach.\nHere we investigate the applicability of predictive modelling to optimize TTL values for rates in our hotel rate cache.  Specifically, we examine survival analysis as a means of modelling hotel rate volatility.  Survival analysis is a statistical technique which models the time until the occurrence of a particular event.  In the context of biological organisms, the event of interest is often the death of the organism (hence the name).  In our context, the event of interest is a change in the rate offered by a hotel or in its availability status.\nWe highlight some of the technical challenges in collecting nearly a billion records each day. This includes how we use MongoDB as a collection mechanism for real-time events emitted by our hotel applications before being transferred to Hadoop for long term storage and processing.  We\u2019ll also cover how the data is prepared in Hadoop prior to being made available for use in building and evaluating our predictive models.\nFinally, we show how our results have both challenged some and confirmed other of our long-held assumptions on rate volatility and how we\u2019re using these results to improve our look-to-book and cache hit ratios while reducing rate discrepancies experienced by consumers.\n", "\nUsing Hadoop based business intelligence analytics, this session looks at the Hadoop source code and its development over time and illustrates some interesting and fun facts we will share with the audience.  This talk will illustrate text and related analytics with Hadoop on Hadoop to reveal the true hidden secrets of the elephant.\nThis entertaining session highlights the value of data correlation across multiple datasets and the visualization of those correlations to reveal hidden data relationships.\n", "\nSince the early days of the data deluge, Lift Lab has been helping many actors of the \u2018smart city\u2019 in transforming the accumulation of network data (e.g. cellular network activity, aggregated credit card transactions, real-time traffic information, user-generated content) into products or services. Due to their innovative and transversal incline, our projects generally involve a wide variety of professionals from physicist and engineers to lawyers, decision makers and strategists.\nOur innovation methods embark these different stakeholders with fast prototyped tools that promote the processing, recompilation, interpretation, and reinterpretation of insights. For instance, our experience shows that the multiple perspectives extracted from the use of exploratory data visualizations is crucial to quickly answer some basic questions and provoke many better ones. Moreover, the ability to quickly sketch an interactive system or dashboard is a way to develop a common language amongst varied and different stakeholders. It allows them to focus on tangible opportunities of product or service that are hidden within their data. In this form of rapid visual business intelligence, an analysis and its visualization are not the results, but rather the supporting elements of a co-creation process to extract value from data.\nWe will exemplify our methods with tools that help engage a wide spectrum of professionals to the innovation path in data science. These tools are based on a flexible data platform and visual programming environment that permit to go beyond the limited design possibilities industry standards. Additionally they reduce the prototyping time necessary to sketch interactive visualizations that allow the different stakeholder of an organization to take an active part in the design of services or products.\n", "\nEntrepreneurs and industry executives know that Big Data has the potential to transform the way business is done.  But thus far, Big Data has not lived up to the hype.  Why?  Because a lot of us are making a rookie mistake \u2013 a mistake that Panjiva CEO Josh Green made the first time he came up against Big Data \u2013 we\u2019re becoming smitten with data sets and trying to find problems that our data sets of choice can help solve.\nIn the past, entrepreneurs and executives had limited data at their disposal.  Coming across a new data set was a rare and precious moment \u2013 a bit like striking gold.  In this world, an intensive focus on a new data set made sense, because you never knew when you would come across additional data.  These instincts, honed in a world of scarce data, are downright dangerous in a world of virtually limitless data.\nWorking with data is hard.  It takes time and money \u2013 and, in today\u2019s world, there\u2019s opportunity cost associated with it.  When you\u2019re playing around with Data Set A, you\u2019re missing out on an opportunity to play with Data Set B.\nTo succeed in the Big Data world, entrepreneurs and executives need to be ruthless in prioritizing which data sets they\u2019re going to dig into \u2013 and which they\u2019re going to steer clear of.  How best to prioritize?  In the same way that businesses have always prioritized \u2013 by focusing our time, our money, and our energy on our toughest problems.\nIt all starts with the identification of a problem worth solving.  This is a decision that can be made without ever touching Big Data.  Once the problem has been identified, the hunt for data is on.  And that\u2019s where the real fun begins.  Because in today\u2019s Big Data world, the hunt is almost always successful.\nJosh will discuss his experiences working with some of the world\u2019s largest companies (Panjiva currently counts over 35 Fortune 500 companies as clients) to track down data to solve a real-world problem \u2013 and help you avoid many of the mistakes that he\u2019s made along the way.\n", "\nWho influences whom? Social Contagion, the spread of sentiments and behaviors, is the dominant force shaping human dynamics.\nBusinesses care about social contagion because they want to understand how their products can go viral. Politicians care about social contagion because the spread of hope or fear can win an election. Public health officials care about contagion because the spread of unhealthy behaviors will overwhelm our health care system.\nMeasuring social contagion, however, is hard, and presents us with considerable data science challenges. I will present our research on social contagion in the context of health behaviors, and how we address the phenomenon of social contagion with data science approaches. I will take the audience on a journey starting with mining open data from online social media services, to supervised machine learning algorithms, to data analysis using novel methods from social network statistics, all the while using only open source tools. The goal of the talk is a) to introduce the audience to the basic concepts of social contagion and b) to demonstrate a real world example of social contagion using open data science tools.\n", "\nHadoop is gaining momentum with most companies having already deployed Hadoop in some fashion or are testing it in the lab. But there are many aspects of Hadoop that are not fully understood and appreciated including \u2013 How Hadoop can easily be leveraged by non-programmers, how to use Hadoop to quickly outperform complex models, how to easily integrate Hadoop into existing environments, and the two step process to use legacy applications with Hadoop.\nDuring the session, Ted Dunning will show that while counter intuitive, as data size increases simple algorithms perform  better than complex models on small data. This can greatly simplify the deployment and development of Hadoop applications and the talk will include several examples of machine learning deployments across multiple industries.\nThis session will also cover recent developments that make Hadoop access available to rank-and -file users. This expands access with standard applications to view and manipulate data beyond programmer access.\nThis session will provide detailed descriptions of the following:\n1)    Getting data into and out of the Hadoop cluster as quickly as possible\n2)    Allowing real-time components to easily access cluster data\n3)    Using well-known and understood standard tools to access cluster data\n4)    Making Hadoop easier to use and operate\n5)    Leveraging existing code in map-reduce settings\n6)    Integrating map-reduce systems into existing analytic systems\n", "\nThis is the story about how a developer and two marketing scientists used design thinking, LEAN methods, and marketing science to generate a meaningful data driven experience for marketers.\nObstacles include unstable data sources, changing requirements, fluid markets, the Gartner Hype Cycle, changing definitions, changing terminologies, seemingly constant feature creep, and dissent.\nSolutions include continuous feedback, iterating violently, rapid design labs, aggressive inquiry, design thinking, seemingly constant feature combat and beer.\nAttendees can expect to learn how not to replicate the mistakes we made, and to make up their own minds if the solution space we discovered is suitable to them.\n", "\nDo you plan to extract insights from mountains of data, including unstructured data that is growing faster than ever?  Attend this session to learn about Microsoft\u2019s Big Data solution that unlocks insights on all your data, including structured and unstructured data of any size.  Accelerate your analytics with a Hadoop service that offers deep integration with Microsoft BI and the ability to enrich your models with publicly available data from outside your firewall. Come and see how Microsoft is broadening access to Hadoop through dramatically simplified deployment, management and programming, including full support for JavaScript.\nThis session is sponsored by Microsoft\n", "\nAs more data become less costly and technology breaks barriers to acquisition and analysis, the opportunity to deliver actionable information for civic purposes grows.  This might be termed the \u201ccommon good\u201d challenge for Big Data. But actionable data has always been the challenge for nonprofits and civic organizations.  The needs haven\u2019t changed on the civic side\u2014 data intermediary organizations live this every day.  Our community and civic clients have struggled with obtaining data\u2014from the public realm or from their own systems\u2014that will inform their decision-making, help tell their stories to funders, mobilize support, and direct their efforts.\nThis presentation will draw from experiences, old and new, deployed by common-good data intermediaries in order to spotlight challenges moving ahead.  We\u2019ll draw on experiences such as that in Chicago where MCIC is endeavoring to run an Apps Competition that\u2019s centered on community/hacker collaboration.  We\u2019ll explore the history of the national neighborhood indicator movement and efforts by voters\u2019 groups to build mapping platforms in order to have a voice in political redistricting plans.  We\u2019ll also talk about new efforts such as Data without Borders and their success in bringing coders to the community.  By touching on these stories, we\u2019ll highlight the potential for disruptive approaches to break through barriers: resources, communication styles, data availability.\n", "\nPractical problem solving with data involves more than just visualization or applying the latest machine learning techniques.  Intuition, domain knowledge, and reasonable approximations can mean the difference between a successful model and a catastrophic failure.  We\u2019ll dive into some best practices I\u2019ve extracted from solving real world problems like computing trending topics, finding related searches, cleaning election data, and ranking experts on social networks.\nNew analysts or engineers are often lost when textbook approaches fail on real world data. Drawing inspiration from problem solving techniques in mathematics and physics, we will walk through examples that illustrate how come up with creative solutions and solve problems with big data.\n\nCreating Models\nSampling & Approximation\nFinding Edge Cases\nTesting Extremes\nWorking Backwards\nJoining to External Data & Crowdsourcing\nTurning Errors into Improvements\n\n", "\nJumpstart looks at how building and running businesses changes in a data-driven world. It\u2019s the missing MBA for Big Data.\nWe live in a world of data, feedback, and realtime interaction with customers, partners, suppliers, and employees. Companies that harness information learn faster, make better decisions, and stay one step ahead of their competitors. Yet most businesses aren\u2019t equipped to put Big Data to work. Jumpstart changes that, bringing together the world\u2019s leading thinkers on data-driven management for a practical look at how information can transform the enterprise.\nAre you the kind of executive who wants to run your business based on facts rather than opinions? Do you demand hard figures? Are you frustrated by how long it takes to discover, and act on, key business metrics? Then this is the place to start.\nSchedule\n9:00am-9:20amJumpstart Welcome\nAlistair Croll\nOpening remarks by Program Chair, Alistair Croll, Founder, Bitcurrent\n9:20am-10:00amWhat Marketers Can Learn From Analysts\nAvinash Kaushik\nAuthor and digital marketing evangelist Avinash Kaushik shares his perspective, drawing from experience with some of the world\u2019s largest online marketers, and looks at how an analyst mentality is quickly permeating all aspects of business and marketing.\n10:00am-10:30amBig Data and Supply Chain Management: Evolution or Disruptive Force??\nMary Ludloff\nThe effect of big data on all business models cannot be denied. This panel of SCM experts looks at how business are using, or should be using, big data to drive supply chain management issues focusing on the broader manufacturing issues that must be addressed as well as practical tips that can be applied in dealing with supply chains that now span the globe.\n10:30am \u2013 11:00am Break\n11:00am-11:25amAmmunition for the CFO: How to be a Hard-Nosed Business Customer for Analytics\nJ. C. Herz\nThis presentation lays out some clear, concrete gating conditions for when it makes sense to pull the trigger on big data initiatives, and how they should be procured, depending on the use case, the data assets, and the resources available.\n11:25am-11:55am3 Essential Skills of a Data Driven CEO\nDiego Saenz\nWhat are the fundamental skills that a CEO needs to become \u201cData Driven\u201d? In this session we will discuss the 3 essential skills that will enable CEOs to effectively lead their organizations into the Data Revolution. These organizations will harness the power of data to innovate, grow profits and beat the competition.\n11:55am-12:30pmBusiness Intelligence: What Have We Been Missing?\nFelix Hamilton\nThere are many rapidly evolving technologies that provide objective metrics and analytics for most outward facing business interactions. The evolution of similar inward facing tools has not kept pace. In this presentation we discuss which sources of internal organizational data are frequently neglected, approaches for automating data collection, and what valuable insights can result from analysis.\n12:30pm \u2013 1:30pm Lunch\n1:30pm-2:10pmThe Business of Big Data\nMark Madsen\nMark Madsen talks about how regular businesses will eventually embrace a data-driven mindset, with some trademark \u2018Madsen\u2019 history background to put it in context. People throw around \u2018industrial revolution of data\u2019 and \u2018new oil\u2019 a lot without really thinking about what things like the scientific method, or steam power, or petrochemicals did as a result.\n2:10pm-2:30pmDo it Right \u2013 Proven Techniques for Exploiting Big Data Analytics\nBill Schmarzo\n\u201cBig data\u201d provides the opportunity to combine new, rich data sources in novel ways to discover business insights.  How do you use analytics to exploit this data so that it will yield real business value?  Learn a proven technique that ensures you identify where and how big data analytics can be successfully deployed within your organization.  Case study examples will demonstrate its use.\n2:30pm-3:00pmBig Data, Serious Games, and the Future of Work\nMichael Hugos\nIn this session, business agility expert Michael Hugos will present examples from his work in applying immersive animation techniques and gaming dynamics, and discuss how they can address the challenges of consuming \u2013 and responding to \u2013 the data deluge, turning information overload into business advantage.\n3:00pm \u2013 3:30pm Break\n3:30pm-3:50pmIt\u2019s Not Just About the Data\u2026...the Power of Driving Impact Through Intent and Interconnectedness\nMarcia Tal\nIn this session, Marcia Tal will demonstrate how significant business value is being realized through sophisticated understanding of intent and interconnectedness, at scale.\n3:50pm-4:20pmSearching for the Future of Big Data\nMarti Hearst\nSearch user interfaces are slow to change; ideas for new search interfaces rarely take hold.  This talk will forecast how search is likely to change and what will stay the same in the coming years.\n4:20pm-4:50pmWrap-up Session\nJonathan Bruner\nJon Bruner leads a panel discussion with a few of the day\u2019s presenters and takes final questions from the audience.\nRegister Now for Strata 2012\n", "\nDo you want to write less code and get more done? This tutorial will demonstrate a natural language parsing technology to extract entities from all kinds of text using massively parallel clusters. Attendees will gain hands-on experience with the newly-released, data-centric cluster programming technology from HPCC Systems to extract entities from semi-structured and free-form text data. Students will leave with all the data and code used in the class along with the latest HPCC Client Tools installation, HPCC documentation, and HPCC\u2019s VMware installation. Prizes, give-aways and a raffle is included.\nThis session is sponsored by HPCC\n", "\nReliability and scalability of your application is dependent on how its application state is managed. To run applications at massive scale requires one to operate datastores that can scale to operate seamlessly across thousands of servers and can deal with various failure modes such as server failures, datacenter failures and network partitions. The goal of Amazon DynamoDB is to eliminate this complexity and operational overhead for our customers by offering a seamlessly scalable database service. In this talk, we will talk about how developers can build applications on DynamoDB without having to deal with the complexity of operating a large scale database.\nThis session is sponsored by Amazon\n", "\nJon Bruner leads a panel discussion with a few of the day\u2019s presenters and takes final questions from the audience.\n", "\nIn this session, Marcia Tal will demonstrate how significant business value is being realized through sophisticated understanding of intent and interconnectedness, at scale.  Follow Marcia as she highlights examples of complex and diverse organization\u2019s recent successes in connecting varied data sources and utilizing emerging techniques to deliver new platforms, capabilities and products that drive large impact.\nWe are at an inflection point as business leaders reassess what is required AND who are the people that will transform operating models.  Marcia\u2019s experience and passion around talent and leadership development will provide insights as to the new roles in the C-suite required to compete in information-based business models.\nMarcia plans to share a couple of new ideas with the audience and welcomes the opportunity to gain attendees perspective over the course of the conference.\n", "\nMark Madsen talks about how regular businesses will eventually embrace a data-driven mindset, with some trademark \u2018Madsen\u2019 history background to put it in context. People throw around \u2018industrial revolution of data\u2019 and \u2018new oil\u2019 a lot without really thinking about what things like the scientific method, or steam power, or petrochemicals did as a result.\n", "\nBig data isn\u2019t just an abstract problem for corporations, financial firms, and tech companies. To your mother, a \u2018big data\u2019 problem might simply be too much email, or a lost file on her computer.\nWe need to democratize access to the tools used for understanding information by taking the hard-work out of drawing insight from excessive quantities of information. To help humans process content more efficiently and to help them capture more of their world.\nTools to effectively do this need to be visual, intuitive, and quick. This talk looks at some of the data visualization platforms that are helping to solve big data problems for normal people.\n", "\n\nAttendees:  The code to be used in this tutorial will be located in it\u2019s github repository.  The following software is required for attendees of this tutorial:\nJDK 1.6 or greater\nApache Maven 3.0.4 or greater\n\nApache Cassandra 1.0.7 or greater (Use the DataStax Community distribution)\nAn IDE (Eclipse or ItelliJ) is helpful, but not necessary\nNote: If you are not a frequent user of maven it may be helpful to go through ahead of time and install the sub-projects in the above repository in order to avoid having to download dependencies during the workshop.\n\nThe database industry has been abuzz over the past year about NoSQL databases and their applications in the realm of solutions commonly placed under the \u2018big data\u2019 heading.\nThis interest has filtered down to software development organizations who have had to scramble to make sense of terminology, concepts and patterns particularly in the areas of distributed computing which were previously limited to academics and a very small number of special case applications.\nLike all of these systems, Apache Cassandra, which has quickly emerged as a best-of-breed solution in this space in the NoSQL/Big Data space, still requires a substantial learning curve to implement correctly.\nThis tutorial will walk attendees through the fundamentals of Apache Cassandra, installing a small working cluster either locally or via a cloud provider, and practice configuring and managing this cluster with the tools provided in the open source distribution.\nAttendees will then use this cluster to design a simple Java web application as a way to gain practical, hands on experience in designing applications to take advantage of the massive performance gains and operational efficiency that can be leveraged from a correctly architected Apache Cassandra cluster.\nAttendees should leave the tutorial with hands-on knowledge of building a real, working distributed database.\n", "\nThis tutorial provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop, plus its associated ecosystem. This session is intended for those who are new to Hadoop and are seeking to understand where Hadoop is appropriate and how it fits with existing systems.\nThe agenda will include:\n\nThe rationale for Hadoop\nUnderstanding the Hadoop Distributed File System (HDFS) and MapReduce\nCommon Hadoop use cases including recommendation engines, ETL, time-series analysis and more\nHow Hadoop integrates with other systems like Relational Databases and Data Warehouses\nOverview of the other components in a typical Hadoop \u201cstack\u201d such as these Apache projects: Hive, Pig, HBase, Sqoop, Flume and Oozie\n\nNo programming experience is required for this session.\n", "\nToday, no one today argues with the fact that a company\u2019s data is its most strategic asset. If you don\u2019t use it as a competitive weapon against your market competitors, it\u2019s guaranteed that you will be at a disadvantage, because they certainly will.\nFor this panel, DataStax CEO BIlly Bosworth will moderate a discussion that will spotlight real mission critical Big Data use cases from \u201chands-on\u201d practitioners. With companies like Walmart, Netflix, & Apigee among many others adopting Apache Cassandra and other new database technologies, there\u2019s never been a more exciting time to be building data intensive applications.\nJoining us will be:\n\nBilly Bosworth, CEO, DataStax\nSTS Prasad, VP, Pricing and Infrastructure, Walmart Labs\nEd Anuff, VP, Mobile Platform at Apigee\nJeremy Edberg, Lead Cloud Reliability Engineer, Netflix.\n\nBilly will be leading a lively conversation around: What is a \u201cbig data\u201d problem?, Are traditional databases dead?, What key features of these new databases are driving adoption?, Are you innovating more with these new tools, how?, What drives their use in your organization \u2013 speed, price, flexibility?\nBring your toughest questions? Billy and the panel have seen it all!\nThis session is sponsored by DataStax\n", "\nOne of the most significant challenges faced by individuals and organizations is how to discover and collaborate with data within and across their organizations, which often stays trapped in application and organizational silos. We believe that internal data marketplaces or data hubs will emerge as a solution to this problem of how data scientists and other professionals can work together to in a friction-free manner on data inside corporations and between corporations and unleash significant value for all.\nThis session will cover this concept in two dimensions.\nPiyush from Microsoft will walk through the concept of internal data markets \u2013 an IT managed solution that allows organizations to efficiently and securely discover, publish and collaborate on data from various sub-groups within an organization, and from partners and vendors across the extended organization\nFrancis, from ScraperWiki, will talk through stories of both how people have already used data hubs, and stories which give signs of what is to come. For example \u2013 how Australian activists use collaborative web scraping to gather a national picture of planning applications, and how Nike are releasing open corporate data to create disruptive innovation.  There\u2019ll be a section where the audience can briefly tell how they use the Internet to collaborate on working with data, and ends with a challenge to use open data as a weapon.\n", "\nNetflix is known for pushing the envelope of recommendation technologies. In particular, the Netflix Prize put a focus\non using explicit user feedback to predict ratings. This kind of recommendation showed its value in the time when Netflix\u2019s\nbusiness was primarily mailing DVDs. Nowadays Netflix has moved into the streaming world and this has spurred numerous changes\nin the way people use the service.  The service is now available on dozens of devices and more than 40 countries.\nInstead of spending time deciding what to add to a DVD queue to watch later, people now access the service and watch whatever appeals to them at that moment. Also, Netflix now has richer contextual information such as the time and day when people are watching content, or the device they are using.\nIn this talk I will describe some of the ways we use implicit and contextualized information to create a personalized experience\nfor Netflix users.\n", "\nCome learn how the Mendeley team built the largest crowdsourced database of research literature, scaled to handle 120M uploaded documents, and how they\u2019re using technologies such as Hadoop, Apache Mahout and Thrift to generate daily statistics and recommendations on over 7 TB of academic research data. Jan Reichelt, Mendeley co-founder, will talk about the lessons learned in building the service and how this is shaking up the stodgy old field of academic publishing.\nIn addition to the technical story, Jan will also show how Mendeley\u2019s real-time data on content usage provides never-before-seen insight into how academics collect, read, share, and annotate academic research. Why should you care about academic publishing? It\u2019s a fascinating story\u2026 while you\u2019re using Github and Google+ to share information, the best that all the world\u2019s big brains can come up with is swapping PDFs!\nAcademic publishing is facing many of the same stressors as other kinds of publishing as their content moves online, but since academic publishing has typically derived revenue from institutional purchases as opposed to individual ones and ad sales don\u2019t contribute as much to revenues, the business models have diverged to where academic publishing has had until now very little end-user focus. Academic content is also read more intensively, curated more carefully by end users, and managed with specialized tools, which gives us a unique opportunity to look at content usage at a level of detail not possible in any other industry and distill some insights that are relevant across all of publishing.\n", "\nThis session will teach participants how to architect big data systems that leverage virtualization and platform as a service.\nWe will walk through a layered approach to building a unified analytics platform using virtualization, provisioning tools and platform as a service. We will show how virtualization can be used to simplify deployment and provisioning of Hadoop, SQL and NoSQL databases. We will describe the workload patterns of Hadoop and the infrastructure design implications. We will discuss the current and future role of PaaS to make it easy to deploy Java, SQL, R, and Python jobs against big-data sets.\nSubtopics:\n\nA layered strategy for building a unified analytic stack:\n\t\nOptions and architecture for building virtual and cloud infrastructure\nValues and problems solved using virtualization and cloud infrastructure\nPicking the right storage and network architecture in light of Hadoop and big-SQL\n*Data layer \nUsing cloud provisioning tools to simplify deployment and management of databases\n\n\nDatabases\n\t\nTaxonomy and feature comparison of datastores and databases for big-data\nQuantitive comparison of popular NoSQL stores\n\n\nPaaS and the runtime layer\n\t\nRuntime and language services for big-data analysis\nUsing PaaS to provide simple/agile access to R, SQL, Python without the headaches\n\n\nAnalytics Tools and services for data scientists and data modelers\n\t\nTools for collaboration and sharing of big-data sources\n\n\nConclusion\nFollowing this approach it is possible to build a unified analytics platform that extends into future needs. Attendees will learn a layered approach for how to build a virtualization-based architecture for multiple layers of a big data system.\n", "\nThe social ecosystem has become the pulse of the world. From delivering breaking news like the death of Osama Bin Laden before it hit mainstream media to  driving trading strategies in the hedge fund world, the real-time social web is flooded with valuable information just waiting to be analyzed and acted upon. With millions of users and billions of social activities passing through the ever-growing realtime social web each day, it is no wonder that companies need to reevaluate their traditional business models to take advantage of this valuable data. However, in today\u2019s social data economy, not all social data is created equal. While all social media data can be valuable, it is a matter of discovering the which type of social data is best suited for each specific use case.\nThrough this session, participants will discover how real-world companies are incorporating social data to everyday business operations. This session will highlight a variety of big data case studies from Financial Services, Emergency Response, Brand Analytics as well as other organizations and will explore the types of social media data that are best suited for each specific use case. Finally, participants will gain a deep understanding of how organizations in these industries are collecting and applying big social data to produce big results.\n", "\nVideographics achieve the two most important criteria of the visualizer.\nThey engage attention and they inform.\nI am currently working with the BBC to define a new format \u2013 that of the \u2018Video Dat Graphic\u2019. \nSome of these exist online to degrees of success but we are codifying best practice, auditing current activity and can show our work in the market context.\nI will discuss how video is an information rich medium \u2013 from a survey of data resolution across media and how these videos can compliment the BBC online offering as a whole.\nSome subjects to cover will be\n- storytelling principles \u2013 what actually works in 2 minutes\n- scripting and storyboarding \u2013 drafting a plan\n- timescales, costs and resources\n- designing for cognition \u2013 how video needs to understand how we perceive\nI\u2019ll be showing many examples in addition to our work.\nThis is a high paced session, with lots to look at and an excellent mix of storytelling and information design ideas. There is an excellent balance between theory and practical advice.\n", "\nIn video surveillance, hundreds of hours of video recordings are culled from multiple cameras. Within this video are hours of recordings that do not change from one minute to the next, one hours to the next and in some cases, one day to the next. Identifying information that is interesting and that can be shared, analyzed and viewed by a larger community from this video is a time-consuming task that often requires human intervention assisted by digital processing tools.\nUsing Map/Reduce we can harness parallel processing and clusters of graphical processors to identify and tag useful periods of time for faster analysis. The result is an aggregate video file that contains metadata tags that link back to the start of those scenes in the original file. In essence, creating an index into hundreds-of-thousands of hours of recording that can be reviewed, shared and analyzed by a much larger group of individuals.\nThis session will review examples where this is being done in the real world and discuss the process for developing a Hadoop process that can break a video down into scenes that are analyzed by maps to determine interest and then reduced into a single index file that contains 30 seconds of recording around that scene. Moreover, the file will contain the necessary metadata to jump back into the original at the start point and allow the viewer to view the scene in context of the entire recording.\n", "\nWhen it comes to natural language processing, general APIs and generic models are often far less accurate than you want. Or maybe the APIs you need don\u2019t even exist. Either way, you can use \u201ccorpus bootstrapping\u201d to create custom models and APIs. Corpus bootstrapping is a method of rapidly producing a custom corpus for training highly accurate natural language processing models. For example, suppose you want to do sentiment analysis for Spanish text, but you can only find APIs and models for English. Or you want to do phrase extraction for phrases that are not exactly noun phrases. Maybe you want to classify text but there\u2019s no corpus in existence with the categories you\u2019re interested in. All of these problems can be solved by iterating your way to a custom corpus for training custom models.\nThis talk will cover:\n\ncreating a classified corpus from scratch\ngenerating a sentiment analysis corpus in Spanish by starting with an English corpus\nusing simplified part-of-speech tags to quickly produce a custom corpus for phrase extraction\ntraining custom models with NLTK-Trainer\n\nCode examples will be in Python using NLTK.\n", "\nIn 2007 Leapfrog embarked on a the Learning Path project to enable their learning toys to upload play logs to Leapfrog as an aid to parents in understanding what and how their children learn from their toys. For Leapfrog this would bolster their position as the educational toy leader and innovator, create opportunities to understand customers better and provide valuable information about the use of products for product lifecycle planning.\nThis talk will present the strategy and business opportunities as they were planned, then discuss the challenges of implementation. In 2007 we looked at map reduce as a solution to potentially large data volumes but settled on Oracle RAC for reporting flexibility and product maturity. We faced demand estimation issues, SLA challenges, metadata and data management issues, data quality issues and then the killer\u2026our data collection from our users was not passive.\n", "\nThe tools of social network analysis \u2013 centrality measures, clustering, graph-traversal algorithms, community detection and so forth \u2013 are largely based on mathematical network theory. There is very little in these techniques that actually requires that the data represents social activity. This presentation will show how these techniques can be applied to data from areas such as geo, the Wikipedia link graph and linguistics.\nWe\u2019ll show how to take tabular or textual data and derive graph representations from it that can be used to apply these techniques. We\u2019ll discuss practical applications of these techniques in delivering new features for web applications. We\u2019ll also show how the powerful visualisation tool Gephi can be used to explore the data once it\u2019s in graph form.\nThis talk will be partly based on content from an Ignite talk given at Strata NYC 2011: http://slideshare.net/mattb/place-graphs-are-the-new-social-graphs\n", "\nThe expected massive growth of connected device, appliance and sensor markets in the coming years \u2013 often called \u2018The Internet of Things\u2019 \u2013 will need a more rich concept of \u2018open data\u2019 than is currently common. When data is generated through activities of people doing things inside their homes and outside in public in their cities, the question of who owns the data becomes almost irrelevant next to the questions of who has access to the data, what do they do with it, and how do citizens manage and make sense of their data while retaining the \u2018openness\u2019 that we\u2019ve seen drive creativity and business on the web over the last few years.\n", "\nHow big data tools and technologies give us back our individual identity \u2026 because if you didn\u2019t know you were unique and special, well, you are. Big data can be applied to solving socio-economic problems that rival the scale and importance of building ad optimization models.\n", "\nApache Hadoop forms the kernel of an operating system for Big Data.\nThis ecosystem of interdependent projects enables institutions to\naffordably explore ever vaster quantities of data.  The platform is\nyoung, but it is strong and vibrant, built to evolve.\n", "\nBack in the late 80s artificial intelligence was set to take over the world; it didn\u2019t happen.\nIn 2012; AI has been stripped down, dressed up and reborn as machine learning. Will it take over the world this time?\nWhat makes a Big Data \u2013 Machine Learning solution \u2018better\u2019? Can machine learning happen with legacy tools? What exactly does it mean to be fully parallel? Do I care? Will I be any better if I get it right?\nThis talk sponsored by HPCC Systems\n", "\nJoin leading data scientists in debating hot issues in the profession.\n", "\nOur education system is not preparing students for college. There is an urgent need to improve academic outcomes and equip students with critical 21st century skills. Evidence from top-performing schools shows that use of data, analysis, and feedback are our best tools for improvement. The increasing use of online software and digital devices in classrooms presents an opportunity to collect high-frequency data for mining. Today\u2019s analytics techniques could be used to develop a deeper understanding of how students learn, recommend personalized learning plans, and identify early warning flags. Rich data, analytics, and feedback enable a process of iteration and continuous improvement, where educators become learners, and we figure out how to improve education. We are at the beginning of a wave of data-driven change in education, with important social consequences and fantastic opportunities.\nThis keynote is sponsored by Shared Learning Collaborative\n", "\nSearch user interfaces are slow to change; ideas for new search interfaces rarely take hold.  This talk will forecast how search is likely to change and what will stay the same in the coming years.\n", "\nWhere are all the coffee shops in my neighborhood?\nSeemingly easy questions can become complex when you consider ambiguity. This one sounds simple until you consider that folks may define \u201ccoffee shop\u201d differently and the boundaries of your \u201cneighborhood\u201d differently. One person\u2019s Central Austin, may be someone else\u2019s South Dallas.\nHow about instead of working too hard to define the parameters in an attempt to completely remove the ambiguity, we instead look at what people do, interact with and talk about.  We can watch what people do and decide from there what a coffee shop is and where the boundaries of your neighborhood are.  It might not be the \u201ctruth\u201d, but it can be darn close.\nWhen we learn to embrace ambiguity, not only can we still find the answers to our questions, but we can also find answers to questions we hadn\u2019t even thought to ask.\n", "\nIn the 1970\u2019s, Codd and Date developed a theory of databases based on sets.  However, a similar theory based on arrays leads to a relational-like database model with significant advantages in analyzing large amounts of data.   In an array based model, the order of the data is significant \u2014 whereas in a set based model, it is not.  Hence, time series operations (like comparing a value to the value in the previous period), are complex and expensive with set-based relational databases, but simple and inexpensive with arrays.  This talk covers some of the problem domains that benefit from using arrays over sets, and illustrates the different techniques that can be used.\n", "\nCompanies are collecting data at amazing rates. Yet no matter what industry we are in, we remain largely at a loss as to why people do what they do. And so Ruth Stanat\u2019s \u201cDrowning in data, yet starved of information\u201d is still very relevant, except that it is not as much information that we are starving for, but the understanding how to use information to pick the right action and to influence people such that they behave in a way that is better for them, better for us, and possibly better for society in general.\nThe time has come to utilize big data to move from the assessment of correlation to causation. Many industries let inertia guide both the methods they use to gain business insights and the metrics they believe measure the success of their business decisions. But ultimately, every (hopefully data-driven) business decision boils down to some action. And those actions affect our customers, our employees, our clients, our competitors, and our society.\nEstablishing metrics that quantify the impact of these business decisions and measuring them is the challenge we are coming to face. And while this statement seems obvious; an examination of multibillion dollar industries reveals that in many cases companies are not choosing metrics that truly measure the degree to which business decisions are successful, let alone make optimal decisions in the data-driven anticipation of the expected impact. \nAnd this is where methods for causal analysis on observational data can lead the way.\nCausal methods allow us to measure the impact of our business decisions and even explore potential business decisions. The theory has been around for a while, and we now have in many cases the data to answer the real questions. In our case: What exactly is the impact of showing this ad to this person at this time? And is this really where you should be spending your marketing dollars? An application of causal methods to the online display advertising industry reveals that the current metrics for evaluating advertising campaign success tell a very different story about the value of our advertising choices than the story told when we actually evaluate causal impact.\nOf course, if good data analysis is already fraught with pitfalls for prediction, wait for the challenges trying to get causal analysis right. More so than ever, careful data preparation is the foundation of reliable answers. Missing one important variable can lead to entirely spurious and misleading results. It did not work for us the first time either but ultimately it is well worth the effort.\n", "\nMany options exist when choosing a framework to build a custom data explorer on top of your company\u2019s stack.  With a brief nod to out-of-the-box business intelligence solutions, the presenters will offer an overview of the creative coding frameworks that lend themselves to data visualization on and across web browsers and native apps written for Mac OS X, iOS, Windows, and Android.  Evaluation of the strengths and weaknesses of libraries such as Processing, OpenFrameworks, Cinder, Polycode, Nodebox, d3.js, PhiloGL,  Raphael.js, Protovis, and WebGL will be explored through visual examples and code.  The audience should come away with a sense of what investments into education will return a high value product that serves unique design goals.\n", "\nPersonalized Cancer Care: How to predict and monitor the response of cancer drugs in individual patients.\n1. Biology: Cancer spreads through the body by cancer cells leaving the primary site of cancer, traveling through the blood to find a new site where it can settle, colonize, expand and eventually kill the patient.\n2. Challenge: the concentration of the cancer cells is about 1 to 1 million normal white blood cells or 1 to 2 billion cells if you include the red blood cells. This makes for about a handful of these cells in a tube of blood (assuming that you have given blood before, you can picture this pretty easily). A cell is about 10 microns in diameter\n3. Opportunity: if can find these cells, we could always just take a tube of blood and characterize the disease in that patient at that point in time to make treatment decisions. We have significant numbers of drugs going through the development pipeline but no good way of making decisions about which drug to take at which time.\n4. Solution: create a large monolayer of 10 million cells, stain the cells, then image them and then find the cells computationally by an iterative process. It is a simple data driven solution to very large challenge. It is simple in the world of algorithms, HPC and cloud, and setup to revolutionize cancer care.\nhttp://4db.us and http://epicsciences.com for more info.\n", "\nToday\u2019s competitive business charts its future course by effectively mining the unstructured, interactional data generated by web logs, machine-to-machine communications, mobile devices, video, social media and sensors.  While these enterprises see an opportunity to increase revenues and decrease costs by becoming a data-driven organization, it is not easy to decide where and how to begin.\nThis session will session will look at two real-world \u201cfirst Big Data Project\u201d use cases:\n\nLeveraging web logs, social media and transactional data to gain the elusive 360 degree view of the customer  \nLeveraging Machine to Machine data to optimize Operations\n\nTopics will include:\n\nMarrying business and Big Data strategies\nSelecting your first project \nPlanning and implementing the project\nIdentifying and quantifying the business value\nLearning lessons from real world projects\n\n", "\nIn a world where data increasing 10x every 5 years and 85% of that information is coming from new data sources, how do our existing technologies to manage and analyze data stack up? This talk discusses some of the key implications that Big Data will have on our existing technology infrastructure and where do we need to go as a community and ecosystem to make the most of the opportunity that lies ahead.\nThis session is sponsored by Microsoft\n", "\nEnterprises today are well on their way to putting Big Data to work.  Many are experimenting with Big Data, if already not in production. The data deluge is forcing everyone to ask the key question \u2013 What is the cost of big data analytics? This session will address some of the key concerns in creating a Big Data solution that will provide for lower cost \u201cper TB Data Managed and Analyzed\u201d\nThe session will talk about why nobody wants to talk about the costs involved with Hadoop, NOSQL and other options.It will also exemplify how to reduce costs, choose the right technology options and address some of the unsaid issues in dealing with BIG Data.\nThis session is sponsored by Impetus Technologies\n", "\nThe biggest problem in data science is \u2026 the data itself.\nIt\u2019s messy, it\u2019s inconsistent, it arrives from myriad\nsources, and it sometimes changes without warning.  Such\nhurdles distract you from your intended purpose: getting\nmeaningful insight out of your data.\nQ Ethan McCallum, consultant and author of Parallel R\n(O\u2019Reilly), will walk through the various forms of bad data\nand explore common pitfalls that can derail your research\nefforts.  Most of all, he\u2019ll explain ways to handle bad data\nso you can get back to work.\n", "\nCurrently, organizations have access to excellent high resolution data, metrics, and automated analytics on many of their external interactions. Similar data and analytics are, however, often not available on interactions that take place inside an organization. For example, an organization using current technologies has access to detailed and objective information about what areas of their website a casual browser is focusing on in real time. The same organization generally doesn\u2019t know what documents an employee is working on in any given day, what tools they are using, or almost any other objective information about what is going on internally. As a result, many organizations rely almost exclusively on subjective opinions and self reported data to inform their decision making process regarding internal issues. Without high resolution objective data and analytics, workforce related decision making becomes more of an art than a science.\nIn this presentation we will discuss this disparity in detail, outline our approach to the problem as well as some of the lessons we have learned along the way, and talk a little about what the potential benefits associated with improving data collection and analytics inside an organization look like in the real world.\n\n\nSub Topics:\n\t\nThe importance of objective internal metrics and analytics\nUnderutilized organizational data sources\nData collection, storage, and processing guidelines\nChallenges associated with automated data analysis, alerting, and reporting\nPotential insights resulting from analysis, with several real world examples\n\nBy improving the technologies that allow an organization to look inward, it is possible to know as much about what is going on inside an organization as is known about what is happening externally. There are no fundamental technical reasons why you should know more about the process associated with a customer making a $50 purchase than you do about the product development, design, QA, marketing, and sales teams that make that purchase possible.\n", "\nOne doesn\u2019t normally think about Big Data when the rain falls, but we\u2019ve been measuring and analyzing Big Weather for years.  Due to recent advancements in Big Data, cloud computing, and network maturity it\u2019s now possible to work with extremely large weather-related data sets.\nThe Climate Corporation combines Big Data, climatology and agronomics to protect the $3 trillion global agriculture industry with automated full-season weather insurance.  Every day, The Climate Corporation utilizes 2.5 million daily weather measurements, 150 billion soil observations, and 10 trillion scenario data points to build and price their products.  At any given time, more than 50 terabytes of data is stored in their systems, the equivalent of 100,000 full-length movies or 10,000,000 music tracks.  All of this is meant to provide the intelligence and analysis necessary to reduce the risk of adverse weather on U.S. farmers, which is the cause of more than 90% of crop loss.\nThe Climate Corporation\u2019s generation system uses thousands of servers to periodically process decades of historical data and generate 10,000 weather scenarios at each location and measurement, going out several years.  This results in over 10 trillion scenario data points (e.g. an expected rainfall value at a specific place and time in the future), for use in an insurance premium pricing and risk analysis system amounting to over fifty terabytes of data in our live systems at any given time.  Weather-related data is ingested multiple times a day directly from major climate models and incorporated into The Climate Corporation\u2019s system.  Under the hood, the The Climate Corporation\u2019s  Web site is running complex algorithms against a huge dataset in real-time, returning a premium price within seconds.  The size of this data set has grown an average of 10x every year as the company adds more granular geographic data. Hear The Climate Corporation CTO Siraj Khaliq discuss how to apply big data principles to the real-world challenge of protecting people and businesses from the financial impact of adverse weather.\n", "\nMany of the new online and device-oriented application models require a high degree of operational and development agility such as unlimited elastic scale and flexible data models. The nascent NoSQL market is aiming to address these requirements but is extremely fragmented, with many competing vendors and technologies. Programming, deploying, and managing NoSQL solutions requires specialized and low-level knowledge that does not easily carry over from one vendor\u2019s product to another. The SQL market on the other hand has a high level of maturity and at least conceptual standardization, but relational database systems were not originally designed for these requirements.\nHowever, in contrast to common belief, the question of big versus small data is orthogonal to the question of SQL versus NoSQL. While the NoSQL model naturally supports extreme sharding, the fact that it does not require strong typing and normalization makes it attractive for \u201csmall\u201d data as well. On the other hand, it is possible to scale relational SQL databases.\nIn this presentation, I will provide a short introduction to some architectural patterns that SQL-based solutions have been using to achieve scale and operational agility, contrast them with the NoSQL paradigms and show how SQL can be augmented with NoSQL paradigms at the platform level by using SQL Azure Federations as an example. I will also show how NoSQL offerings can benefit from the lessons learned with SQL.\nWhat this all means is that NoSQL, BigData and SQL are not in conflict, like good and evil. Instead they are sometimes overlapping, but often complementary solutions that benefit from common paradigms addressing different requirements and can and will coexist.\n", "\r\n\t\t  NetApp is a fast growing provider of storage technology. Its devices\n\u201cphone home\u201d regularly, sending unstructured auto-support log and\nconfiguration data back to centralized data centers. This data is used\nto provide timely support, to improve sales, and to plan product\nimprovements. To allow this, data is collected, organized, and\nanalyzed. The system currently ingests 5 TB of compressed data per\nweek, which is growing 40% per year. NetApp was previously storing\nflat files on disk volumes and keeping summary data in relational\ndatabases. Today, NetApp is working with Accenture to design, build,\nand implement the enterprise transformation project for next\ngeneration auto-support, with Think Big Analytics as a partner and\nexpert in Big Data solutions. The new system uses Hadoop, HBase and\nrelated technologies to ingest, organize, transform and present\nauto-support data. This will enable business users to make decisions\nand provide timely response, and will enable automated response based\non predictive models. Key requirements include:\n\t\nQuery data in seconds within 5 minutes of event occurrence.\nExecute complex ad hoc queries to investigate issues and plan accordingly.\nBuild models to predict support issues and capacity limits to take\naction before issues arise.\nBuild models for cross-sale opportunities.\nExpose data to applications through REST interfaces\n\n\n\nIn this session we look at the lessons learned while designing and\nimplementing a system to:\n\t\nCollect 1000 messages of 20MB compressed per minute.\nStore 2 PB of incoming support events by 2015.\nProvide low latency access to support information and configuration\nchanges in HBase at scale within 5 minutes of event arrival.\nSupport complex ad hoc queries that join multiple data sets\naccessing diverse structured and unstructured large scale data sets\nOperate efficiently at scale.\nIntegrate with a data warehouse in Oracle.\n\n", "\nOver the last 12 years, S&P Capital IQ has built a successful world-class data product: the Capital IQ platform. Join two key members of the Data Team as they tell this amazing story, focusing on the challenges, opportunities, and wins of an organization where information itself is the product, and Wall Street is the setting.\nTopics will span the data flow lifecycle from data collection, curation and quality, to aggregation and standardization of a multitude of complex data sources, to the creation of valuable analytics, including recommendations that connect users to the data.\nTo surmount the obstacles and discover the opportunities, we\u2019ve built a focused team of technologists that use a variety of techniques and tools \u2013 including SQL, Hadoop, Solr, and others \u2013 as well as a small army of dedicated researchers that churn out timely, high-quality data.\nS&P Capital IQ is a leading provider of data and analytics for global financial professionals. We founded our business in 1999, and now work with over 4,200 client firms, including many of the world\u2019s most successful investment banks, asset management firms, private equity firms, universities, and corporations. Our mission is to change the way global professionals gather and analyze information, so they can work faster, better, and smarter. We help our clients identify investment opportunities, draw unique insights and increase returns by providing them with deep information on companies, markets, and people along with powerful applications for desktop research and analysis.\n", "\nMachine learning holds the key for massive waves that are already starting to fundamentally change business, from targeted advertising, to personalization, to real-time data-driven business processes.  But is ML really possible on big data with state-of-the-art methods (which yield the highest predictive accuracies), or just simple ones (such as linear models)?  Can ML really be done in real time today?  Is MapReduce really the best technical solution to large-scale ML?  Does it really make sense to send data to the cloud and do ML there?  In this talk I will review the current state of machine learning technology both at the research level and the industry-readiness level, and current best solution options.\nThis session is sponsored by Skytree, Inc\n", "\nApache Hadoop is the leading platform for storing, processing and managing \u201cbig data\u201d. Please join Arun C. Murthy, Hortonworks co-founder and VP of Apache Hadoop for the Apache Software Foundation, for a discussion about the next generation of Apache Hadoop, known as hadoop-0.23. Attend this session to learn how MapReduce has been re-architected by the community to improve reliability, availability and scalability as well as the ability to support alternate programming paradigms. You will also learn about HDFS Federation, which allows for significant scalability improvements, as well as other important advancements. Arun will also share details about the roadmap and answer questions from the audience about future enhancements to Apache Hadoop.\nThis session is sponsored by Hortonworks\n", "\nFlurry\u2019s analytics and advertising platform for mobile tracks data on over 330 million devices per month. We operate a 500 node Hadoop and HBase cluster to mine and manage all this data. This talk will go over some of the lessons learned, architecture choices, and advantages of running this big data platform. Some of the covered topics include:\n\nMining data for marketing\nLessons learned from operating a large production Hadoop cluster\nFault tolerance in the Flurry architecture\nAlgorithms for estimating demographics across applications\n\n", "\nBig data isn\u2019t just about multi-terrabyte data sets hidden inside eventually-concurrent distributed databases in the cloud, or enterprise-scale data warehousing, or even the emerging market in data. It\u2019s also about the hidden data you carry with you all the time, about the slowly growing data sets on your movements, contacts and social interactions.\nUntil recently most people\u2019s understanding of what can actually be done with the data collected about us by our own cell phones was theoretical; there were few real-world examples. But over the last couple of years this has changed dramatically.\nThis talk will discuss the data that you carry with you; the data on your cell phone and other mobile devices, along with the possibilities for making use of that hidden data to reveal things about our lives that we might not realise ourselves. We will explore the types of data that is collected, and the online data sources that you could be usefully cross-correlated with it.\n", "\nNetworks are a data structure common found across all social media services that allow populations to author collections of connections.  The Social Media Research Foundation\u2019s (http://www.smrfoundation.org) free and open NodeXL project (http://nodexl.codeplex.com) makes analysis of social media networks accessible to most users of the Excel spreadsheet application.  With NodeXL, Networks become as easy to create as pie charts.  Applying the tool to a range of social media networks has already revealed the variations present in online social spaces.  A review of the tool and images of Twitter, flickr, YouTube, and email networks will be presented.\nWe now live in a sea of tweets, posts, blogs, and updates coming from a significant fraction of the people in the connected world.  Our personal and professional relationships are now made up as much of texts, emails, phone calls, photos, videos, documents, slides, and game play as by face-to-face interactions.  Social media can be a bewildering stream of comments, a daunting fire hose of content.  With better tools and a few key concepts from the social sciences, the social media swarm of favorites, comments, tags, likes, ratings, and links can be brought into clearer focus to reveal key people, topics and sub-communities.  As more social interactions move through machine-readable data sets new insights and illustrations of human relationships and organizations become possible.  But new forms of data require new tools to collect, analyze, and communicate insights.\n", "\nBirds of a Feather (BoF) sessions provide networking opportunities for attendees interested in the same projects and topics to meet in a casual setting. BoFs topics are created by attendees and can be about individual projects or broader topics (best practices, open data, standards).\nBoFs at Strata will happen during lunch on Wednesday, February 29th and Thursday, March 1st, where lunch is served.\nStop by the BoF signup board near Registration to check out the topics or start your own.\n", "\nBirds of a Feather (BoF) sessions provide networking opportunities for attendees interested in the same projects and topics to meet in a casual setting. BoFs topics are created by attendees and can be about individual projects or broader topics (best practices, open data, standards).\nBoFs at Strata will happen during lunch on Wednesday, February 29th and Thursday, March 1st, where lunch is served.\nStop by the BoF signup board near Registration to check out the topics or start your own.\n", "\nLearn how Microsoft manages a 10,000 person IT Organization utilizing Business Intelligence capabilities to drive communication of strategy, performance monitoring of key analytics, employee self-service BI, and leadership decision-making throughout the global Microsot IT organization.  The session will focus on high-level BI challenges and needs of IT executives, Microsoft IT\u2019s BI strategy, and the capabilities that helped to drive BI internal use from 300 users to over 40,000 users (and growing) through self-service BI methodologies.\n", "\nWhen doing predictive modelling, there are two situations in which you might find yourself:\n\nYou need to fit a well-defined parameterised model to your data, so you require a learning algorithm which can find those parameters on a large data set without over-fitting\n\n\nYou just need a \u201cblack box\u201d which can predict your dependent variable as accurately as possible, so you need a learning algorithm which can automatically identify the structure, interactions, and relationships in the data\n\nFor case (1), lasso and elastic-net regularized generalized linear models are a set of modern algorithms which meet all these needs. They are fast, work on huge data sets, and avoid over-fitting automatically. They are available in the \u201cglmnet\u201d package in R.\nFor case (2), ensembles of decision trees (often known as \u201cRandom Forests\u201d) have been the most successful general-purpose algorithm in modern times. For instance, most Kaggle competitions have at least one top entry that heavily uses this approach. This algorithm is very simple to understand, and is fast and easy to apply. It is available in the \u201crandomForest\u201d package in R.\nMike and Jeremy will explain in simple terms, using no complex math, how these algorithms work, and will also explain using numerous examples how to apply them using R. They will also provide advice on how to select from these algorithms, and will show how to prepare the data, and how to use the trained models in practice.\nNow Available\n\nWatch a video of the presentation.\nDownload the free O\u2019Reilly report Designing Great Data Products by Jeremy Howard, Margit Zwemer, and Mike Loukides.\n", "\nStorm makes it easy to write and scale complex realtime computations on a cluster of computers, doing for realtime processing what Hadoop did for batch processing. Storm guarantees that every message will be processed. And it\u2019s fast \u2014 you can process millions of messages per second with a small cluster. Best of all, you can write Storm topologies using any programming language. Twitter relies upon Storm for much of its analytics.\nAfter being open-sourced, Storm instantly attracted a large community. It is by far the most watched JVM project on GitHub and the mailing list is active with over 300 users.\nStorm has a wide range of use cases, from stream processing to continuous computation to distributed RPC. In this talk I\u2019ll introduce Storm and show how easy it is to use for realtime computation.\n", "\nTools like Pig, Hive, and Cascading ease the burden of writing MapReduce pipelines by defining Tuple-oriented data models and providing support for filtering, joining and aggregating those records. However, there are many data sets that do not naturally fit into the Tuple model, such as images, time series, audio files and seismograms. To process data in these binary formats, developers often go back to writing MapReduces using the low-level Java APIs.\nIn this session, Cloudera Data Scientist Josh Wills will share insights and \u201chow to\u201d tricks about Crunch, a Java library that aims to make writing, testing and running MapReduce pipelines that run over any type of data easy, efficient and even fun. Crunch\u2019s design is modeled after Google\u2019s FlumeJava library and focuses on a small set of simple primitive operations and lightweight user-defined functions that can be combined to create complex, multi-stage pipelines. At runtime, Crunch compiles the pipeline into a sequence of MapReduce jobs and manages their execution on the Hadoop cluster.\n", "\nWith recent advances in linguistic algorithms, data processing capabilities and the availability of large structured data sets, it is now possible for software to create long form narratives that rival humans in quality and depth. This means content development can take advantage of many of the positive attributes of software, namely, continuous improvement, collaborative development and significant computational processing.\nRobbie Allen, the CEO of Automated Insights, and his team have done this to great effect by automatically creating over 100,000 articles covering College Basketball, College Football, NBA, MLB, NFL in a 10 month period. Automated Insights is now branching out beyond sports into finance, real-estate, government, and healthcare.\nIn this talk, Robbie will share the lesson\u2019s his company has learned about the viability of automated content and where the technology is headed. It all started with short sentences of uniform content and has expanded to the point where software can generate several paragraphs of unique prose highlighting the important aspects of an event or story.\n", "\nIn \u201cThe Evolution of Data Products\u201d, O\u2019Reilly Media\u2019s Mike Loukides notes: \u201cthe question of how we take the next step \u2014 where data recedes into the background \u2014 is surprisingly tough. Do we want products that deliver data? Or do we want products that deliver results based on data? We\u2019re evolving toward the latter, though we\u2019re not there yet.\u201d In this talk, Jeremy Howard will show why taking this step is tough, and will lay out what needs to be done to deliver results based on data. He will particularly draw on his experience in building Optimal Decisions Group, where he developed a new approach to insurance pricing which focused on delivering results (i.e.: determine the optimal price for a customer) instead of delivering data (i.e. calculating a customer\u2019s risk, which had been the standard approach used by actuaries previously).\nDelivering results based on data requires 3 steps:\n1)    Creating predictive models for each component of the system\n2)    Combining these predictive models into a simulation\n3)    Using an optimization algorithm to optimize the inputs to the simulation based on the desired outcomes and the system constraints\nUnfortunately, many data scientists today are not sufficiently familiar with steps 2) and 3) of this process. Although many data scientists have been developing skills in predictive modelling, simulation and optimization skills are still rare. Jeremy will show how these 3 steps fit together, give examples of their use in real world situations, and will introduce some of the key algorithms and methods that can be used.\n", "\nOne of the most complex tasks in a data processing environment is record linkage, the data integration process of accurately matching or clustering records or documents from multiple data sources containing information which refer to the same entity such as a person or business.\nThe massive amount of data being collected at many organizations has led to what is now being called the \u201cBig Data\u201d problem which limits the capability of organizations to process and use their data effectively and makes the record linkage process even more challenging.\nNew high-performance data-intensive computing architectures supporting scalable parallel processing such as Hadoop MapReduce and HPCC allow government, commercial organizations, and research environments to process massive amounts of data and solve complex data processing problems including record linkage.\nA fundamental challenge of data-intensive computing is developing new algorithms which can scale to search and process big data.  SALT (Scalable Automated Linking Technology) is new tool which automatically generates code in the ECL language for the open source HPCC scalable data-intensive computing platform based on a simple specification to address most common data integration tasks including data profiling, data cleansing, data ingest, and record linkage.\nSALT is an ECL code generator for use with the open source HPCC platform for data-intensive computing.  The input to the SALT tool is a small, user-defined specification stored as a text file which includes declarative statements describing the user input data and process parameters, the output is ECL code which is then compiled into optimized C++ for execution on the HPCC platform.\nThe SALT tool can be used to generate complete applications ready to-execute for data profiling, data hygiene (also called data cleansing, the process of cleaning data), data source consistency monitoring (checking consistency of data value distributions among multiple sources of input), data file delta changes, data ingest, and record linking and clustering.\nSALT record linking and clustering capabilities include internal linking \u2013 the batch process of linking records from multiple sources which refer to the same entity to a unique entity identifier;  and external linking \u2013 also called entity resolution, the batch process of linking information from an external file to a previously linked base or authority file in order to assign entity identifiers to the external data, or an online process where information entered about an entity is resolved to a specific entity identifier, or an online process for searching for records in an authority file which best match entered information about an entity.\nSALT Use Case \u2013 LexisNexis Risk Solutions Insurance Services used SALT to develop a new insurance header file and insurance ID to combine all the available LexisNexis person data with insurance data.  Process combines 1.5 billion insurance records and 9 billion person records.  290 million core clusters are produced by the linking process.  Reduced source lines of code from 20,000+ to a 48 line SALT specification.  Reduced linking time from 9 days to 55 hours.  Precision of 99.9907 was achieved.\nSummary and Conclusions \u2013 Using SALT in combination with the HPCC high-performance data-intensive computing platform can help organizations solve the complex data integration and processing issues resulting from the Big Data problem, helping organizations improve data quality, increase productivity, and enhance data analysis capabilities, timeliness, and effectiveness.\n", "\n\nAttendees: Please read the instructions & prerequisites before arriving to the tutorial.\n\nThis tutorial will teach attendees about the key aspects of scalable web mining, via six modules:\n\n\n1. Introduction\n\t\n Why web data is valuable\nKey challenges to web crawling\nRealistic definitions for success\n\n\n\n2. Focused Web Crawling\n\t\nReducing time & cost by focusing the crawl\nApproaches to classifying and scoring pages\nSolutions for scalable web crawling\n\n\n\n3. Structured Data Extraction\n\t\nData mining essentials\nStructured text extraction\nAutomated vs. manual extraction\n\n\n\n4. Analyzing the Data\n\t\nMaking it searchable\nFinding \u201cinteresting\u201d text\nMachine learning with Mahout\n\n\n\n5. Barriers to Success\n\t\nPolite crawling versus deep crawling\nSpam, splog, honeypots and nasty webmasters\nAjax, robots.txt and Facebook\n\n\n\n6. Examples and Summary\n\t\nHotel reviews\n * Music pages\n * SEO analysis\n\n", "\nLearn how to integrate MongoDB with Hadoop for large-scale distributed data processing. Using tools like MapReduce, Pig and Streaming you will learn how to do analytics and ETL on large datasets with the ability to load and save data against MongoDB. With Hadoop MapReduce, Java and Scala programmers will find a native solution for using MapReduce to process their data with MongoDB. Programmers of all kinds will find a new way to work with ETL using Pig to extract and analyze large datasets and persist the results to MongoDB. Python and Ruby Programmers can rejoice as well in a new way to write native Mongo MapReduce using the Hadoop Streaming interfaces.\n", "\nBig Data provides big banks with the means to monetize the transaction data stream in ways that are both pro-consumer and pro-merchant.  By utilizing data-driven personalization services, financial institutions can offer a better customer experience and boost customer loyalty.  For example, integrating rewards and analysis within a consumer\u2019s online banking statement can save a consumer on average $1,000 per year just by comparing plans, pricing, and usage habits within wireless, cable, and gas categories.  Financial institutions benefit by increasing their relationship value with customers.  Merchants benefit from increased analytics and are able to reward loyal customers with deals that matter most based upon their purchasing habits.\nThese data driven services increase a bank\u2019s relationship value with customers.  94% of consumers indicate that they\u2019d use a specific card that was ties to money-saving discounts over a card that did not and 3 in 4 admitted that they\u2019d switch banks if their bank did not offer loyalty rewards.\nBig data is not just big stakes for loyalty\u2014it can be used to drive customer acquisition and increase market share (or credit card \u2018share of wallet\u2019) which drive other banking revenue streams.\nFurthermore, data driven offerings help promote the conversion of non-online customers to online banking and billpay, a cost reduction potential of $167 per account per year or $8.3 billion annually according to Javelin.\n", "\nA story on the U.S. Census will tell the broad themes behind the data and use people to exemplify those themes. But what every reader also wants to know answers to more specific questions: How did my community change? What happened where I live, in my neighborhood? And being able to provide those answers through an interactive visualization is what story-telling through the data is all about. A story or report on a subject by its very nature summarizes the underlying data. But readers may have questions specific to a time, date or place. Visualizing the data and providing effective, targeted ways to drill deeper is key to giving the reader more than just the story. The visualization can enhance and deepen the experience. Cheryl Phillips will discuss data visualization strategies to do just that, providing examples from The Seattle Times and other journalism organizations.\n", "\nCompanies are wrestling with the challenges of managing and exploiting big data.  Larger, more diverse data sources and the business need for low-latency access to that data combine to provide new data monetization opportunities.  But \u201cbolting\u201d analytics onto your existing data warehouse and business intelligence environment does not work.\nHow do business owners and IT work together to identify the right business problem and then design the right architecture, to exploit these new data monetization opportunities?  How do you ensure the successful deployment of these new capabilities, given the historically high rate of failure for new technologies?\nThis session will present a tried and proven methodology that is based upon a simple premise\u2014business opportunities must drive all information technology deployments.  While a technology-led approach is useful for helping an organization gain insight into what a new technology does, it is critical that the business opportunities drive the \u201cwhy,\u201d \u201chow,\u201d and \u201cwhere\u201d to implement new technologies.\nThis methodology provides the following key benefits:\n\nEnsures that your big data analytics initiative is focused on the business opportunities that provide the optimal tradeoff between business benefit and implementation feasibility\n\n\nBuilds the organizational consensus necessary for success by aligning corporate resources around common goals, assumptions, priorities, and metrics\n\nCase study examples will demonstrate its use.\n", "\nThere are many different approaches to putting data on the web, ranging from bulk downloads through to rich APIs. These styles suit a range of different data processing and integration patterns. But the history of the web has shown that value and network effects follow from making things addressable.\nFacebook\u2019s Open Graph, Schema.org, and a recent scramble towards a \u201cRosetta Stone\u201d for geodata, are all examples of a trend towards linking data across the web. Weaving data into the web simplifies data integration. Big Data offers ways to mine huge datasets for insight. Linked Data creates massively inter-connected datasets that can be mined or drawn upon to enrich queries and analysis\nThis talk will look at the concept of Linked Data and how a rapidly growing number of inter-connected databases, from a diverse range of sources, can be used to contextualise Big Data.\n", "\nCollborative filtering is a method of making predictions about a user\u2019s interests based on the preferences of many other users. It\u2019s used to make recommendations on many Internet sites, including LinkedIn. For instance, there\u2019s a \u201cViewers of this profile also viewed\u201d module on a user\u2019s profile that shows other covisited pages. This \u201cwisdom of the crowd\u201d recommendation platform, built atop Hadoop, exists across many entities on LinkedIn, including jobs, companies, etc., and is a significant driver of engagement.\nDuring this talk, I will build a complete, scalable item-to-item collaborative filtering MapReduce flow in front of the audience. We\u2019ll then get into some performance optimizations, model improvements, and practical considerations: a few simple tweaks can result in an order of magnitude performance improvement and a substantial increase in clickthroughs from the naive approach. This simple covisitation method gets us more than 80% of the way to the more sophisticated algorithms we have tried.\nThis is a practical talk that is accessible to all.\n", "\nHave the demographics of inventors changed over the last 40 years with the Midwest\u2019s industrial center becoming the rust belt while Silicon Valley has grown to dominate the high tech industry?   Have the number of inventors declined in Michigan and grown in California?   What states have the most then versus now?  These simple questions are used to illustrate the simplicity of processing big data using the HPCC Systems open-source platform.  The focus of this session will be on the Enterprise Control Language (ECL) and its use for ETL, data analytics, and query processing.\nThis session is sponsored by LexisNexis/HPCC\n", "\nWhy unstructured data beats structured.\n", "\nThe Hadoop framework is an established solution for big data management and\nanalysis. In practice, Hadoop applications vary significantly. Your data\ncenter infrastructure is used by multiple lines of business and\nmultiple differing workloads.\nThis session looks at the requirements for a multi-tenant big data cluster:\none where different lines of businesses, different projects, and\nmultiple applications can be run with assured SLAs, resulting in\nhigher utilization and ROI for these clusters.\nThis session is sponsored by Platform Computing\n", "\nVisual analysis is an iterative process for working with data that exploits the power of the human visual system.  The formal core of visual analysis is the mapping of data to appropriate visual representations.\nIn this talk, you\u2019ll learn:\n\u2022What years of research by psychologists, statisticians and others have taught us about designing great visualizations \n\u2022Fundamental principles for designing effective data views for yourself and others\n\u2022How to systematically analyze data using your visual system\n", "\nJoin us for an in depth architectural review of the latest infrastructure built by Citygrid to process and serve the local places data available via Citygrid APIs.\nWe will present how Hadoop is used to process large amounts of inbound data from disparate sources and to solve the complex problem of matching for places.\nWe will also discuss how Hadoop is used to generate the Solr and MongoDB indexes used for serving.\nWe will describe the function of the places, content and ad APIs and SDKs, and the characteristics of their underlying data, in the context of real world use cases.\nWe will focus on some of the limitations of Lucene and Solr for geographic search, and discuss some of the most recent developments we are exploring for our next generation APIs.\nFinally, we will give a preview of Citygrid\u2019s next generation real time event processing system, inspired by Twitter\u2019s Rainbird and build on top of Cassandra.\n", "\nImportant: please read the equipment requirements at the bottom of this page before attending the tutorial\nData has always been a second class citizen on the web. As images, then audio, then video\nmade their way onto the internet, data was always left out of the party, forced into dusty\nExcel files and ugly HTML tables. Tableau Public is one of the tools aiming to change that by allowing\nanyone to create interactive charts, maps and graphs and publishing to the web\u2014no\nprogramming required.\nIn this tutorial you will learn why data is vital to the future\nof the web, how Tableau Public works, and gain hands-on experience with taking data from numbers to the web.\nThrough three different use cases, you will learn the capabilities of the Tableau Public product. The tutorial will conclude with an extended hands-on session covering the visualization process from data to publishing. Topics covered will include:\n\nconstructing a data set for best performance\nformatting visualizations to match your preferred branding\ndesigning charts for clear communication and impact\n\nIMPORTANT: EQUIPMENT REQUIREMENTS\nThis is a hands-on tutorial. You will need to bring either a Windows laptop or a laptop with a Windows virtual machine installed. Before arriving, you should download and install Tableau Public from this URL: http://www.tableausoftware.com/public\n", "\nAs the size and performance requirements of storage systems have\nincreased, file system designers have looked to new architectures to\nfacilitate system scalability.\nCeph\u2019s architecture consists of an object storage, block storage and a POSIX-compliant file system. It\u2019s in the most significant storage system that has been accepted into the Linux kernel. Ceph has both kernel and userland implementations.The CRUSH algorithm  controlled, scalable, decentralized placement of replicated data. In addition, it has a fully leveraged, highly scalable metadata layer. Ceph offers compatibility with S3, Swift and Google Storage and is a drop in replacement for HDFS (and other File Systems).\nCeph is unique because it\u2019s massively scalable to the exabyte level.\nThe storage system is self-managing and self-healing which means limited system administrator involvement.\nIt runs on commodity hardware, has no single point of failure, leverages an intelligent storage node system and it open source.\nThis talk will describe the Ceph architecture and then focus on the\ncurrent status and future of the project.  This will include a discussion\nof Ceph\u2019s integration with Openstack, the file system, RBD clients in the\nLinux kernel, RBD support for virtual block devices in Qemu/KVM and\nlibvirt, and current engineering challenges.\n", "\nSo much of the privacy discussion is about data collection and access, fears of a future dystopia, and the complexities of law.  There seems to be a real vacuum around how societal norms should be mapped to rapidly growing capabilities of big data.  What\u2019s difficult about some of these big data use-cases is that even the intended and approved uses of data can lead to decisions or actions that negatively affect specific individuals or groups.  These can range from effects on safety (by making a person more easily identifiable or locatable), to fairness (because the purpose of the application is some form of discrimination), to autonomy (by limiting individual choice or through subtle manipulation).\nRegrettably, data professionals (e.g., scientists, engineers, designers, analysts) are left in a \u201cdon\u2019t ask don\u2019t tell\u201d privacy conundrum where no framework exists to assess the societal impact of their work.  Such a framework would need to go beyond default \u201cprocedural protections\u201d (e.g., the Fair Information Practice Principles) to \u201csubstantive protections\u201d that evaluate possible product impact at design-time and track actual impact as the product moves into the market.\nThis conversation will address, from academic and industrial perspectives, specific use-cases within people search, background checks, online advertising, and voter targeting.  Through these use-cases, we\u2019ll explore the feasibility of a \u201cresponsible innovation\u201d framework that might guide data professionals.\n", "\nI am a doctor and a data geek. I worry that data geeks are too easily seduced by the glamour of laboratory science and forget about clinics. Randomised controlled trials are the best tool we have in medicine for finding out if a treatment works or not. Lots of trials are done. Unfortunately, the results of these trials can go missing in action after they are completed.\nMissing data is always a challenge: but we also know that \u201cnegative results\u201d are more likely to go missing. This means we have a biased sample, overestimating the benefits of treatments. To prevent all this happening, people have set up registers of trial protocols, to be completed before trials begin. These have not been correctly used, and they are not matched to published trials, which show up what data has been left unpublished.\nI will describe a small project to fix this, illustrate how that can lead on to fixing other similar problems in medicine, and make a cry for help.\n", "\nOpening remarks by the Strata program chairs, Edd Dumbill and Alistair Croll.\n", "\nOpening remarks by the Strata program chairs, Alistair Croll and Edd Dumbill.\n", "\nRecommendation systems have become critical for delivering relevant and personalized content to your users. Such systems not only drive revenues and generate significant user engagement for web companies but also are a great discovery tool for users. Facebook\u2019s newsfeed, Linkedin\u2019s people you may know and Eventbrite\u2019s event recommendations are some great examples of recommendation systems.\nDuring this talk we will share the architecture and design of Eventbrite\u2019s data platform and recommendation engine. We will describe how we mined a massive social graph of 18M users and 6B first degree connections to provide relevant event recommendations. We will provide details of our data platform, which supports processing more than 2 TB social graph data daily. We intent to describe how Hadoop is becoming the most important tool to do data mining and also discuss how machine learning is changing in presence of Hadoop and big data.\nWe hope to provide enough details that folks can learn from our experiences while building their data platform and recommendation systems.\n", "\nGoogle Insights for Search provides an index of search activity for millions of queries.  These queries can sometimes help understand consumer behavior.  Hal describes some of the issues that arise in trying to use this data for short-term economic forecasts and provide examples.\n", "\nThis session will shed light on real-world use cases for NoSQL databases by providing case studies from enterprise production users taking advantage of the massively scalable and highly-available architecture of Apache Cassandra.\n\nNetflix \u2013 See how, with Cassandra, Netflix achieved cloud-enabled business agility, capacity and application flexibility,  and never worried about running out of space or power.\nBackupify \u2013 Cassandra enables reliable, redundant and scalable low-balance data storage, eliminating downtime and ensuring they can backup customer data around the clock.\nOoyala \u2013 The elastically scalable Cassandra database allows Ooyala to absorb and leverage massive amounts of digital video data by simply adding nodes which can grow to hundreds or thousands.\nConstant Contact \u2013 Cassandra enables Constant Contact to massively scale an operationally simple application that deployed in 3 months for $250k, compared to 9 months for $2.5 million if they used a traditional RDBMS.\n\nAt the end of this session you will have an good understanding of the types of requirements Cassandra can satisfy through a carefully thought-out architecture designed to manage all forms of modern data, that scales to meet the requirements of \u201cbig data\u201d management, that offers linear performance scale-out capabilities, and delivers the type of high availability that most every online, 24\u00d77 application needs.\n", "\nScientists dealt with big data and big analytics for at least a decade before the business world precipitated buzz-words like \u2018Big Data\u2019, \u2018Data Tsunami\u2019 and \u2018the Industrial Revolution of data\u2019 from the strange broth of their marketing solution and came to realize they had the same problems. Both the scientific world and the commercial world share requirements for a high performance informatics platform supporting the collection, curation, collaboration, exploration, and analysis of massive datasets.\nIn this talk we will sketch the design of SciDB and explain how it differs from hadoop-based systems, SQL DBMS products, and NoSQL platforms, and explain why that matters.  We will present benchmarking data and present a computational genomics use case that showcase SciDB\u2019s massively scalable parallel analytics.\nSciDB is an emerging open source analytical database that runs on a commodity hardware grid or in the cloud.  SciDB natively supports:\n\u2022    An array data model \u2013 a flexible, compact, extensible data model for rich, highly dimensional data\n\u2022    Massively scale math \u2013 non-embarassingly parallel operations like linear algebra operations on matrices too large to fit in memory as well as transparently scalable R, MatLab, and SAS style analytics without requiring code for data distribution or parallel computation\n\u2022    Versioning and Provenance \u2013 Data is updated, but never overwritten. The raw data, the derived data, and the derivation are kept for reproducibility, what-if modeling, back-testing, and re-analysis\n\u2022    Uncertainty support \u2013 data carry error bars, probability distribution or confidence metrics that can be propagated through calculations\n\u2022    Smart storage \u2013 compact storage for both dense and sparse data that is efficient for location-based, time-series, and instrument data\n", "\nAttendees: All attendees should bring paper an pen for quick sketching.  Attendees should bring their own data to work with. Alternately, they can download interesting data sets from sites such as infochimps.com, buzzdata.com, and data.gov. People with access to a windows machine might want to install Tableau Public.\n\n\nWe will discuss how to figure out what story to tell, select the right data, and pick appropriate layout and encodings. The goal is to learn how to create a visualization that conveys appropriate knowledge to a specific audience (which may include the designer).\nWe\u2019ll briefly discuss tools, including pencil and paper. No prior technology or graphic design experience is necessary. An awareness of some basic user-centered design concepts will be helpful.\nUnderstanding of your specific data or data types will help immensely. Please do bring data sets to play with.\n", "\nThis tutorial will explain how to leverage a Hadoop cluster to do data analysis using Java MapReduce, Apache Hive and Apache Pig.  It is recommended that participants have experience with some programming language.  Topics include:\n\nWhy are Hadoop and MapReduce needed?\nWriting a Java MapReduce program\nCommon algorithms applied to Hadoop such as indexing, classification, joining data sets and graph processing\nData analysis with Hive and Pig\nOverview of writing applications that use Apache HBase\n\nSome programming experience is strongly recommended for this session.\n", "\nManagement Strategies for Big Data will demystify the concepts of Big Data, bring real world examples and give practical guide on how to apply Big Data within their organization. This is based on the upcoming O\u2019Reilly book, \u201cManagement Strategies for Big Data\u201d. We will bring a practical approach to mapping the value of emerging Big Data technologies to real business value.\nThe target audience is business and technical decision makers.  The session objective is to give the listener the high level understanding of this new emerging technology and how to apply it to their business strategy. There will be a focus on supporting technology, target architectures and case studies of how to find new revenue, improve efficiencies and uncover new innovation.\n", "\nIn this session we will discuss two key aspects of using JavaScript in the Hadoop environment. The first one is how we can reach to a much broader set of developers by enabling JavaScript support on Hadoop. The JavaScript fluent API that works on top of other languages like PigLatin let developers define MapReduce jobs in a style that is much more natural; even to those who are unfamiliar to the Hadoop environment.\nThe second one is how to enable simple experiences directly through an HTML5-based interface. The lightweight Web interface gives developer the same experience as they would get on the Server. The web interface provides a zero installation experience to the developer across all client platforms. This also allowed us to use HTML5 support in the browsers to give some basic data visualization support for quick data analysis and charting.\nDuring the session we will also share how we used other open source projects like Rhino to enable JavaScript on top of Hadoop.\n", "\nBig Data is about extracting value from fast, huge, varied, complex data sets. But simply crunching data is only the first step. As adoption of MapReduce and data analytic technologies increases, forward thinking companies are starting to build applications on their core data assets. In this keynote, MarkLogic\u2019s Gary Lang will explore what these Big Data Applications look like, offering some tantalizing real-world glimpses at what data wrapped in applications makes possible.\nThis keynote is sponsored by MarkLogic\n", "\nThe advent of crowdsourcing has wildly expanded the ways we think of incorporating human judgments into computational workflows. Computer scientists, economists, and sociologists have explored how to effectively and efficiently distribute microwork tasks to crowds and use their work as inputs to create or improve data products. Simultaneously, crowdsourcing providers are exploring the bounds of mechanical QA flows, worker interfaces, and workforce management systems.\nBut what tasks should be performed by humans rather than algorithms? And what makes a set of human judgments robust? Quantity? Consensus? Quality or trustworthiness of the workers? Moreover, the robustness of judgments depends not only on the workers, but on the task design. Effective crowdsourcing is a cooperative endeavor.\nIn this talk, we will analyze various dimensions of microwork that characterize applications, tasks, and crowds. Drawing on our experience at companies that have pioneered the use of microwork (Samasource) and data science (LinkedIn), we will offer practical advice to help you design crowdsourcing workflows to meet your data product needs.\n", "\nThere are many modern techniques for identifying anomalies in datasets. There are fewer that work as online algorithms suitable for application to real-time streaming data.  What\u2019s worse? Most of these methodologies require a deep understanding of the data itself.  In this talk, we tour what the options are for identifying anomalies in real-time data and discuss how much we really need to know before hand to guess at the ever-useful question: is this normal?\n", "\nA data scientist usually starts with some hypothesis about what should go in a predictive model. Since a model is a simplified representation of the world, we will never conclusively know that a variable causes some effect. The data scientist can only try to differentiate correlation and causation, by measuring a variable\u2019s predictive power across a long time-span. Perhaps even across the whole lifetime of the project.\nIn reality a hypothesis of important variables is shaped by the technical realities of the organization. Whatever data the organization collects is usually the place to start looking for predictive power. Something buried away in the Hadoop cluster must be correlated with customer churn, right?! The data scientist also reviews the academic literature for finding data worth her attention. Which variables turn up in an researcher\u2019s toy model? Or on that well-capitalized dream project, the data scientist considers third-party sources. Does anyone have a chunk of data that predicts our churn? The data scientist inevitably arrives at the point where the extent of a variable\u2019s usefulness needs to be measured. In other words, how do we choose which variables to include in our model?\nThe big data community stands on the shoulders of several giants. Appropriately there are many different approaches to estimating variable importance. A traditional statistician might calculate p-statistics, while a time-series modeller measures improvement in blind, out-of-sample forecasting accuracy. And the machine learning expert may use Shannon\u2019s entropy to calculate the information gain of a variable. Each of these measures makes assumptions about the process being modeled. For example, the data scientist with a time-series bent must decide the minimum amount of hold-out data that still represents a population. All this nuance in variable importance is worth negotiating, because feature selection has a multiplicative effect on the overall modeling process. Good variable importance and feature selection means:\n\nLess Data, so easier data layer\nSimpler Models, so faster machine learning\nLower Variance, so models generalize\n\nFeature selection is something of an open secret. An interview with a candidate for a data scientist role is probably going well if you spend most of the time talking about variable importance, and only touch on the \u201csecret code\u201d of a particular machine learning algorithm at the interview\u2019s end.\nMeasuring the predictive power of a variable is even useful outside the context of training a predictive model. If three variables sampled every day have done a decent job of predicting customer churn, an organization might spend the money to sample those variables every hour or every minute. This is an example of data ecology and data feedback. Measuring variable importance pushes the organization to change how it collects and analyzes its data. Given enough time, this feedback loop optimizes how the organization as a whole approaches data.\nMy presentation introduces feature selection in the context of real-life trade-offs. I will briefly cover several variable importance measures, and also discuss the difference between variable reduction (regularization), and variable selection. We will also contrast on-the-fly variable importance (i.e. LARS) with a separate selection pass. Principal components analysis will be critiqued as a \u201cblack box\u201d approach to dimensionality reduction, but one that fails to provide useful feedback the organization. Since important variables are often useful when clustering, I will also touch on unsupervised learning. Throughout the presentation I will refer to real-life use cases from my work with Altos Research, and our forecasting model of residential real estate.\n", "\nWhile Map/Reduce is an excellent environment for some parallel computing tasks, there are many ways to use a cluster beyond Map/Reduce. Within the last year, the YARN and NextGen Map/Reduce has been contributed into the Hadoop trunk, Mesos has been released as an open source project, and a variety of new parallel programming environments have emerged such as Spark, Giraph, Golden Orb, Accumulo, and others.\nWe will discuss the features of YARN and Mesos, and talk about obvious yet relatively unexplored uses of these cluster schedulers as simple work queues. Examples will be provided in the context of machine learning. Next, we will provide an overview of the Bulk-Synchronous-Parallel model of computation, and compare and contrast the implementations that have emerged over the last year. We will also discuss two other alternative environments: Spark, an in-memory version of Map/Reduce which features a Scala-based interpreter; and Accumulo, a BigTable-style database that implements a novel model for parallel computation and was recently released by the NSA.\n", "\nHow are businesses using big data to connect with their customers, deliver new products or services faster and create a competitive advantage?  Luke Lonergan, co-founder & CTO, Greenplum, a division of EMC, gives insight into the changing nature of customer intimacy and how the technologies and techniques around big data analysis provide business advantage in today\u2019s social, mobile environment \u2013 and why it is imperative to adopt a big data analytics strategy.\nThis session is sponsored by Greenplum, a division of EMC\u00b2\n", "\nThe race is on to create the next competitive advantage. Attend this customer session for a brief introduction to Greenplum\u2019s Big Data Analytics platform. Hear from McAfee, Knotice, and Havas Digital using Greenplum\u2019s solutions to harness big data for real business advantage \u2013 providing rapid information delivery to discover fresh insights for smarter decisions, profiting from customer intelligence, and optimizing productivity.\nThis session is sponsored by EMC\u00b2\n", "\nCall closes 11:59pm 01/20/2012 PST.\nSubmit a proposal\nIf data is the new oil, then thousands of new companies have started to drill for it. Some are building data marketplaces to host the world\u2019s knowledge; others, powerful tools to process petabytes instantly. And many more are re-thinking what\u2019s possible in old industries, disrupting lazy incumbents and revitalizing entire markets.\n\n\n\nShowcase your company to industry leaders\nOnce again this year at Strata, we\u2019ll be inviting the best of the best to show what they\u2019re up to. The Startup Showcase is a chance for ten finalists to demonstrate their innovations to a packed room of investors, entrepreneurs, journalists, and potential employees. At the end of the showcase, our team of judges will choose three companies from among the ten finalists, selecting those whose technology, team, and offering stand apart from the rest.\n\nJudges:\n \n\n\n\nAlistair Croll (Solve for Interesting)\n\n\n\nTim Guleri (Sierra)\n\n\n\nRoger Ehrenberg (IA Ventures)\n\n\n\nDJ Patil (Greylock)\n\n\n\nTodd Papaioannou (Battery)\n\n\n\nRenee DiResta (OATV)\n\n\n\nStartup Showcase Finalists\n\nLex Machina\nMetalayer\nTokutek\nPolychart\nBitdeli\nMemSQL\nNgdata\nPropeld\nQuadrigram\nSpot Influence\n\nImportant Dates & Details\n\nIf selected, your company will receive 2 passes to attend Strata Conference\nYou\u2019re expected to be able to make your own way there\nThe showcase takes place on Wednesday, February 29\nEach company will get a tabletop, power, and bandwidth within the showcase area to demonstrate their product \n\n", "\nDeep Data is a no-holds-barred program for data scientists. The advanced technical content will keep you up to speed with the latest techniques, and give you the opportunity to debate and network with the most skilled data scientists in our industry.\nSchedule\n9:00am \u2013 9:45amSQL and NoSQL Are Two Sides Of The Same Coin\nMichael Rys\nContrary to popular belief, SQL and NoSQL are not at odds with each other, they are duals\u2014in fact NoSQL should really be called coSQL.  Recognizing this duality can change the way we think about which technology to use when, and what we need to invest in next.\n9:45am \u2013 10:30amFrom Knowing \u2018What\u2019 To Understanding \u2018Why\u2019\nClaudia Perlich\nWith the collection of almost every piece of information about your customers comes the ability to start asking your data the right question: Why do they do what they do? And even more: what would they do if I could interact with them. We show for the case of online display advertising, how causal analysis gives interesting new answers about the right (and wrong) ways of spending your money.\n10:30am \u2013 11:00am Break\n11:00am \u2013 11:30amThe Model and the Train Wreck: A Training Data How-to\nMonica Rogati\nGetting training data for a recommender system is easy: if users clicked it, it\u2019s a positive \u2013 if they didn\u2019t, it\u2019s a negative. \u2026 Or is it? In this talk, we use examples from production recommender systems to bring training data to the forefront: from overcoming presentation bias to the art of crowdsourcing subjective judgments to creative data exhaust exploitation and feature creation.\n11:30am \u2013 12:00pmCorpus Bootstrapping with NLTK\nJacob Perkins\nLearn various ways to bootstrap a custom corpus for training highly accurate natural language processing models. Real world examples will be presented with Python code samples using NLTK. Each example will show you how, starting from scratch, you can rapidly produce a highly accurate custom corpus for training the kinds of natural language processing models you need.\n12:00pm-12:30pmThe Importance of Importance: An Introduction to Feature SelectionBen Gimpert\nTwenty-first century big data is being used to train predictive models of emotional sentiment, customer churn, patient health, and other behavioral complexities. Variable importance and feature selection reduces the dimensionality of our models, so an unfeasible and complex problem may become somewhat more predictable.\n12:30pm \u2013 1:30pm Lunch\n1:30pm-2:15pmSocial Network Analysis Isn\u2019t Just For People\nMatt Biddulph\nThe tools of social network analysis are based on mathematical network theory. There is very little in these techniques that actually requires that the data represents social activity. We\u2019ll show how these techniques can be applied to data from areas such as geo, linguistics and the Wikipedia link graph. We\u2019ll visualise and explore the data using Gephi, the \u201cPhotoshop for graphs\u201d.\n2:15pm-3:00pmArray Theory vs. Set Theory in Managing Data\nRobert Lefkowitz\nRelational databases were based on Set theory \u2014 which insists that the order of items does not matter.  For many (most?) data problems, however, order does matter.  By using Array theory, a relational-like database gains a considerable advantage over set-theory based engines.\n3:00pm \u2013 3:30pm Break\n3:30pm \u2013 4:00pmSurvival Analysis for Cache Time-to-Live Optimization\nRobert Lancaster\nWe examine the effectiveness of a statistical technique known as survival analysis to optimize the cache time-to-live for hotel rates in a hotel rate cache.  We describe how we collect and prepare nearly a billion records per day utilizing MongoDB and Hadoop. Finally, we show how this analysis is improving the operation of our hotel rate cache.\n4:00pm-5:00pmThe Data Science Debate\nPeter Skomoroch, Michael Driscoll, DJ Patil, Amy Heineike, Pete Warden, Toby Segaran\nEnd the day by joining leading data scientists in debating the hot issues in the profession.\nRegister Now for Strata 2012\n", "\nDr. Richard Merkin, President and CEO of Heritage Provider Network, that was recently named one of Fast Company\u2019s 10 most innovative healthcare companies for 2012, will announce the winner of the second progress prize in the $3 million dollar Heritage Health Prize competition. Responding to our country\u2019s $2 trillion dollar health care crises, Dr. Merkin created, developed and sponsored the $3 million dollar Heritage Health Prize seeking to find an algorithm that will predict future hospitalization, and save our Country the $30 billion that is spent annually on avoidable hospitalizations. It is the largest predictive modeling prize in the world, larger than the Nobel Prize for Medicine and the Gates Prize for Health. Dr. Merkin is genuinely excited to bring new minds to healthcare through the prize, and believes data miners hold great potential for solving many of the Nation\u2019s healthcare challenges.  Currently, there are over 11,000 entries in the prize competition from all over the world.  Dr. Merkin will present the top two teams with $80,000 in the second progress prize, split as $50,000 and $30,000.\n", "\n\nMini Maker Faire\nEnjoy drinks and mingle with other attendees as you peruse a gallery of fun, innovative Maker Faire projects. Meet the Makers and discuss the ideas behind their demos, and learn how these unconventional new technologies can be adapted into existing business strategies. Mini Maker Faire projects will be selected from research, academia, and yet-to-be-discovered entrepreneurs.\nData Crush: Where Wine and Data Meet\nWho says data can\u2019t be fun? This new event at Strata will host wine tastings for participants, whose feedback data will be compiled and analyzed to extrapolate behavioral trends and factors influencing their responses. More details will be unveiled onsite.\n\n", "\nJoin us at the Expo Hall Reception immediately following the first day of sessions. Have a drink or two, network with other attendees, and visit our companies who are innovating in the data space. This event is open to all sponsors, exhibitors and attendees.\n", "\nGary Lang, Senior VP Engineering, MarkLogic, will discuss the concept of Big Data Applications and walk through three in-production implementations of Big Data Applications in action. These applications include how LexisNexis built a next-generate search application, how a major financial institution simplified its technology infrastructure for managing complex derivative trades, and how a major movie studio implemented an Enterprise Data Layer for access to all of their content across multiple silos.\nThis session is sponsored by MarkLogic\n", "\nThis talk uses Mike Loukides\u2019 essay on the evolution of data products (from \u201covert\u201d to \u201ccovert\u201d) as a springboard. The dichotomy between power-user interfaces and \u201cjust tell me which app to buy\u201d is rhetorically powerful and conceptually seductive. But it\u2019s more useful to think about data services in the context of an OODA loop: Is the system augmenting the user\u2019s ability to observe, orient, decide, or act.\nThe Google self-driving car is clearly acting on behalf of the user, based on a \u201cwhere do I want to go\u201d decision.\nThe notional ultimate app-recommender is deciding, based on a passively constructed orientation of what the user has relative to what\u2019s out there, and some clever algorithms that produce what the user thinks is serendipity but is actually a high degree of affinity along different axes. Having done a substantial amount of targeting and profiling work in analytics, my sense is that there are usually implicit meta-axes of affinity which make it fairly easy to proposition someone with what seems like an out-of-the-blue option (whether a physical/digital good or a message/idea), but actually maps very closely to some affinities that cross categories. This happens in politics all the time \u2013 the art there is not only modeling the meta-axes (which evolve) but ultimately expressing those axes in a way that customers (e.g. innumerate politicians and media) can understand. E.g. \u201csoccer moms\u201d as political shorthand. The apparel business does this as well, by creating data-driven narratives around types of customers, and even giving human names to those profiles \u2013 because it\u2019s all about selling the customer an item that she doesn\u2019t currently own, but that speaks to her sense of personal style. As Jean Cocteau said, \u201cStyle is a simple way of saying complicated things.\u201d When you get into a realm as complicated as politics or fashion, there\u2019s a lot of meta-modeling you have to do in order to provide decision support (or influence) that an end-user is going to respond to and trust.\nIf you walk back the OODA loop to \u201cOrient,\u201d you get a lot more of what Loukides calls overt data products, because these these products are provisioned at a point in the knowledge chain where a decision has not only not been made, but it often isn\u2019t clear what type of decision should be made. There is a strategic process that has to happen before that decision is reached, and therefore the most important thing to do is orient, so that you can decide, \u201cDo I even need to buy a new camera?\u201d vs. \u201cWhat kind of camera should I buy?\u201d Orientation is where you discover that the new models are coming out in four months, and that nothing on sale right now is going to substantially improve on your camera\u2019s performance as much as the new models that are coming out in four months. Of course, if your strategic question was made explicit and the criteria mapped out, you could advance that to a more \u201ccovert\u201d decision support capability. But framing a decision is a reflective process, often a non-standardized one (i.e. the decision about \u201cdo I even need to buy a camera\u201d varies based on budget, interest in photography, technophilia etc. Orientation support isn\u2019t just the provision of data \u2013 it actually helps that what-type-of-decision process along. So it\u2019d be difficult or undesirable to dispense with entirely. That said, orientation support is mostly useful when you have to figure out what kind of decision to make, and most people don\u2019t have to figure out what kind of decision to make, most of the time. Hence Loukides\u2019 argument for more \u201ccovert\u201d decision support capabilities, vs. tools that simply orient a user in what\u2019s out there.\nWalking the loop all the way back to \u201cObserve,\u201d this is where you actually do want to see all the data (although you may want to see it visualized vs. listed), because if you\u2019re in a situation where you\u2019re not even sure of what context in which to orient yourself, you need to observe anything that might be salient. Essentially, this is the domain of difficult, complicated problem-solving, deep investigation, and scientific inquiry in complex systems where not all relevant variables are known. This is an episode of House: something is horribly wrong with a patient, and not only is there no diagnosis but it\u2019s not even clear what kind of diagnostic context is appropriate. Is it an auto-immune reaction? An environmental/toxicity issue? A congenital problem, or the fact that one medication is suppressing the symptoms of a hidden disease? Or a combination of any or all of these factors? In situations like this, all kinds of evidence has to be gathered and examined in detail, and you need to be able to generate hypotheses along the lines of \u201cif I do X, then Y should happen. Oh, Z happened in a completely different physiological system? Why would that happen, and which of my hypotheses would Z disprove?\u201d It\u2019s the evidence you weren\u2019t looking for, because it\u2019s part of a different context, that solves the puzzle. Or it\u2019s the reversal of figure ground, where the problem is what\u2019s absent vs. what\u2019s present. And all of that requires lots of (very expensive) observation.\nHouse is basically a contextualization exercise, and contextualization problems are like crack for me, which is why House is my favorite TV show.\nIt\u2019s a useful exercise for anyone who has a data-driven capability (or is building a capability) to game out how that capability would best serve in an observation vs. orientation vs. decision vs. action capacity, and where delivering on one part of the OODA loop reduces functionality in another part of the loop, i.e. lots of information is great for observation, not so great when you have to make a quick purchasing decision. A reflective process that enables new kinds of question-asking is great in an investigative context. Not so great in a pacemaker.\nThe OODA loop provides a powerful, broadly useful and transportable tool for product strategy and design.\n", "\nThese days users won\u2019t tolerate slow applications. More often than not, the database is the bottleneck in the application. To solve this many people add a caching tier like memcache on top of their database. This has been extremely successful but also creates some difficult challenges for developers such as mapping SQL data to key-value pairs, consistency problems and transactional integrity. When you reach a certain size you may also need to shard your database, leading to even more complexity.\nVMware vFabric SQLFire gives you the speed and scale you need in a substantially simpler way. SQLFire is a memory-optimized and horizontally-scalable distributed SQL database. Because SQLFire is memory oriented you get the speed and low latency that users demand, while using a real SQL interface. SQLFire is horizontally scalable, so if you need more capacity you just add more nodes and data is automatically rebalanced. Instead of sharding, SQLFire automatically partitions data across nodes in the distributed database. SQLFire even supports replication across datacenters, so users anywhere on the globe can enjoy the same fast experience.\nStop by to learn more how SQLFire gives high performance without all the complexity.\nThis session is sponsored by VMware\n", "\nEquipment Requirements: If you wish to participate directly in the hands-on tutorials then please bring along:\nMac or Windows laptop with at least 2GB RAM\n\n\nVMWare player installed\n\nOracle VM VirtualBox\n\nThe big data world is extremely chaotic based on technology in its infancy. Learn how to tame this chaos, integrate it within your existing data environments (RDBMS, analytic databases, applications), manage the workflow, orchestrate jobs, improve productivity and make using big data technologies accessible to a much wider spectrum of developers, analysts and data scientists. Learn how you can actually leverage Hadoop and NoSQL stores via an intuitive, graphical big data IDE \u2013 eliminating the need for deep developer skills such as Hadoop MapReduce, Pig scripting, or NoSQL queries.\nIn this hand-on tutorial you will learn:\n\nIntroduction to basic data integration concepts.\nHow to use Kettle\u2019s visual interfaces to input, output, manipulate and report on data, and orchestrate jobs, for the leading Hadoop distros including Apache Hadoop, Cloudera and MapR, as well as NoSQL databases such as Cassandra, HBase and MongoDB.\nHow to use a visual alternative to writing Hadoop MapReduce, Pig scripts, and NoSQL queries.\nHow to build hybrid systems that integrate big data environments with data marts or warehouses for high-performance interactive data discovery and visualization.\nHow to use built-in agile BI capabilities to rapidly report, visualize and explore your big data.\n\nEQUIPMENT REQUIREMENTS\nIf you wish to participate directly in the hands-on tutorials then please bring along:\n\nMac or Windows laptop with at least 2GB RAM\nVMWare player installed: http://www.vmware.com/products/player/\n\nThis session is sponsored by Pentaho\n", "\nIn an increasingly mobile world, we are each generating tons of geo-tagged data. Photo uploads to Instagram, tweets, Foursquare check-ins, local searches, and even real-time public-transportation feeds are commonplace. The companies that gather this data make a lot of it freely available. The people who work for these companies have many opportunities to learn from this data. But in order to learn, we must first figure out what questions to ask. Visualization is a tool that helps us think of questions and begin to answer them.\n\n\nThere are 3 different major ways to think about geodata:\n\t\nOver time\nAggregated spatially (e.g. by county)\nAggregated by density (e.g. heatmap)\n\nAdditionally, creating tools that allow users to explore data on multiple scales (i.e. zoom) is important, but adds complexity: you have to find a tile source and perhaps even render your data to tiles.\nChoice of projection is key. Most of us grew up with the Mercator projection, but an equal-area projection is often a better choice.\nI will take one data set and walk through visualizing it using the 3 approaches described above.\nThe first example will use Processing and Tile Mill to generate a zoomable animated map, playing back a month worth of data. I\u2019ll show how to render the map to a movie for easy distribution.\nThe second example will use d3.js to show the same data at a county level in a chloropleth map. I\u2019ll discuss color schemes and interaction, and compare what can be done with d3.js to Fathom\u2019s Stats of the Union project.\nThe last example will talk about how to make a heatmap with millions of data points.\n", "\nModCloth.com is an online clothing, accessories, and decor retailer with a focus on independent and vintage-inspired fashion. The fashion industry has slowly become more accessible to the end customer beyond its traditional enclave of celebrities, magazine editors and the fashionistas, and companies such as ModCloth are using technology to push the trend further.\nModCloth creates an engaging platform and leverages social channels to cultivate a community driven shopping experience. ModCloth is also letting users curate the types of pieces they\u2019d like to see featured on the site through its program Be the Buyer. If the item receives enough votes, the site will produce and sell the item.\nIn this session we will cover how ModCloth has leveraged this rich user interaction to cater to their demands\n\nMining of user\u2019s purchase patterns and site interaction to determine what styles, colors, etc are trending\n User interactions on loves, reviews and restock notifications help predict demand\nHow we pick the product from the Be the Buyer\n\n", "\nAttendees: We will be using Revolution R Enterprise 5.0.1 during the tutorial.  Download a 90-day evaluation copy of Revolution R Enterprise software as well as the scripts and other auxiliary material.  The file STRATA2012scripts.zip contains 40 R scripts that form the basis of the tutorial.  The auxiliary material is in two files:\n\npdfs.zip contains some optional reference material \nmovies.zip contains a data used in a data base example. It is not essential for the tutorial but is included for completeness.\n\nThe R scripts were tested on R 2.13.2. However, there should be no problem running them on R 2.14.1. The scripts also require a number of packages to be loaded. The primary package is rattle. When rattle loads, it also loads most of the other packages that will be needed.  Additionally, near the top of each script required packages will be identified by line of code similar to library(ggplot2). CRAN packages can be dound and downloaded here.\nOnly 5 of the 40 scripts require Revolution R Enterprise. All of the others can be run from open source R which you may download directly from CRAN.\nThis tutorial will enable anyone with some programming experience to begin analyzing data with the R programming language\nSyllabus\n\nWhere did R come from?\nWhat makes R different from other statistical software?\nData structures in R\n\t\nReading and writing data sets\nManipulating Data\n\n\nBasic statistics in R\n\t\nExploratory Data Analysis\nMultiple Regression\nLogistic Regression\n\n\nData mining in R\n\t\nCluster analysis\nClassification algorithms\n\n\nWorking with Big Data\n\t\nChallenges\nExtensions to R for big data\n\n\nWhere to go from here?\n\t\nThe R community\nResources for learning R\nGetting help\n\n\n", "\nOur presentation will cover the nascent fusion of automatically-collected live Digital Records of sports Events (DREs) with Augmented Reality (AR), primarily for television broadcast.\nAR has long been used to in broadcast sports to show elements of the event that are otherwise difficult to see \u2013 the canonical examples are the virtual yellow \u201c1st and 10\u201d line for American Football and ESPNs KZone\u2122 strike zone graphics.  Similarly, sports leagues and teams have historically collected large amounts of data on events, often expending huge amounts of manual effort to do so.  Our talk will discuss the evolution of data-driven AR graphics and the systems that make them possible.  We\u2019ll focus on systems for automating the collection of huge amounts of event data/metadata, such as the race car tracking technology used by NASCAR and the MLB\u2019s PitchFX\u2122 ball tracking system.  We provide a rubric for thinking about classes of sports event data that encompasses scoring, event and action semantics metadata, and participant motion.\nWe\u2019ll briefly discuss the history of these sports data collection technologies, and then take a deeper look at how the current first generation of automated systems are being leveraged for increasingly sophisticated analyses and visualizations, often via AR, but also through virtual worlds renderings from viewpoints unavailable or impossible from broadcast cameras. The remainder of the talk will examine two case studies highlighting the interplay between rich, live sports data and augmented reality visualization.\nThe first case study will describe one of the first of the next-gen digital records systems to come online and track players \u2013 Sportvision\u2019s FieldFX\u2122 system for baseball.  Although exceeding difficult to collect, the availability of robust player motion data promises to revolutionize areas such as coaching and scouting performance analysis, fantasy sports and wagering, broadcast TV graphics and commentary, and sports medicine. We\u2019ll show examples of some potential applications, and also cover data quality challenges in some detail, in order to examine the impact that these challenges have on the applications using the data.\n\n\nThe second case study will examine the rise of automated DRE collection as an answer to that nagging question about AR \u2013 \u2018what sort of things do people want to see that way?\u2019  Many of the latest wave of AR startups are banking huge amounts of venture money that the answer is in user-generated or crowd-sourced content.  While this may end up being true for some consumer-focused mobile applications, our experience in the notoriously tight-fisted rights and monetization environment of sports has led directly to the requirement to create owned, curated data sources.  This came about from four realizations that we think are more generally applicable to AR businesses\u2026\n\t\nCool looking isn\u2019t a business, even in sports.\nIt must be best shown in context, over video, or it won\u2019t be shown at all.\nThe ability to technically execute AR is no longer a barrier to entry.  Cutting edge visualization will only seem amazing for the next six seconds.  \nWe established impossibly high quality expectations, and now the whole industry has to live with them.\n\n", "\nRhadoop is an open source project spearheaded by Revolution Analytics to grant data scientists access to Hadoop\u2019s scalability from their favorite language, R.  RHadoop is comprised of three packages.\n\nrhdfs provides file level manipulation for HDFS, the Hadoop file system\nrhbase provides access to HBASE, the hadoop database\nrmr allows to write mapreduce programs in R\n\nrmr allows R developers to program in the mapreduce framework, and to all developers provides an alternative way to implement mapreduce programs that strikes a delicate compromise betwen power and usability. It allows to write general mapreduce programs, offering the full power and ecosystem of an existing, established programming language. It doesn\u2019t force you to replace the R interpreter with a special run-time\u2014it is just a library. You can write logistic regression in half a page and even understand it. It feels and behaves almost like the usual R iteration and aggregation primitives. It is comprised of a handful of functions with a modest number of arguments and sensible defaults that combine in many useful ways. But there is no way to prove that an API works: one can only show examples of what it allows to do and we will do that covering a few from machine learning and statistics. Finally, we will discuss how to get involved.\n", "\nIt\u2019s arguably the case that failure, in any form, is the most significant reason that distributed systems are difficult. Without the possibility of failure many traditional challenges in distributed computing, such as replication or leader election, become much simpler, much more performant, or both. But failure, of machines, processes and people, remains an unavoidable reality.\nIn this talk I want to demonstrate three things. First, why failure makes distributed systems design hard. Second, why understanding the root cause of a failure or outage is vital to operators of large distributed systems. Third, why doing that root cause analysis is itself difficult \u2013 because of the problems with understanding causality in distributed systems \u2013 and how we\u2019ve had some success at Cloudera treating it as a big-data problem.\nI\u2019ll explain the role that failure plays in distributed systems design quickly, by showing how complex operations become trivially simple when the possibility of failure is removed.\nI\u2019ll motivate the problem of root-cause analysis by showing how bugs can be diagnosed after the fact, and repeat behaviour avoided, once we know what caused an incident, supported by anonymised examples that we have seen at Cloudera.\nThe key to understanding failures is knowing what event caused what \u2013 the causal relationship between incidents. Unfortunately, Hadoop and other systems do a poor job of sharing causal relationships, and doing so in general is fundamentally hard due to the lack of perfectly synchronised clocks.\nIn lieu of knowing the causal relationships between components, we have to try and infer them from correlations that we see between disparate signals, from log files to user actions to operating system monitoring. This data is readily available, but huge! The challenge is to have this data help us in forming hypotheses about causal links, which we can then validate. This can be cast as a big data analysis problem of searching for the most likely causal relationships between millions of seemingly independent events. I\u2019ll show two ways we can attack this problem: by visualisation and by algorithm.\nFinally I\u2019ll show how the community can help this effort, by building tracing tools that make some causal relationships explicit, and therefore drastically cut down the amount of searching we have to do.\nConclusion:\n", "\nI am frequently asked for advice about using data visualization to solve communication problems that are better served through improved information architecture. A nicely formatted bar chart won\u2019t rescue you from a poorly planned user interface. When designing meaningful data experiences it\u2019s essential to understand the problems your users are trying to solve.\nIn this case, I was asked to take a look at a global data-delivery platform with a number of issues.\nHow do we appeal to a broad cross-section of business users?\nHow do we surface information to our clients in a useful way?\nHow do we facilitate action, beyond information sharing?\nHow do we measure success?\nA user-centered approach allowed us to weave together a more meaningful experience for our business users and usability testing revealed helpful insights about how information sharing and data analysis flows within large organizations.\nData visualization is a powerful tool for revealing simple answers to complex questions, but context is key. User-centered design methods ensure that your audience receives the information they need in a usable and actionable way. Data visualization and user experience practices are not mutually exclusive. They work best when they work together.\n", "\nOnce social media and web companies discovered Hadoop as the good enough solution for any data analytics problem that did not fit into mysql, Hadoop was on a rapid rise on the financial industry. The reasons the financial industry is adopting Hadoop very fast are very different than in other industries. Banks typically are not engineering driven organizations and terms like agile development, shared root key or cron tab scheduling are no go\u2019s in a bank but standard around Hadoop.\nThis entertaining talk for bankers and other financial services managers with technical experience or engineers discusses four business intelligence platform deployments on Hadoop:\n1.    Long-term storage and analytics of transactions and the huge cost saves Hadoop can provide;\n2.    Identifying cross and up sell opportunities by analyzing web log files in combination with customer profiles;\n3.    Value-at-risk analytics; and\n4.    Understanding the SLA issues and identifying problems in a thousands-of-nodes, big services oriented architecture.\nThis session discusses the different use cases and the challenges to overcome in building and using BI on Hadoop.\n", "\nGetting training data for a recommender system is easy: if users clicked it, it\u2019s a positive \u2013 if they didn\u2019t, it\u2019s a negative.\n\u2026 Or is it? You\u2019ve probably learned an algorithm to run on top of your existing algorithm, now and every time you re-train. And what do you do when the data product you\u2019re building doesn\u2019t have any users yet? Do you really launch with random results, hand label 50K examples, or ask a Turker to pretend they\u2019re User #1337?\nUnlike having a better algorithm, having better training data can improve your results by orders of magnitude. Yet training data generation is often an afterthought\u2014a footnote in a formula-filled publication.\nIn this talk, we use examples from production recommender systems to bring training data to the forefront: from overcoming presentation bias to the art of crowdsourcing subjective judgments to creative data exhaust exploitation and feature creation.\n", "\nMapReduce, Hadoop, and other \u201cNoSQL\u201d big data approaches open opportunities for data scientists in every industry to develop new data-driven applications for digital marketing optimization and social network analysis through the power of iterative, big data analysis. But what about the business user or analyst? How can they unlock insights through standard business intelligence (BI) tools or SQL access? The challenge with emerging big data technologies is finding staff with the specialized skill sets of the data scientist to implement and use these solutions. Business leaders and enterprise architects struggle to understand, implement, and integrate these big data technologies with their existing business processes and IT investments and provide value to the business.\nThis session will explore a new class of analytic platforms and technologies such as SQL-MapReduce\u00ae which bring the science of data to the art of business. By fusing standard business intelligence and analytics with next-generation data processing techniques such as MapReduce, big data analysis is no longer just in the hands of the few data science or MapReduce specialists in an organization! You\u2019ll learn how business users can easily access, explore, and iterate their analysis of big data to unlock deeper sights. See example applications with digital marketing optimization, fraud detection and prevention, social network and relationship analysis, and more.\nThis session is sponsored by Teradata Aster\n", "\nThe whole world streams data. From your car to your phone, your website to your supply chain, everything\u2019s generating a flood of real-time information. There are patterns in this data \u2013 if you know what to look for. There are signals in the noise \u2013 if you know how to listen. The challenge is not only to analyze Big Data, but also to process it, and to respond to it, in real time.\nHow do businesses navigate through this ocean of information effortlessly, immersed in it, seeing, hearing, and responding to it moment to moment? To marinate in the data, we need tools born of 3D animation. And to work with it and find the best solutions, we need the kinds of gameplay mechanics pioneered by massively multiplayer gaming.\nIn this session, business agility expert Michael Hugos will present examples from his work in applying immersive animation techniques and gaming dynamics, and discuss how they can address the challenges of consuming \u2013 and responding to \u2013 the data deluge, turning information overload into business advantage.\n", "\nBig data analytics are in the hockey stick phase of media buzz, and there is pressure to buy these technologies just because everyone else seems to be doing it (or at least, that\u2019s what we read in the business magazines), or because all the cool kids are doing it. But when you have to justify a business decision to invest hundreds of thousands or millions or tens of millions of dollars in IT and services, that\u2019s not enough. This presentation lays out some clear, concrete gating conditions for when it makes sense to pull the trigger on big data initiatives, and how they should be procured, depending on the use case, the data assets, and the resources available.\n", "\nAttendees of this session will learn, among other things, how to handle component failures in a complex distributed system. The cloud monitoring team uses geographic redundancy and isolation along with an innovative build process to create a system where failures can be quickly detected and addressed by the team.\nThey can also expect to learn how to understand and cope with the relational/non-relational impedance. There are data modeling anti-patterns that are easy to fall prey to when coming from a relational background, and the right approaches are often not intuitive.\nFinally, attendees will hear on how to make open source work. Many open source projects suffer from poor documentation and support, or companies that offer support at exorbitant prices. Understanding how open source communities work and employing engineers familiar with open source software makes it easier to leverage these projects.\n"], "2013": ["\nModern enterprises are performing complex analyses on increasingly large data sets to drive business decisions. Root cause analysis from system logs, social media analytics for customer retention, new customer acquisition and digital marketing are rapidly gaining importance. These tasks consist of three major analytic phases: text analytics / entity resolution, structured data processing, and predictive modeling. Traditionally these phases have been handled by a combination of custom code and separate systems such as ETL engines, relational databases, and statistical packages. However, the size of the datasets involved in these modern applications make it prohibitively expensive to move data across different specialized systems for analysis. This has resulted in the need for a single infrastructure that is sufficiently flexible to handle all these workloads.  At IBM, we have embedded innovative analytics capabilities into our big data products to support each of these phases.  By attending this insightful session, you will learn first-hand how advanced analytics are enabling modern enterprises to deal with big data challenges.\nThis session is sponsored by IBM\n", "\r\n\t\t  Tuesday, February 26 \u2013 Thursday, February 28\nExpo Hall\n\nSponsored by:\n\n\n\n\n\n\nSensors are the future of distributed data. General-purpose computing is dissipating out into the environment and becoming increasingly invisible and embedded into our lives. We will soon begin to move in a sea of data, our movements tracked and our environments measured and adjusted to our preferences, without need for direct intervention. \nAt the Strata Conference in New York last year, we gave attendees a taste of the super-connected world that\u2019s ahead of all of us by instrumenting the conference environment with basic off-the-shelf sensors and mesh networking. At the Strata Conference in Santa Clara this February, we will observe and report on the conference once again, with more sensors, real-time visualization, and some new interactive features for attendees. \nResults will be presented from the keynote stage. Data visualizations will be shown in real time on a monitor in the Data Visualization Showcase. And attendees are welcome to stop by the Data Sensing Lab booth in the Expo Hall throughout the conference, to view some sample sensor motes up close and talk with the experts. From hardware and software to data analysis and visualization, the project will give attendees a taste of their lives in a more measured and quantified world. \n\n\n\nDistributed Network Data\nby Alasdair Allan and Kipp Bradford\nHardware hacking for data scientists.\n\nEbook: $15.99\n\nPre-Order Print: $19.99\n\nBuy from oreilly.com\n\n\nSee video about the Data Sensing Lab\n", "\nThis is expected to be the worst flu outbreak in recent years, but this type of epidemic can be recognized and tracked earlier with the right tools. Microsoft partner, Ascribe, is using Microsoft\u2019s Big Data solutions to turn emergencies into actionable data by gathering handwritten notes from Emergency Departments, using Natural language Processing, analyzing this data and then monitoring to proactively assess & predict outbreaks of infectious diseases. This data is also being used to provide better levels of care at the personal level by feeding the data back down to patient screens in the Emergency Department.\nLearn more about how the UK is changing the face of healthcare with these tools from Microsoft to analyze unstructured data \u2013 SQL Server 2012, Excel 2013, Azure and specifically HDInsight Service and SharePoint. Now you know.\nThis session is sponsored by Microsoft\n", "\nMath is proof. Given enough data\u2014and today, we have plenty\u2014we can know. \u201cThe right information in the right place just changes your life,\u201d said Stewart Brand. But your life won\u2019t change by itself. Bruce Mau defines design as \u201cthe human capacity to plan and produce desired outcomes.\u201d Math informs; design compels. Which matters more? A well-designed collection of flawed information\u2014or an opaque, hard-to-parse, but unerringly accurate model? From mobile handsets to social policy, we need both good math and good design. Which is more critical?\nThe Great Debate series returns to Strata. In this Oxford-style debate, two opposing teams take opposing positions. We poll the audience, and the teams try to sway opinions. It\u2019ll be a fast-paced, sometimes irreverent look at some of the core challenges of putting data to work.\n", "\nStrata Program Chairs, Edd Dumbill and Alistair Croll, welcome you to the first day of keynotes.\n", "\nProgram Chairs, Edd Dumbill and Alistair Croll, welcome you to the second day of keynotes.\n", "\nVisualizing  patterns, relationships and anomalies in multi-sourced data is challenging when the number of records continues to grow exponentially.  Many traditional methods of visualization for business intelligence and reporting aggregate the results into units which are easily computed and displayed. However, these visuals often fail to reveal the true patterns of the underlying data. If we used more pixels in our visualizations we could see millions of data representations on a screen.\nComputing a value for each pixel is a difficult challenge because it often requires whole set aggregates and table scans. There are so many pixels that it quickly becomes efficient to render these visuals on the server.\nIn this session we will cover some of our favorite examples of big data visualizations and show you how rendering every pixel can be an effective tool for analysis. We will also discuss the tools and architectural considerations necessary to create these visualizations for your organization.\n", "\nWith the growth in volume and velocity of data, businesses need a scalable solution alongside batch processing to process events on the fly and provide real time insights. The canonical use case is log file analysis, for which Hadoop is well suited as part of a batch process. However, there is a need to understand and detect the events contained within the data and make that insight available to the organization so that it can react rapidly. The open-source real-time platform Storm provides a distributed and fault-tolerant solution to analyze streams of data, run complex distributed computations, and integrate with scalable distributed storage such as Cassandra.\nIn this session, we will describe how we used Storm to analyze network data to detect causes of network performance degradation. We will describe:\n- Mechanisms for ingestion of network data\n- Processing data at high velocity from multiple streams\n- Deriving appropriate real time analytical models\n- Creating Storm topologies to execute the analytics and events processing in real time\n", "\nBooth Crawl is back at Strata, immediately after the afternoon sessions on Wednesday. \nQuench your thirst with vendor-hosted libations and snacks while you check out all the cool stuff in the Expo Hall. It\u2019s also a great time to meet and mingle with fellow attendees and sightings of Strata speakers and authors.\n\nSponsored by:\n                             \n\n\n", "\nData visualization and data stores can be thought of as two sides of the same coin. Visualizations, if expressed correctly, can drive the database queries needed to tell the story in question. Today, in practice, visualization is the result of a programmer writing the\nnecessary data queries, munging the data, and writing the rendering code. This requires a sophisticated programming skill set across data querying, data transformation and integration, and user interface design.\nThis session will discuss an approach to developing a language that describes both visualization and database queries. We will explore how recursive application of the split-apply-combine principle popularized by Hadley Wickham (http://www.jstatsoft.org/v40/i01) can be used to describe both the structure of a visualization and a database query to serve that visualization. This approach benefits and empowers both developers and business users, especially as a way to quickly explore new data sets.\n", "\nUsing the web to view pages one at a time is like using a personal computer to only store recipes. Just as most people immensely underestimated the full potential of personal computers 30 years ago, most people have yet to recognize the full potential of web data today.\nThe web is the largest collection of information in human history and growing at a staggering rate \u2013 the estimated number of webpages grew from 26 million in 1998 to 1 billion in 2000 and hit the 1 trillion mark back in 2008. Modern big data tools and technological advances lowering compute costs have made it possible to gain extremely valuable insight from large scale analysis of web data, but until recently few people had access to the data. Now tools like Grep the Web, indexing services that provide raw access to web data, and repositories of open data make it possible for almost anyone to extract knowledge from the web that was previously only available to large search engine companies.\nThis presentation will explain the various tools available and share examples of powerful insights gained from analysis of web data, including results from recent research projects. You will hear firsthand from Blekko CTO Greg Lindahl about their motivation for building the free Grep the Web service, how they built it, and what they have learned from it. Kevin Burton, Spinn3r CEO, co-inventor of RSS, and long time Apache contributor will share his experiences from a decade of analyzing web content. Lisa Green brings her insight into the relationship between data accessibility and innovation. The three speakers will discuss practical applications of web data analysis that you can incorporate into your research or products.\n", "\nAttendees will be shown how to leverage crowdsourcing to commit various types of fraud such as Tax Refund Fraud, Disaster-Relief Fraud, Benefits Fraud (Medicaid Fraud, Medicare Fraud, Welfare Fraud and Section 8 Housing Assistance Fraud). We will look at which opportunities and segments of the population are easy targets for large scale identity fraud what insights are gained from this analysis and what can be done on the ground to narrow the window of opportunity for these types of operations and schemes.\nAttendees will also be given a solution to how to detect and combat these various types of fraud. This session will discuss the challenge of resolving identity from billions of identity fragments and why the bigger data, the better the resolution. The session will outline what this means for Business and why Consumers should care about how to best generate a meaningful public data identity for themselves.\nWe will look at some of the interesting high level identity statistics and insights we have gained from large scale data analysis and exploration using our massive data assets.\nThe discussion will include questions like: What is the average age a person starts to establish an identity footprint (with and without immigrant identities)?\nLastly, we will cover the one surprising Big Data perspective that differentiates fake or synthetic identities from real identities.  We\u2019ll give you a hint: it\u2019s the one thing that identity thieves forget to do when they create a fake or synthetic identity, and this one thing, helps distinguish real identities from fake identities.\n", "\nI will discuss how a wearable sensing platform, the Sociometic Badge, allows us to measure and analyze human behavior in real-world settings at an unprecedented level of detail.  The Sociometric Badge uses social signals derived from vocal features, body motion, and relative location to capture individual and collective patterns of behavior.  Building on these features, we\u2019re able to precisely measure face-to-face interaction, conversational time, physical proximity to other people, and physical activity levels.  Through a series of studies, we show how we can use the badges to measure persuasiveness, interest levels, and social support.  Finally, we detail how we have used the badges in real companies to transform organizational design and deepen our scientific understanding of management.\n", "\nIn this talk we will look at how operations research can be married with domain knowledge and machine learning to solve \u201cthe Birchbox problem\u201d.\nAt the core of Birchbox\u2019s subscription service lies an optimization problem. Birchbox is a discovery platform for products. Birchbox subscribers receive boxes of samples, which they can try, learn about via our editorial content, and purchase full-size versions of in our store.\nEach month we design a number of boxes (e.g., 30) and then assign the boxes to our customers by encapsulating our business rules and objectives into an optimization problem. There are a variety of hard constraints that we consider, including those derived from customer profile attributes and sampling history (e.g. must not receive the same sample twice). We use soft-constraints, which express how suitable we think each box is for each customer, to incorporate domain knowledge and statistical inference. The optimization problem is to ensure that each customer is assigned a box, subject to supply constraints for each box, while maximizing the \u201creward\u201d expressed by the soft-constraints.  The notion of \u201creward\u201d encoded in the soft-contrainsts can be quite complex, incorporating infered notions of customer happiness, likelihood of conversion to full-size sale, effect on customer retention etc.\nWe will discuss how we tackle the box allocation problem in practice including how we formulate it as a variant of the well-known transportation problem (resulting in a matrix of more than a trillion entries), our plans for future work, and how the techniques we use can be applied to other domains.\n", "\nMore people are on social networks like Twitter, Facebook, Tumblr, and Pinterest, or using social elements \u2013 like comments \u2013 of web sites. With rising participation and changes in the ways people communicate, comes increased risk and challenge.\nSpam and security problems have gotten significantly more complicated since Paul Graham\u2019s seminal \u201cA Plan for Spam.\u201d The emergence of unguarded social media channels, easy to operate botnets, and cheap labor that can very quickly produce new strains of an attack have rapidly altered the landscape of problems that today\u2019s systems have to deal with.\nMachine learning is a proven approach to solving classification problems, and is often cited as a great fit to solving spam and security challenges as well. However, most spam and security solutions that incorporate machine learning are unable to keep up with the rapidly changing nature of the abuse domain and wear down quickly in effectiveness. As a result, the field is rife with failed machine learning projects.  Considering the complexity of a machine learning implementation and the margins for error involved, one has to wonder, does machine learning stand a chance as a long-term approach to abuse and security problems?\nMachine learning, when an active adversary is involved, requires a novel approach to feature engineering, training and classification. A simple set of design patterns surrounding the building blocks of machine learning and emerging big data frameworks like Hadoop hold the key to a more secure, abuse free world of communication. This session will discuss how to apply machine learning to create an effective, long-term solution to solving the issue of online abuse and security.\n", "\nMachine learning (ML) algorithms are the cornerstone of Data Science. Traditional ML tools do not scale to large data sets, leading to the next generation tools such as Mahout. However, these tools have implemented only simpler ML algorithms such as linear regression and k-means clustering using Hadoop Map-Reduce (MR) for scaling \u2013 this is because of the natural misfit of Hadoop MR for iterative ML algorithms.\nThe third generation tools such as HaLoop, Twister, Apache Hama and GraphLab provide implementations of iterative ML algorithms and some (GraphLab) even go beyond the MR paradigm. The aim of this session is to give a comprehensive and demonstrative view of the third generation ML tools by taking real-life use cases from Retail and/(or) Healthcare domains.\n", "\nAs an ecommerce site with more than 800,000 different sellers, Etsy is particularly interested in understanding how shoppers find the items they seek.  Part of this understanding involves attributing successful purchases to specific features on the site.  This attribution model allows us to compare and refine Etsy\u2019s features, but also provides valuable signals for A/B testing, search quality, and recommenders.  However, the path to a successful handmade purchase often involves multiple features over the course of several visits.\nThis talk will discuss the challenges of funnel analysis at Etsy and the corresponding deficiencies of several widely used web analytics tools.  We\u2019ll then dive into our event sequence matching tool, which we\u2019ve successfully applied to hundreds of millions of visits in a single Hadoop job and is widely used across our big data stack.  Finally, we\u2019ll take a look at some of our applications of the tool and compare it to related work.\n", "\nElectronic discovery has transformed the way cases are litigated. Gone are the days of manual review, where litigators spent days poring over emails, messages, and documents. Today\u2019s e-discovery technologies mine through vast troves of information, looking for the needle in the proverbial haystack that will blow a case wide open. But these same litigators are now skeptical of the results machines can deliver, and are reluctant to rely on them without peering under the covers. To address this, we need to help lawyers think about numbers and certainty. We need to ease them into concepts like false positives and false negatives. Without overwhelming them, we need to show them how the whole process works. This is, above all, a design problem.\nIn this session, we\u2019ll describe how we designed a novel technology-assisted review platform for Daegis. It blends modern machine learning and Hadoop Map/Reduce algorithms with minimal input from human reviewers. Importantly, our process continuously estimates the precision and recall of the review process and displays relevant information in a dashboard designed for statistical novices (e.g., attorneys) and savants (e.g., expert witnesses) alike.\n", "\nOpower, the global leader in the field of energy information and analysis, works with 80 utility companies worldwide to give families context, insights, and advice about how to save energy. With access to an unprecedented (and still growing) amount of energy data \u2013 currently drawn from 50 million US homes \u2013 Opower is uncovering unique trends in how people are using energy at home.\nThe recent advent of smart meters \u2013 which record electricity usage every hour, and in some cases every 15 minutes \u2013 has brought about an unprecedented proliferation of energy information. Correspondingly, there is now more interest than ever in capturing insights from detailed energy data, and determining how these insights can empower individual households and communities to make their energy consumption more environmentally and financially sustainable.\nThis presentation will:\n\nProvide background on the structure and scale of household energy consumption data that populates Opower\u2019s unique dataset\n\n\nDiscuss how Opower uses Hadoop and other big-data tools to analyze energy data across space and time\n\n\nHighlight how to use storytelling, visualizations, and data journalism to make the results of big-data analysis interesting and accessible to the public (drawing upon Opower\u2019s big-data energy blog, which has been featured in Scientific American, LA Times, and Gizmodo)\n\n\nPresent the findings of Opower\u2019s most popular cross-disciplinary analyses (combining energy, weather, and demographic data), which have earned large audiences and heightened public interest in big-data analytics\n\n\nExplore Opower\u2019s big-data investigations and data journalism approach on a range of topics, such as: Who uses more electricity \u2014 Gmail users or Yahoo users? What do hour-by-hour energy usage patterns look like on a 100-degree day, and how does it affect the power grid? How does energy usage change during the Super Bowl? By answering these questions, Opower is leveraging big data to raise public awareness and boost citizen engagement around energy, the environment, and big data itself.\n\n", "\nOpposites attract and that\u2019s the case with Hadoop and analytic databases. Both have a role to play in your Big Data projects. This session explores the various approaches to cementing the bond between Hadoop to your analytic database, how SAP customers are integrating Hadoop into BI and advanced analytic environments, and why you\u2019ll want to do that too.\nThis session is sponsored by SAP\n", "\nCatch up with fellow geeks and old friends at the Strata Data Drinkup, a big data happy hour where you can relax and swap insights and stories at the conclusion of another busy day at Strata Conference. It\u2019s a time to hoist a glass and toast the camaraderie born of a shared  experience.\nThe party is open to data geeks of all stripes \u2013 whether you\u2019re in big data, search, analytics (predictive or otherwise), machine learning, or quants. Come join us and share a libation with your fellow conference-goers: we\u2019ll help you get into the mood for socializing with a free drink ticket for each registered attendee.\nCheers!\n", "\nCode for America fellows have been tackling not only the promise of\ndata in America\u2019s cities, but the reality of the challenges, for the\npast two years. In February 2013, six new fellows will be working on\nour hardest problem yet: using data to unclog the criminal justice\nsystem in Louisville and New York City. If the public sector can\ninnovate using data, and results benefit us all.\n", "\nData Science has created quite the movement in the data world, yet confusion between data science and analytics still remain across the enterprise. Rather than approach the subject talking about semantic differences between the two, we will discuss the topics as they relate to solving problems, how businesses are approaching them and what you can start doing with data science.\nThis session is sponsored by Accenture\n", "\nThis tutorial begins by reviewing human perception and our ability to decode graphical information.  It continues by:\n\nRanking elementary graphical perception tasks to identify those that we do the best.\n\n\nShowing the limitations of many common graphical constructions.\n\n\nDemonstrating newer, more effective graphical forms developed on the basis of the ranking.\n\n\nProviding general principles for creating effective graphs, as well as metrics on the quality of graphs with a checklist of possible defects.\n\n\nCommenting on software packages that produce graphs.\n\n\nComparing the same data using different graph forms so the audience can see how understanding depends on the graphical construction used.\n\n\nDiscussing Trellis Display (a framework for the visualization of multivariate data) and other innovative methods for presenting more than two variables.\n\n\nPresenting Mosaic Plots and other graphical methods for categorical data.\n\n\n\nSince scales (the rulers along which we graph the data) have a profound effect on our interpretation of graphs, the section on general principles contains a detailed discussion of scales including:\n\t\nTo include or not to include zero?\n\n\nWhen do logarithmic scales improve clarity?\n\n\nWhat are breaks in scales and how should they be used?\n\n\nAre two scales better than one?  How can we distinguish between informative and deceptive double axes?\n\n\nCan a scale \u201chide\u201d data?  How can this be avoided?\n\nThe tutorial concludes with before and after examples that are appropriate for the audience.\nParticipants will learn to:\n\nPresent data more effectively in all media.\n\n\nUnderstand principles of effective simple graphs to build upon when creating interactive or dynamic displays.\n\n\nBecome more critical and analytical when viewing graphs.\n\n\nRecognize misleading and deceptive graphs.\n\n", "\nWhen creating graphics in the browser there\u2019s no consensus on what language or API should be used. From markup languages like SVG to OpenGL based APIs like WebGL, the browser provides several different ways for creating visualizations. In this talk I\u2019ll show some data visualizations for the web I worked on for different projects and for Twitter as well, and show what standards and libraries were used to create them. I\u2019ll dissect each example showing what standards were used not only for rendering but also for data loading and interaction, describing the flow of each visualization.\n", "\nThe award-winning Guardian Datablog is now one of the most popular data journalism destinations in the world today, with 1.5 million users a month accessing raw data, visualisations and analysis from the site every month.\nBut how does it work data to day?\nThis hands-on session will show how a dataset turns into a story, the narrative process the Guardian\u2019s team goes through, the tools used and the lessons learned.\n", "\nData science may seem like all machine learning hotness, but the reality is less flattering and for many counting the number of minor annoyances would result in an overflow unless you explicitly cast that variable as double-precision.\n\u2022    Are those data little-endian or big-endian?\n\u2022    Were those date created using Unix time, the Windows version of Excel, or the OS X version?\n\u2022    Is that governmental PDF machine-readable? No? Good luck. \n\u2022    Does the EU follow daylight savings time? What about Arizona?\n\u2022    Is that lat/lng inside that geographic boundary or not?\n\u2022    This JSON object has nonsense attribute names\u2026 which data am I supposed to use?\n\u2022    Oh, there are non-printing UNICODE characters in this file breaking my munging algorithm. I\u2019m glad I only spent 7 hours hunting that bug down.\n\u2022    These data have a non-explicit ID numbering system based on their ordinal position. Great. But does their system use zero-based or one-based indexing?\nYou\u2019re all just bits of information, just play together nicely!\nIn this presentation I\u2019ll outline some of the common data munging frustrations that I\u2019ve encountered, and offer advice on how people can spot and avoid them, covering some specific examples from my own work such as I hint at above.\n", "\nIn this session we\u2019ll first discuss our experience extending Hadoop development to new platforms & languages and then discuss our experiments and experiences building supporting developer tools and plugins for those platforms.  First, we\u2019ll take a hands on approach to showing our experiments and successes extending Hadoop to languages such as Javascript and .NET with LINQ.  Second, we\u2019ll walk through some of the developer & developer ops tools and plugins we\u2019ve experimented with in an effort to simplify life for the Hadoop developer across both on premises and cloud-based projects.\n", "\nEndorsements are a one-click system to recognize someone for their skills and expertise on LinkedIn, the largest professional online social network.  This is one of the latest \u201cdata features\u201d in LinkedIn\u2019s portfolio, and the endorsement ecosystem generates a large graph of reputation signals and viral user activity.\nUnderneath this feature, there are several interesting and difficult data questions:\n1. How do you automatically create a taxonomy of skills in the professional context?\n2. How do you disambiguate between different contexts of skills? For instance, \u201csearch\u201d could mean information retrieval, search & seizure, search & rescue, among others.\n3. How can you leverage data to determine someone\u2019s authoritativeness in a skill?\n4. How do you use that authoritativeness to recommend people to endorse?\n5. How do you optimize a complex large scale machine learning system for viral growth & engagement?\nIn this talk, we\u2019ll examine the practical aspects of building a data feature like Endorsements. We\u2019ll talk about marrying product design and data, deep diving into several of the lessons we\u2019ve learned along the way - all using skills & endorsements as an empirical case study.  We\u2019ll include technical detail on our approaches and how we combine crowdsourcing, machine learning, and large scale distributed systems to recommend topics to users.\nWe\u2019ll also show interesting results on how members are using the endorsements feature and how it\u2019s spread across the network.\n", "\nThis tutorial follows up on our previous tutorial introducing BDAS, the open-source Berkeley Data Analytics Stack. Attendees will use Spark and Shark, two key components of BDAS, to manipulate a real-world Wikipedia dataset. We will provide each audience member access to a Spark/Shark cluster running on EC2 and walk them through hands-on coding examples. Attendees will learn how to use the Spark and Shark command line interfaces to perform ad-hoc analysis that take advantage of Spark\u2019s in-memory caching primitives to speed up queries by an order of magnitude. The lessons will include practice using Spark\u2019s Java and Scala language APIs and Shark\u2019s SQL-like query language. Additionally, users will write a more complex standalone Spark program that uses a parallel machine learning algorithm (K-Means Clustering) to analyze a real Wikipedia dataset.\n", "\nLearn how LivePerson and Zoomdata perform stream processing and visualization on mobile devices of structured site traffic and unstructured chat data in real-time for business decision making.  Technologies include Kafka, Storm, and d3.js for visualization on mobile devices.  Byron Ellis, Data Scientist for LivePerson will join Justin Langseth of Zoomdata to discuss and demonstrate the solution.\n", "\nAt Strata 2012 in New York, we discussed the hazards of curbing big data inferences by defining a new category of thoughtcrime. After all, acting on thoughts might constitute a crime, but thoughts, in isolation, cannot be criminal. It\u2019s time to go deeper. Let\u2019s create and evaluate a predictive criminal model that highlights where the sensitivities lie, both technically and ethically.\nOver the last decade, Intelius has built a people-centric big data platform \u2014 what we call the inome platform. We\u2019ll use it and our criminal database of several hundred million U.S. criminal records to train and evaluate a predictive criminal model. As part of this talk, we\u2019ll release the model and some of the inome machine-learning scaffolding code.\nWhat makes big data so scary is that, for the first time, we are leveraging huge data mines to make inferences outside the wisdom of our own minds. Is it possible to predict, with meaningful recall and acceptable precision, who might commit a crime? We\u2019ll showcase our model\u2019s shortcomings due to inescapable precision/recall trade-offs \u2014 false negatives miss criminals while false positives indict the innocent. And even if we could build a perfect predictor, does a powerful government have the right to use it and eclipse free will?\n", "\nThe Cloudera Impala project is for the first time making scalable parallel database technology, which is the underpinning of Google\u2019s Dremel as well as that of commercial analytic DBMSs, available to the Hadoop community. With Impala, the Hadoop community now has an open-sourced codebase that allows users to issue low-latency queries to data stored in HDFS and Apache HBase using familiar SQL operators.\nThis talk will start out with an overview of Impala from the user\u2019s perspective, followed by a presentation of Impala\u2019s architecture and implementation, and will conclude with a comparison of Impala with Apache Hive, commercial MapReduce alternatives, and traditional data warehouse infrastructure.\n", "\nThis tutorial-the first of a two-part series-will provide an introduction to BDAS, the Berkeley Data Analytics Stack. BDAS is an open source, next-generation data analytics stack under development at the UC Berkeley AMPLab whose current components include Spark, Shark and Mesos. We will start by covering Spark, a high-speed cluster computing system compatible with Hadoop that can outperform it by up to 100x thanks to its ability to perform computations in memory. Spark provides concise, high-level APIs in both Scala and Java, and is in use at Foursquare, Conviva, Klout, Quantifind, and other companies. We will provide an overview of the Spark architecture, typical data analytics workflows (e.g., loading data from HDFS into memory and interactively querying it), and how users are applying Spark. In addition, we will also introduce Shark, a port of Apache Hive onto Spark that is compatible with existing Hive warehouses and queries. Shark can answer HiveQL queries up to 100x faster than Hive without modification to the data and queries, and is also open source as part of BDAS.\n", "\nMedical level information is becoming increasingly accessible to consumers, and Rest Devices is looking to transform the way that people \u2013 and, more specifically, parents \u2013 view, interpret, analyze, and use biometric information. By utilizing medical grade sensing technology and retooling it for the consumers, Rest Devices is creating new standards, platforms, and data libraries for respiration, sleep, and physiological development. Accompanying our reliance on data, however, is a deep need for design, as the user interface for consumer hardware has become increasingly significant.\nThis talk will discuss Rest Devices proprietary low-cost sensor technology, its use of and vision for big biometric data, and the need for design integration in all facets of product development, be it software or hardware.\n", "\nExperts agree that the age of Big Data has finally arrived, fueled largely by growth of the Hadoop ecosystem. However, real world success stories \u2013 with tangible business benefits \u2013 are hard to find. HP has enabled many customers on to Big Data platforms through a combination of world class solutions, services and consulting. In this session you will hear about five such stories, complete with solution details and business results, from the technical teams that packaged and delivered these solutions. Learn how HP has established itself as the premier Big Data vendor with a solid portfolio of turnkey solutions that can be deployed faster than ever, while keeping acquisition and operational costs down. Learn more at hp.com/go/information.\nThis session is sponsored by HP\n", "\nDistributed systems make for tricky diagnosis problems. Which component is at fault? Is it the network, the machine, the process, or, even worse, some emergent complex behavior?  We\u2019ll discuss in depth the tools most commonly used to figure out these thorny issues:\nLogs. The perennial go to. We\u2019ll point out unexpected ways to increase logging verbosity at runtime and how to use time-based search across many machines to look at inconsistencies.\nWeb-based consoles. Must-haves for the modern distributed system, debug servlets can provide surprising insights.  We\u2019ll point out what information Hadoop exposes and present easily embeddable open source consoles.\nLinux has many goodies in /proc, as well as several tools to look at performance (perf, top). We\u2019ll cover old standbys like lsof and tcpdump, too.\nTracing, when it\u2019s available, can be an incredible tool.  HBase recently introduced some, and several other libraries (e.g., Zipkin) have emerged.\nSince Hadoop runs on the JVM, we\u2019ll discuss unusual things you can do there, like enabling verbose GC after the fact, poor man\u2019s profiling, and extracting JMX metrics from even the most recalcitrant of processes.\nFinally, we\u2019ll bring it all together by doing some simple stats to find and visualize outliers.\nThe talk will be illustrated by examples from open source systems (especially Hadoop).\n", "\nMost stable systems rely on feedback \u2013 from central heating and cruise\ncontrol in cars to industrial plants and biological mechanisms. In\nthis introduction, I will argue that feedback can also be suitable for\ntypical problems in enterprise software development: whenever\ndesigning software to control a process or deliver according to a\nplan, feedback control should be considered as an option. Examples\ninclude the control of supply chains and the fulfillment of orders,\nresource management of buffers and queues, access to shared resources\n(data centers!), and more.\nThis introductory talk will explain what feedback is and how to apply\nit in typical enterprise architecture situations. I will describe how\nsystems involving feedback are different and what challenges to watch\nout for. Specifically, improperly designed feedback loops are\nsusceptible to undesirable oscillations or instabilities, and I will\ndiscuss techniques to recognize and prevent them.\nNo previous knowledge of feedback or control theory is required.\n", "\nThe DocGraph referral graph encoded using the National Provider Identifier (NPI).\nThe core NPI database is already open data, including location and type of doctor. But most of the data is held at the state level by the various medical boards.\nSo we crowdfunded the purchase of this data, which makes the referral graph far deeper. As a result it is possible to use the graph to analyize how doctor come to trust and work with one another.\nWe cannot be sure, but we think this is the largest open data set, that uses real names that details social relationships. Of course Facebook et al have much larger social graphs, but they only release a sliver at a time. If you are a data scientist interested in graph big data, this is a pretty deep well.\n", "\nApache Hadoop is an innovative emerging technology causing CIOs to rethink their data architecture \u2013 making this an exciting time to be a \u201cbig data\u201d technologist. The data warehouse like no other technology is most often compared to Hadoop. This raises some important questions. Will Hadoop replace the data warehouse? When is each more appropriate for different applications? How do they integrate? This tag-team presentation brings leaders in both Apache Hadoop and data warehousing \u2013 Hortonworks and Teradata \u2013 on the stage, to answer these questions by sharing their vision for the future of big data management and analytics. Even more, practical advice and examples will be shared for extracting the most value from your big data with a unified data architecture.\n", "\nIf you\u2019re an evil genius with a yen for data science, what are your possible attack vectors?\nIf you\u2019re a good guy and want to protect your data from unscrupulous competitors, what are your counter-attacks? How effective are they?\nThis talk will focus on data science attack vectors that can be exploited for commercial, not military, gain. We\u2019ll look at black-hat internet marketing techniques, and then explain how data science can be used to scale and refine these black-hat techniques.\nThese techniques include:\n\nSpam, article spinning, and content generation.\nUnethical scraping and spidering.\nCATPCHA breaking, voting rings, and sockpuppetry.\n\nHow can these techniques be scaled and refined using data science? Which of these black hat techniques are effective for nefarious large entities, and which of them rely upon being too small to notice (security through obscurity)? How can you prevent against your competitors using these techniques?\nWe\u2019ll conclude with speculation on how grey-markets might emerge that all black hats to launder and resell their wares.\n", "\nMicrosoft believes getting value from data should be available to anyone and has a strategy to consumerize Big Data. Come hear how we\u2019re making this real for the Halo 4 team and how they have used big data technologies to enhance the Halo 4 gaming experience.\nThis keynote is sponsored by Microsoft\n", "\nJoin us for a model strategy for data journalism, managing big data, in a country without open data. We will present 3 Case Study for Argentina: \nDatabase transportation subsidies; Analysis of data from health insurance payments and Monitoring of efficiency fund a loan from the World Bank  to the Argentine government\nWhat were the boundary conditions?\nNo Law on Access to Information\nNo Open Data\nNo Open Government\nWe will show how we could design a news apps with data scattered in various government websites, repositories, databases, some semi-open, others closed, mostly in PDF.\nWe will also discuss what strategies were used with simple and free tools: Create a Data Team, join to hackathons, evangelization in the newsroom from personal training and by projects and promote a the fist Latinamerican Data Fest, in spanish, to open and data mining.\nFinally, we will give a preview of our work and discuss the impact of the Data Team\u2019s work in a country with no open data.\n", "\nHBase is one of the new NoSQL data stores that have come up in the recent years and has been gaining popularity at a fast pace. It is a true open source implementation of the Google Bigtable, and is a part of the Hadoop ecosystem. HBase is known to scale to 100s of nodes easily, providing fast random access to terabytes and petabytes of data. This tutorial is to get you started in the world of HBase so you can build a scalable application of your own.\n\n\nWe\u2019ll accomplish this by covering the following aspects:\n\t\nThe background of HBase as a datastore\nSetting up HBase on a *nix machine (bring your laptop with Linux on it. Macs work just as well and so does a remote EC2 instance)\nBuild a real world application from ground up, thinking about scale from the get go. The application will use HBase as it\u2019s backend store.\nHBase data model and schema design basics\nOverview of HBase internals and design and what that means for your application\n\n\n\nWe\u2019ll also touch upon the following topics:\n\t\nProduction deployment strategies and things to think about\nTuning HBase for different workloads and performance testing your system\n\nAt the end of the tutorial, you\u2019ll have a good understanding of how to effectively use HBase as the backend store for your application.\n", "\nBig data analytics have blossomed because there is too much information for people to manually process. It is probably less widely known that as of the last couple of years, the average person could not understand the majority of the world\u2019s data, even if there wasn\u2019t so much of it: the majority of all the world\u2019s data is now in non-English unstructured text and speech, in around 5000 languages.\nUnstructured text, especially outside of English, is one of the least understood areas of information processing. English is a linguistic outlier in a number of ways: strictness in word order; simplicity of prefixing/suffixing; size of vocabulary; standardization of spelling. From search engines to social media firehoses, English-centric design decisions have left us unprepared to accurately process information from more typical languages. For example, assumptions about the utility of \u2018keywords\u2019 might not apply when every word in a language has several prefixes, suffixes or spelling variations, or when we cannot accurately break a sentence into words in the first place. These examples are problems that exist in languages that account for about 25% of the world\u2019s digital data.\nThis talk will give a high-level overview of how languages vary, what kinds of communications are the most widely used in different languages, what current language processing technologies can (and cannot) achieve, and how we can process and visualize this information at scale.\n", "\nData analytics and the mathematical programming languages that support them are changing the world but they are also suffering from growing pains. Technical computing languages that have been around for decades have been slow to adopt new compiler technologies such as JIT, optional type indications, and others.\nJulia was introduced to solve these limitations. Julia is built on a solid foundation of JIT compiling, parallelism, and a mathematical syntax that will look familiar to users of other mathematical languages. Julia supports a rich type system. Scalars, vectors, arrays, tuples, composite types and several others can be defined in Julia. Julia is designed for technical computing and supports a fully remote cloud computing mode. Julia is free, open source, and library-friendly. The core Julia language is licensed under the MIT free software license.\nThis session will demonstrate some of the special capabilities of Julia and give you the tools you need to get started using this exciting technical computing language.\nThis session will review examples where forecasting models and other predictive analytic tools are being used by people who aren\u2019t analytics experts in the real world. The session will then provide recommendations for creating interactive predictive analytic tools for non-data scientists and a review of technologies and tools that can assist with development.\n", "\nUnderstanding data to make meaningful business decisions is a difficult task. Big data has made this task even more challenging. Traditional user models for analytic applications break under the strain of ever increasing data volumes and unstructured data formats. Whether the user is a business user or an IT user, with today\u2019s data complexity, there are a number of design principles that are key to achieving success. Hear how to approach product designing for today\u2019s data challenges and meet new user expectations for fast and timely insights at scale.\n", "\nThe Apache Hadoop platform has become a critical aspect of modern enterprise data warehousing strategies for many commercial, government and research organizations looking to analyze their data for actionable intelligence. Early adopters have already begun to leverage Hadoop to solve truly big challenges, like how to make energy use more sustainable and how to more effectively treat diseases. However, despite Hadoop\u2019s potential to power big competitive advantages, many traditional enterprises have not yet adopted this powerful technology. Now in it\u2019s sixth year, Hadoop is reaching maturity and is poised to more aggressively take increasingly wider workloads within mainstream enterprises.\nIn this session, Cloudera VP of Product Charles Zedlewski will discuss how Hadoop is evolving to address enterprise compliance, security and performance concerns, as well as how it complements legacy enterprise data warehousing infrastructure to deliver new levels of insight, performance and efficiency. The discussion will offer insights on new advancements to the platform and enterprise successes based on real-world use cases .\nThis session is sponsored by Cloudera\n", "\nThis talk discusses the broad design considerations necessary for effective visualizations. Attendees will learn what\u2019s required for a visualization to be successful, gain insight for critically evaluating visualizations they encounter, and come away with new ways to think about the visualization design process.\n\n\nTo be most effective, a visualization must have, in this order: \n\t\nPurpose (Why are we creating this visualization? Who is it for?)\nContent (What data matters? What relationships matter?)\nStructure (How do we best reveal those data and relationships?)\nFormatting (How does it look & feel? How will it be consumed?)\n\nThis talk will define these four pillars, reveal why they must be selected in this order, and discuss the importance and impact each has on your visualization.\n", "\nThe Apache Drill project is gaining tremendous interest in the industry. Apache Drill is a large-scale interactive query system to analyze large datasets with an inexpensive and flexible solution. Drill is another big data system that, like Hadoop, was inspired by a Google white paper.\nIn this session you will hear an overview of Apache Drill, details on its architecture, and gain an understanding of the vision and product roadmap.\n", "\nIn our relentless pursuit to map the world, Google has spent years building the world\u2019s largest GIS. We process petabytes of satellite and aerial imagery, render millions of map tiles on the fly daily for serving to maps.google.com and through our Maps API, and process and publish millions of user-submitted corrections to our basemap.\nBeyond the services that create our maps, we\u2019ve also purpose-built the world\u2019s largest geospatial global decision engine, capable of on-demand multi-spectral imagery and vector analysis\u2014with a goal of permitting our users to make informed decisions from Google\u2019s mapping data.\nWe live in an age of massive georeferenced datasets, prodigious spatially (e.g., global), temporally (e.g., spanning decades), and physically (e.g., consuming petabytes of storage).  And yet despite this abundance of data, the ability to store, manage, and process it as Google does has typically been the domain of large institutions, requiring specialist training and a frequently massive investment in storage and computing power.\nThis talk will focus on several key technologies powering Google\u2019s geospatial cloud computing platform, on how we\u2019ve begun exposing these geospatial services for anyone to use, and will showcase several examples of customers using the services to help them solve their real-world problems.\n", "\nThe jury is out: running analytics using MapReduce, Hive, or Impala is extremely slow with very little support for SQL.  With data piling up and analytics poised to be the leading value creation engine for big data apps, there is growing demand from companies to predict, prescribe, and prevent. ParAccel runs analytic queries 100x faster than Hive with much deeper SQL Support. Hear how companies are using analytic platforms for fast, interactive analysis on big data.\nThis session is sponsored by ParAccel\n", "\nHow must big companies evolve in order to realize big value from big data? Investing in data, technology and data scientists is just a first step. In this topic, Jeanne Harris, (Global Managing Director of Information Technology Research at the Accenture Institute for High Performance and co-author of the business bestselling books Competing on Analytics and Analytics at Work) will explain how leaders who want to consistently out-smart and out-execute their rivals are also transforming their decision processes, culture, organization and managerial skills.\n", "\nIn many modern web and big data applications the data arrives in a streaming fashion and needs to be processed on the fly. In these applications, the data is usually too large to fit in main memory, and the computations need to be done incrementally upon arrival of new pieces of data. To do these computations, sketches of the data are designed and used that not only take a small amount of memory but also allow for fast queries and updates on the fly. Such sketches are useful both in applications run on a single machine and for applications run on distributed systems such as Twitter Storm. We will present the techniques used to design these sketches and also provide a number of examples, such as frequent item-sets (used for e.g. retail product recommendations), clustering, and heavy hitters (used for e.g. fraud and intrusion detection), etc. to clarify the techniques and how to apply them.\n", "\nSuppose you have a real-world big data problem before you, and you want to use machine learning (ML) to solve it.  Which ML method(s) should you use?  How does the fact that the dataset is big affect your choices?  Drawing on two decades of experience in ML on big data, I will highlight a few key principles that can be distilled from the thousands of theoretical and experimental results in the research literature surrounding such questions.  These will be illustrated through a handful of real-world ML success stories, where best-in-class results were achieved, including difficult examples in medical diagnosis, direct marketing, financial services, and astronomy.\n", "\nProper forecasting is crucial to Facebook\u2019s business operations. We rely on reliable forecasts in deciding how many people to hire and where, in making multi-million dollar investment decisions, and determining the allocation of servers and bandwidth.\nI have led Facebook\u2019s demand and advertising forecasting effort for the past year and previous to Facebook developed several forecasting models including a model of future international student enrollments, named `ISAFM`, for ICG consulting group: http://www.illuminateconsultinggroup.biz/isafm/.\nI have found in my experience that forecasting is a less accessible topic than most other statistical inference techniques. It is often done in an ad-hoc manner and best practices are commonly violated. Too often big decisions are made without any forecasting at all.\nForecasting has also been given the reputations as being more of an art than a science and as only being appropriately applied by domain experts. In reality, every data analyst should have forecasting in their toolkit to know what the data tells us about the probabilities of future scenarios. Domain expertise is crucial for appropriately interpreting forecasting models, but this should not present a barrier to entry for forecasting newbies. Instead, forecasters should gather all the information the data contains and seek out as much domain knowledge as possible to appropriately interpret the output.\nIn this session, I will cover the basic of forecasting to make forecasting accessible to every data analyst. I plan to cover:\nBasic techniques\nModel Selection\nModel Validation\nDifference between forecasting and prediction\nBest Practices\nTips\nSimple examples in R\nI may also cover forecasting lessons learned at Facebook and Facebook\u2019s forecasting process.\n", "\nBirds of a Feather (BoF) sessions provide networking opportunities for attendees interested in the same projects and topics to meet in a casual setting. BoFs topics are created by attendees and can be about individual projects or broader topics (best practices, open data, standards).\nBoFs at Strata will happen during lunch on Wednesday, February 27 and Thursday, February 28, where lunch is served.\nStop by the BoF signup board near Registration to check out the topics or start your own.\n", "\nThere\u2019s no question.  NoSQL is behind much of the innovation in data.   Strata wouldn\u2019t be happening were it not for the Cloudera\u2019s, the Google\u2019s, and the Facebook\u2019s of  the industry investing in technologies that allowed certain classes of applications to scale beyond a single data-center.   Analytics at a massive scale for genetics, social networking, political campaigns, intelligence, and finance has been a driving force behind foundational technologies like Hadoop and HBase.   The emergence of application development patterns that emphasize documents over data has created a market for products like MongoDB and Couchbase.   It is clear that the center of attention in this new revolution of data is NoSQL.\nYet, the majority of the market for databases continues to reside with the relational database.  A popular refrain in favor of NoSQL is the idea that relational databases cannot scale \u2013 there are fundamental limits to the model.  While it is true that traditional relational databases pose seemingly intractable problems that limit horizontal scale, that hasn\u2019t stopped the best and brightest from giving it a try.\nThis talk challenges this assumption that relational databases cannot scale and explores recent trends that may invalidate the now-popular idea that relational databases are incompatible with \u201cBig Data\u201d due to fundamental limits.  Topics discussed include:\nTrends in Software\n\nHow are framework adapting to NoSQL?  (Django, Rails, Spring) Where is the \u201cmedian\u201d and what is the \u201cstandard deviation\u201d for developers?\nWhat is the real problem with mapping Objects to \u201cTables\u201d? What are the emerging solutions?\nOld Wine, New Bottles: What new problems emerge when you map objects to documents?\n\nTrends in Hardware\n\nWhat does the next generation of Memory technology mean for the relational database?  \nHow are query optimizers adapting to a disk-less reality?\nHow improvements in wide-area networks drive scalability for relational databases.\nHow are query optimizers and execution plans being adapted to make use of GPUs.\n\nWWGD: What Would Google Do?\n\nHow is Google a canary in the coal mine for Big Data?\nWhat does Spanner mean in the context of Google\u2019s evolving approach to persistence?\nWhat concepts and features does Google Spanner share with other existing databases?\nWhen can we expect to see the first global-scale, relational database in the cloud?\n\n", "\nData Science is an emerging field in industry, yet not well-defined as an academic discipline (or even in industry for that matter). I proposed the \u201cIntroduction to Data Science\u201d course at Columbia in March, 2012. This was the first course at Columbia that had the term \u201cData Science\u201d in the title. I had three primary motivations:\n1) Bringing industry to students: I wanted to give students an education in what it\u2019s like to be a data scientist in industry and give them some of the skills data scientists have. This is based on my experience as a lead analyst on the Google+ Data Science team. But I didn\u2019t want to limit them to only my way of seeing the world, so each week, guest speakers from the NYC tech community came to teach the class.\n2) I wanted to think more deeply about the science of data science: Data Science has the potential to be a deep and profound research discipline impacting all aspects of our lives. Columbia University and Mayor Bloomberg announced the Institute for Data Sciences and Engineering in July, 2012. This course created an opportunity to develop the theory of Data Science and to formalize it as a legitimate science.\n3) Personal Challenge: I kept hearing from data scientists in industry that you can\u2019t teach data science in a classroom or university setting and I took that on as a challenge. I wanted to test the hypothesis that it was possible to train awesome data scientists in the classroom.\nIn February 2013, 2 months will have passed since the class ended. I\u2019ll be able to reflect on how the class went, how I thought about the curriculum, how I engaged the NYC tech community to be involved in the class, who the students were, whether I had impact on them, etc.\n", "\nData on consumers is a valuable commodity in today\u2019s marketplace.  Privacy and marketing laws as to a company\u2019s obligations on data collection, use, and disclosure are changing rapidly, and failing to remain up to date and having a solid understanding of how the laws affect personal data assets can often result in media exposes, regulatory investigations, Congressional hearings, lawsuits, and risky contract terms with business partners. Time invested in \u201cprivacy by design\u201d compliance at the outset is the best insurance policy to avoid what could be expensive and embarrassing situations down the road.\nThis proposed session, presented by privacy and advertising law attorney Alysa Z. Hutnik of Kelley Drye & Warren LLP, will include the following topics of discussion:\n\u2014Case studies of privacy enforcement and litigant activity \n\u2014Privacy Law landscape\n\u2014Practical tips to avoid becoming a target of scrutiny for privacy business practices\nStrata Conference attendees will want to attend this session because managing the legal boundaries is critical to managing personal data responsibly. But with this serious potential for costly privacy mishaps, where should companies begin? Ms. Hutnik will break down the compliance obligations and provide actionable steps to help companies avoid legal scrutiny at the start.\nMs. Hutnik is a previous speaker at the 2012 Where Conference Location Marketing Boot Camp where she spoke on the top privacy issues for location companies. She will bring this experience plus a diversity of perspective to the Strata conference, based on her work in an industry comprised primarily of men. She is ranked nationally in the area of privacy and data security law by such directories as Chambers USA and US Legal 500 for the past several years, and was also ranked as a top global expert on data and privacy protection in Computerworld\u2019s list of best privacy advisers. More information about her professional background is included below.\n", "\nThe success of Hadoop has led many enterprises to the same common vision: a central repository of data, inexpensive, efficient, and scalable. This \u201cData Reservoir\u201d eliminates data silos and feeds all business analytic applications. Driving this vision is an ideal that business users should have access to all the data they need, without limitation, and be able to explore and combine internal and external datasets to answer questions that have not been possible or practical in the past.\nImplementation of the vision has been slow and in many cases has not lived up to expectations. The promise of this new technology has been overhyped. IT teams have been left managing yet another data source leading back to the existing data silos we were trying to avoid.\nThis talk will examine the pitfalls we have found in working with customers and the requirements that, when followed have led to success. Including:\n\nPerformance\nSecurity\nReliability\nSelf-Service Access\nMetadata Consistency and Management\n\nWe will present a reference architecture for the Hadoop Data Reservoir and discuss ecosystem technologies required to make it a reality.\nThis session is sponsored by Platfora\n", "\nRecommendation Systems, though commonly seen, are among the challenging aspects of Predictive Analytics and Big Data Platforms/Solutions. Various Algorithms are used to build the products to which Recommendation Systems are applied, and there is always buzz in regard to the various schemes employed.\nThis session brings more science to the art, laying the foundation of starting off a Recommendation Platform at a higher ground rather than from scratch. We\u2019ll discuss the ML algorithms that should be applied to various use cases, and the architecture of a Recommendation Platform on Hadoop\u2014including ETL/Data Pipeline, Feature Generation, Model Generation, Recommendation Server, A/B testing, Reporting and Tracking. We\u2019ll also review the various patterns of recommendation use cases and the ML algorithms that apply to them.\n", "\nIf you are signed up for this tutorial, you will need to be prepared with the following, before you arrive onsite:\n\nUp-to-date R installation (also an IDE, like R-Studio)\n\n\nPython 2.7 with a package manager (pip or easy_install) and the following packages: numpy, scipy, scikits-learn, pandas, matplotlib, ipython\n\n\nGit and a Github account\n\n\nA text editor\n\n\nExcel (not mandatory, but may be useful)\n\n\nMatlab/Octave (not mandatory, but may be useful)\n\n\nOther modeling software or languages you prefer (for contest use)\n\nThis tutorial will target people with basic programming experience to introduce them to the end-to-end analysis of predictive data problems.  We will cover the topics in a largely language-agnostic way, drawing on examples from R and Python.  The tutorial is comprised of four sections.  The last of section will be a hands-on Kaggle competition in which participants can experience firsthand the joys of creating a model and the sorrows of overfitting:\n\nIdentifying a problem (30 min)\n\n- Identifying opportunities to collect data\n- Reading data into a useful format\n- Understanding limitations in the data\n\nPerforming the analysis (45 min)\n\n- Feature extraction\n- Basic prediction methods\n- Cross validation\n- Numerical ways to assess performance\n\nVisualizing the solution (30 min)\n\n- Showing the results\n- Telling a story through visualization\n\nHands-on, for-fun contest (75 min)\n\n", "\nIn this talk, we describe using Redis, an open source, in-memory key/value store, to capture large volumes of data from numerous remote sources while also allowing real-time monitoring and analytics in a production environment. With this approach, we are able to capture a high volume of continuous data from numerous remote environmental sensors while consistently querying our database for real time monitoring and analytics. NoSQL data store implementations have gained mass attention in recent years, in part due to the flexibility and efficiency of working with high volumes of data without the overhead of traditional structured database systems. As these technologies mature, their potential application to big data collection and analytics continues to grow.\nThe two biggest I/O bottlenecks in distributed applications are network I/O and filesystem I/O. Our particular use case required large numbers of remote client deployments in which we had no control over network infrastructure, and thus was always at the mercy of network latency. However, we found we were able to successfully combat filesystem I/O by leveraging an in-memory database for incoming data, enabling us to scale data collection rates to meet requirements. Our use case required not only large volumes of data to be continually collected, but also required data to be collected in small 300 byte chunks, resulting in a proportionally large number of inserts per second. We chose Redis, a popular open source, in-memory key/value store, to collect all incoming data from our various remote deployments. We found that Redis was not only capable of handling a data collection at a high rate, but was also able to serve real-time analytics queries simultaneously, a task that traditional databases proved incapable of when tested within our system.\nIn implementing such a system, there are some important factors to consider, e.g.:\n\nHow should your data be structured within the key/value store to maximize collection efficiency?\nWhat are the real-time analytics requirements, and how should the data be structured to most efficiently serve them?\nWhat are the data persistence needs of your system?\nWhat are the long-term scalability requirements and expectations of the system?\n\nWe will walk through our system architecture, highlighting design choices made based on the above considerations, with a specific focus on considerations that may be at odds with each other, such as designing a data model to meet both collection efficiency and real-time analytics needs. We will also present lessons learned through our production deployments and provide an introspective view of our solutions, along with proposed enhancements for future iterations and divergent requirements.\n", "\nAs an attendee of the \u201cData Wrangling with R\u201d tutorial, you should come with a laptop computer that has the latest version of R installed (currently R 2.15.2). R can be downloaded from http://cran.r-project.org/\nIf you\u2019ve never used R before, it is recommend that attendees also install the RStudio Desktop IDE, which can be downloaded from http://www.rstudio.com/ide/\nAll attendees should also install the latest versions of these packages:\nplyr (1.8)\nreshape2 (1.2.2)\nstringr (0.6.2)\nggplot2 (0.9.3)\nThese packages can be installed from within R, by opening R and running the following code in the command line. If prompted to select a mirror, choose a location near to you. You must be connected to the internet.\ninstall.packages(c(\u201cplyr\u201d, \u201creshape2\u201d, \u201cstringr\u201d, \u201cggplot2\u201d))\nPlease make sure to install all downloads BEFORE you arrive onsite.\nLearn how to wrangle data in R: from acquiring and cleaning data, to changing data formats and performing targeted, groupwise calculations. This course will emphasize the reshape2 and plyr packages.\n", "\nData does not speak for itself. While successful data science is dependent on data and modeling, visualization is also essential for communicating the patterns in the data and the context around it. Although many solutions are available to create rich interactive graphs and charts, they are generally too separated from the analysis process itself. This creates inefficiencies at the boundary and makes rapid iteration of research more difficult. Minimizing these inefficiencies is the focus of this talk. By combining agile data analysis tools with web-based visualization libraries, we can optimize the data scientist\u2019s tool chain for both exploratory research and the presentation of results.\nA proper solution needs to take into account all of the components of data analysis that happens between raw data and the final visualization.\n\nData processing \u2013 raw data must be loaded and cleaned, and then the model computations must be performed. I will use the pandas library in Python to give an overview of common data operations and research best practices. \nData reshaping \u2013 the output data along with supplemental data must be grouped, merged, and reshaped so they can be used for visualization. Fast and flexible data wrangling operations are essential for this step, and so are easy methods for converting the data into the right format such as JSON. \nData visualization \u2013 an efficient visualization toolkit must include  reusable components with configurable parameters and default settings that make sense. It must enable the data scientist to create commonly used graphs and charts without too much effort spent on plumbing and scaffolding.\n\n", "\nManaging data in Hadoop gets complex quickly \u2013 Loom is the data set management system for Hadoop that makes it easy. Loom provides tools to track the lineage and provenance of all registered HDFS data, and Activescan so that all of the critical information about data sets is collected dynamically.\nData scientists working with multiple data sets need tools for describing data, automatically tracking their activities, and making the hard work of creating finished products from raw materials easier. Loom has a fabulous UI for Hive and very good integration with the R programming language and R consoles.\nWhen a good Hive UI and integration with R are coupled with Loom\u2019s data set management capability the result is significant productivity increases for all Hadoop users, especially data scientists. We will provide a thorough demonstration Loom.\nThe team at Revelytix has delivered complex information management software for 14 years, over our past two companies to both commercial customers and the U.S. Department of Defense. This talk will show how to dramatically increase the productivity of Hadoop users.\nRevelytix announces the Early Access availability of Loom.\nThis session is sponsored by Revelytix\n", "\nBig data gives us a powerful new way to see patterns in information \u2013 but what can\u2019t we see? When does big data not tell us the whole story? This talk opens up the question of the biases we bring to big data, and how we might work beyond them.\n", "\nManaging Big Data is not for the faint of heart. Data driven organizations, especially those operating petascale workloads, know that managing and making sense of vast quantities of data requires an intrepid and innovative engineering team, and that the need to address data volume and cost efficiency is paramount. Few companies understand this imperative better than Quantcast.\nThe company, which ingests more than 40TB of new data and processes in excess of 20PB of data each day, pioneered the Quantcast File System (QFS) and released it to open source in 2012. This best of breed file system offers a high performance, more efficient alternative to HDFS for managing, storing and processing massive data clusters. Leveraging QFS as its primary data store in tandem with Apache Hadoop, Quantcast can now better support the performance demands of its multi-petabyte workload, driving greater cost efficiency and consuming only half the hard drive space.\nIn this session, Quantcast VP of Research and Development Jim Kelly will offer a comprehensive overview of the QFS platform, its genesis, and explore when it makes sense to leverage QFS, HDFS or another comparable file system.\n", "\nThe emergence of Apache Hadoop over the past few years has required organizations\nto completely rethink architectures that have been in place for decades. And with\nchanges in the underlying data fabric, come ripple effects, and often bottlenecks,\nthat impact all levels of an organization both business and technical. Discover what\nis possible for radical Hadoop performance improvements \u2013 an important capability\nallowing organizations to use data to discover and drive new top-line opportunities.\n", "\nWe will describe the BigData Top100 List initiative\u2014an new, open, community-based effort for benchmarking big data systems. The BigData Top100 list will rank big data systems according to a well-defined, audited performance metric. The benchmark also provides an accompanying efficiency metric. With \u201cbig data\u201d becoming a major force of innovation across enterprises of all sizes, new platforms for managing big data sets are being announced almost on a weekly basis with increasingly more features. Yet, there is currently a lack of any means of comparability among such platforms. While the performance of traditional database systems is well understood and measured by long-established institutions such as the Transaction Processing Performance Council, there is neither a clear definition of the performance of big data systems nor a generally agreed upon metric for comparing these systems. This session unveils a community-based effort for defining an end-to-end application-layer benchmark for big data applications, with the ability to easily adapt the benchmark specification to evolving challenges in the big data space.  We actively seek community input into this process.\n", "\nA lot of the vision pieces out there these days paint a world where decisions are made for us based on data. On the one hand, this is appealing \u2013 machines taking on tasks and simple choices I can\u2019t be bothered to make. On the other hand, it\u2019s disorienting \u2013 what is the logic underlying these decisions? How can I participate in it? Am I comfortable with decisions being made for me when I don\u2019t understand why or how?\nTo address this, we designers need to focus on the things that make us uniquely human, and allow those to illuminate a path through the challenges and opportunities of the brave new landscape built on and with Big Data. The primary role of design is to make a translation between the abstract and the human, to make connections between humans and the things they need, want and love. Sometimes that means exposing the mechanics, sometimes that means hiding the complexity. Either way, it means enabling people to make better decisions and live better lives, not to let the technology lead.\n", "\nIn our uses of Data Science to build products (namely, apps for business verticals) we encounter repeated patterns among the common use cases for Big Data. In practice, these patterns involve the integration of multiple systems and heterogeneous frameworks. We specify the business process and trade-offs for any given app, defined as a combination of many parts. That work typically crosses several different vendors and open source platforms, in contrast to the notion of \u201cOne Size Fits All\u201d. In other words, we leverage a notion of blended use cases to build Big Data apps.\nFor example, MapReduce (e.g., Hadoop) as a compute framework is rarely if ever used in isolation. Hadoop-based apps tend to consume data from multiple sources, e.g., distributed file systems, key/value stores, document collections, JDBC into relational database, S3 and other durable grids, etc. In turn, they produce results which tend to get stored elsewhere, e.g., in the common case of an API consuming from a cache layer.\nThe workflow, as an abstraction layer, is generalized as a directed, acyclic graph (DAG). \nA DAG specifies a set of endpoints, dependencies, and transformations, which tie together the many required parts and subsystems. This analysis leads toward a notion of data access patterns, akin to the design patterns leveraged in software engineering. The resulting pattern language provides a formalism for architectural recipes, best practices, code reuse, and Enterprise-scale optimizations.\nThis talk examines common use cases in Data Science, leading toward a set of data access patterns. For example, marketing funnel optimization is one such use case, which is ubiquitous in e-commerce. A formalism for workflow abstraction is proposed, then reviewed in the context of a sample app based on the Cascading open source project.\n", "\nMapReduce, Hadoop, and other \u201cNoSQL\u201d big data approaches opened opportunities for data scientists in every industry to develop new data-intensive applications. But what about the more traditional SQL users or analysts? How can they unlock insights through standard business intelligence (BI) tools or ANSI SQL access?\nIn this session, you will learn about Teradata Aster Analytics Portfolio which is the industry\u2019s largest library of pre-built MapReduce functions based on patented SQL-MapReduce\u00ae framework that extends SQL for big data analytics. We will discuss how SQL-MapReduce\u00ae based functions can be used by business analysts through familiar tools on a variety of data sources. The session will cover how native R programs can be utilized in the Teradata Aster Discovery Platform. Using a combination of SQL-MapReduce functions, we will walk through use cases of pattern analysis on retail e-commerce data sets.\nThis session is sponsored by Teradata\n", "\nMany of the services that are critical to Google\u2019s ad business have historically been backed by MySQL. We have recently migrated several of these services to F1, a new RDBMS developed at Google. F1 implements rich relational database features, including a strictly enforced schema, a powerful parallel SQL query engine, general transactions, change tracking and noti\ufb01cation, and indexing.  F1 is built on top of Spanner, a new globally-distributed synchronously-replicated storage system that scales on standard hardware in Google data centers.  Spanner supports efficient externally-consistent distributed transactions and includes useful features like non-blocking read-only transactions and multi-versioned snapshot reads.\nThe strong consistency properties of F1 and Spanner come at the cost of higher write latencies compared to MySQL. Having successfully migrated a rich customer-facing application suite at the heart of Google\u2019s ad business to F1, we will describe how we restructured schema and applications to largely hide this increased latency from external users. The distributed nature of F1 also allows it to scale easily and to support signi\ufb01cantly higher throughput for batch workloads than a traditional RDBMS.\nWith F1, we have built a novel hybrid system that combines the scalability, fault tolerance, transparent sharding, and cost bene\ufb01ts so far available only in \u201cNoSQL\u201d systems with the usability, familiarity, and transactional guarantees expected from an RDBMS.\n", "\nThere are a few setup steps I need you to do in advance. They should take a few minutes at most. You\u2019ll find the instructions here: http://thinkbig-academy.s3.amazonaws.com/Strata2013/HiveTutorial/index.html\nYou can view these instructions now. The zip file with the tutorial content will be available Friday, February 22nd. If you have any problems, post a comment to the tutorial page on the conference site: http://strataconf.com/strata2013/public/schedule/detail/26899\nSee you in Santa Clara!\nDean Wampler\nIn this hands-on tutorial, you\u2019ll learn how to use Hive for Hadoop-based data warehousing. You\u2019ll also learn some tricks of the trade and how to handle known issues.\nWriting Hive Queries\nWe\u2019ll spend most of the tutorial using a series of hands-on exercises with actual Hive queries, so you can learn by doing. We\u2019ll go over all the main features of Hive\u2019s query language, HiveQL, and how Hive works with data in Hadoop.\nAdvanced Techniques\nHive is very flexible about the formats of data files, the \u201cschema\u201d of records and so forth. We\u2019ll discuss options for customizing these and other aspects of your Hive and data cluster setup. We\u2019ll briefly examine how you can write Java user defined functions (UDFs) and other plugins that extend Hive for data formats that aren\u2019t supported natively.\nHive in the Hadoop Ecosystem\nWe\u2019ll learn Hive\u2019s place in the Hadoop ecosystem, such as how it compares to other available tools. We\u2019ll discuss installation and configuration issues that ensure the best performance and ease of use in a real production cluster. In particular, we\u2019ll discuss how to create Hive\u2019s separate \u201cmetadata\u201d store in a traditional relational database, such as MySQL. We\u2019ll offer tips on data formats and layouts that improve performance in various scenarios.\n", "\nNot sure how leverage Big Data technology in your application? This hands-on tutorial will give you on an overview of how AWS can quickly and easily enable you to start generating insights from your company\u2019s data.  After a brief overview of the AWS Big Data toolset, we\u2019ll focus on Amazon Elastic Map Reduce, showing how to easily create and customize dynamic Hadoop clusters.  Participants will leverage the Amazon Elastic Map Reduce command line tools along with Amazon Public Datasets to generate insights in the domains of social media using public twitter follow graphs, life sciences using public PubChem datasets and log analysis using sample data.  In the process participants will explore several different programming models including Hive, Pig and Hadoop Streaming.\nNo prior experience with Amazon Web Services, however the tutorials are aimed at developers.  Familiarity with Linux and command line tools will be beneficial, as will a general understanding of Hadoop and map-reduce.\n", "\nWith the need for Big Data comes the need for the apt tools to work with the data.   Data scientists  and engineers need the best tools available  to efficiently build data models and  finely tuned algorithms .  The right tools and infrastructure  to collect and store data  in a time and space  efficient manner become indispensable.  And on top of all this data collection requirements , you still need  to make sure your latency on  site is minimal.   Any mechanism put in to collect analytics data must not interfere with the performance of the site.   And yes it would be great to have a platform that let\u2019s  data scientist run AB tests with minimal effort. And above all is these how do you scale?\nStumbleUpon, the leading personalized discovery engine on the web for the last decade is in the midst of what one could call a data and information explosion. Producing over 50GBs of data a day, there is a real need to manage all this information and make it accessible to all relevant stakeholders in the company. This talk will focus on our Analytics infrastructure platforms that solve these problems and help our analysts and data scientists extract the most value out of our data.\nAt StumbleUpon we believe in free and open source software  and in this talk we ll demonstrate how state of the art open source  systems like Hadoop , Hbase , Kafka & Redis  are being used to build a world class data platform. We will talk about how we have implemented Kafka to collect logs efficiently and how data gets organized into optimized partitioned tables in Hive/HDFS, the most favored tool among analysts for adhoc querying. We will also discuss how we leverage hbase to collect millions of data points & metrics in near real time, while adding minimal latency to the site. Additionally, we will also talk about the adoption of Scala at Stumbleupon that has gone a long way in helping us build complex back end systems  in record time .  Akka\u2019s actors and remoting models have made developing concurrent systems easier and more robust  like never before.\nThis talk will elaborate on how these technologies are put to use and work harmoniously to build a big data infrastructure that is fast, scalable and most importantly user friendly.\n", "\nLocation Intelligence (LI) transforms how public health and agriculture initiatives are managed and monitored by translating big complex data from multiple sources and varying temporal and spatial scales into local, actionable insight.  Innovative tools and data techniques are utilized for comprehensive data collection, integration, and analysis to dynamically visualize information crucial for local to national logistical and strategic insight.  LI empowers national governments and global development organizations to focus on saving lives and building healthy, sustainable communities.\nInformation is the foundation of every successful national initiative whether driven by government policy makers or international aid organizations.  Location and time plus descriptive attributes are the focus of an innovative database architecture enabling the integration of critical data from many strategic sources.\nA structure to manage BIG DATA\n\nThe architecture and organization of data \u2013 by location and time\nGeneric technology stack\nData sources\nTemplates, smartphones, remote sensing, weather stations, spatial interpolations, predictive models, social data.\nFeatures like dynamic spatial and temporal aggregation, meta-data\nDemonstration of modular user interfaces\nMonitoring\nEvaluation\nWeather\nReports\nAlerts\nNotifications\n\nBusiness applications\n\nMonitor and evaluate objectives with location-based technology, track impact in real time with mobile data collection, and implement mid-course optimization. \nAssess impact on the well-being of individuals, families, and communities with local to national relevance by quantifying results and understanding \u201cwhere, when, and why\u201d.  \nManage data, massive data, to create an integrated national data asset empowered with access to the global information ecosystem.  Remotely sensed, weather, handhelds \u2013 massive data all with location and time as common attributes.\nCollaborate within and across projects, locally or globally, across people, projects and resources to share insight, recognize results, publish to the public, and engage the donors. \nAct on evidence-based decisions and early warnings using real-time location-specific information for policy guidance, operational decision support, and local impact. \nUnderstand the impact of localized weather and climate on development initiatives then target specific interventions by applying integrated analysis of local weather data content.\n\nData currently spread across many programs and sources have much greater impact if integrated into a common, context-generating infrastructure.  Contextual insight allows for better decisions, smarter investments, and predictable results within and across programs.\nAttend this conference to learn about practical, pragmatic implementation of how Location Intelligence in conjunction with big data techniques can be implemented for national development initiatives \u2013 growing the value and power of a location intelligence infrastructure for developing nations.  Data collected for one initiative is easily, but securely accessible across all initiatives to benefit overall national development such as:   Malaria and other public health campaigns, food security policies and administration, agricultural research and extension, and famine early warning systems.\n", "\nOn-the-fly aggregation with human-time (or \u201cinteractive\u201d) queries against fresh, at-the-moment data represents a growing trend.  Many newly announced systems are starting to provide interactive queries on batched data streams.  This talk will discuss how Druid allows users to have interactive queries on real-time data at scale; we feature a case study with Netflix leveraging Druid to obtain at-the-moment insight as it ingests over two terabytes per hour.\n", "\nAs Hadoop establishes itself as the data reservoir for analytics at enterprises in a number of industries, its users are demanding more from the platform and the vendors that support it. Join this session led by Greg Khairallah, Intel Hadoop Evangelist, to learn how utility companies are directing some of the most demanding requirements at Apache Hadoop while using it to deliver energy and telecommunications services at previously unimagined scale. Find out how Intel is addressing these needs by enabling Apache Hadoop to take better advantage of imminent breakthroughs in compute, storage, and networking hardware.\n", "\nRackspace Hosting is the industry leader in the Open Cloud.  Rackspace runs mission critical applications for tens of thousands of business of all sizes with a portfolio of products and services that encompasses dedicated infrastructure, private cloud and public cloud services. In this session, Natasha Gajic talks about the ACG project and how it leveraged NoSQL and Big Data technologies with OpenStack for the company\u2019s complex information needs.\nRackspace\u2019s Enterprise Business Intelligence group (EBI) was looking for a cost-effective way to support the reporting and information needs of its internal users, which include business and operations personnel. It was also looking to scale out new infrastructure in order to meet their increasing business demands, house increasing amounts of data, and customize the collection of data, while seeking a way to move away from their legacy Data Warehouse solution.  To do this, Rackspace built the Analytical Compute Grid (ACG) by using Hadoop, Cassandra and PostgreSQL with an OpenStack cloud.\n\n\nAnalytical Compute Grid (ACG) is a solution that enables Rackspace to:\n\t\n     House an ever growing set of data collected from multiple business units\n   Allow for quick collection of data\n   Rapidly scale up and down to meet fluctuating demands\n   Provision a wide variety of open sourced virtual machines\n   Utilize open source technology to move away from enterprise license fees and avoid vendor lock-in to any one particular product.\n\n\n\nThe team selected OpenStack to be the heart of the Analytic Compute Grid for the following reasons:\n\t\n   OpenStack provides a rich and robust API allowing the ACG engine to dynamic scale\n   OpenStack provides the necessary speed of provisioning and scale to rapidly create and destroy virtual machines\n   An OpenStack image contains all components necessary for a given VM to join the ACG system. ACG utilizes OpenStack images to create system VMs.\n\n\n\nOpenStack allows Rackspace to configure images that use different data stores, including:\n\t\n   Cassandra database for columnar data structures\n   PostgreSQL  for relational data structures\n   Hadoop distributed file system for large unstructured and noisy data\n\nThe ACG engine enables end users to select the best data storage technology for each information need, and provides SQL-like syntax for data retrieval via a standard JDBC interface regardless of the underlying data storage.\nCome hear about how Rackspace is using OpenStack, Big Data and NoSQL to help end users manage information and data.\nThis session is sponsored by Rackspace\n", "\nRecommendation algorithms have long been a valuable component of ecommerce.  They drive incremental revenue by helping customers find what they are looking for.   But a new shopping service is taking recommendations to the next level.  Stitch Fix, with its disruptive business model, is betting big on algorithms, technology, and domain expertise to transform the way we shop.  To Stitch Fix, recommendations represent more than incremental revenue; the company has oriented its entire business model around its ability to get relevant merchandise to its customers.   They go beyond helping customers find what they are looking for; Stitch Fix helps customers find what is right for them.  And, sometimes what\u2019s right isn\u2019t obvious until the customer has it in her hands.\nIn this talk you\u2019ll learn how Stitch Fix approaches algorithms, acquires unprecedented data, and fuses analytics with human intuition \u2013 all the necessary ingredients to make this innovative business model work. The talk will inspire those with conceptual or aspirational knowledge of algorithms by showing what is possible.  And, it will intrigue the experience data scientist by showing what has been done.\n", "\nHow do you make big data accessible, usable and valuable for everyone? And mine your data for intelligence in minutes and hours, not weeks and months? What about getting real-time insights from your data, even before you persist and replicate it? In this talk, we\u2019ll examine compelling, real-world examples that offer a blueprint for integrating big data technologies, delivering rapid visibility and insights to IT professionals, data analysts and business users, and that accelerate the adoption of big data in the enterprise.\nThis session is sponsored by Splunk\n", "\nGiven the exponential rise in data, attorneys have an obligation to meet today\u2019s Governance, Risk and Compliance (GRC) challenges and stay on top of technology in order to achieve broader institutional benefits. Moving from document-centric to entity-centric analytics is the key to gain valuable knowledge.\nNowhere is this more evident than in the Compliance and eDiscovery markets, where organizations are facing increased pressure from two groups: regulators to analyze more data and CFOs to keep legal costs down.  Therefore, by employing \u201csmart\u201d analytics, organizations can mitigate the escalating cost of eDiscovery by utilizing innovative technologies to reduce time and cost associated with manual review.\nThis presentation will explain how the combination of new entity-centric analytics can empower organizations to meet their information governance challenges and reduce the time and cost associated with discovery.\n\n\nKey topics for discussion include:\n\t\nInnovative processes and technology to meet GRC and eDiscovery requirements\n\n\nEvolution and trends in scalable unstructured text analytics\n\n\nApplication of entity-oriented analytics to cost-effectively manage compliance and   eDiscovery challenges\n\nThis session is sponsored by Digital Reasoning\n", "\nThe informatic challenges of 2013 and beyond are bigger than any one company.  This presentation provides an overview of a number of recent, successful crowd-sourced and community-driven applications that combine \u2018Big Data\u2019 approaches with Community involvement.  The speaker dives into the numbers and specific details of Factual\u2019s approach to large-scale, multi-authored data collection and aggregation, and how the company\u2019s data ethos and business positioning dictates both the shape of its technology and its vision of large-scale, collective data ecosystems.\n", "\nToday\u2019s smartphones have evolved into incredibly rich sensing and computing devices, that oh-by-the-way can also make phone calls. They are jam-packed with on-board sensors for things like location, movement, temperature, atmospheric pressure, and more. Altogether we are talking about dozens of signals that can be combined and used to infer complex and interesting things about us, our environment, and our communities.\nSensing and automated data collection via mobile phones is becoming a key component in the digital revolution of the social sciences like psychology, sociology, and economics, among others. These techniques are also starting to make their way into consumer applications and services, but there are still many challenges (computational, efficiency, privacy/ethics). We spent the last several years at the MIT Media Lab working on developing these techniques and taking them from the lab into the real world. We believe that understanding human behavior and context is the next frontier, and we want to democratize this capability and make it ubiquitous.\nIn this talk I will give an overview of user-centric, continuous behavioral sensing with off-the-shelf mobile devices (phones). I will describe our work at MIT \u2013 the \u201cSocial fMRI\u201d approach for conducting data-rich experiments with people living their everyday lives, the system we\u2019ve built to implement it, and the \u201cFriends and Family\u201d living laboratory where a community of ~150 people lived under continuous data collection for over a year, in one of the largest mobile experiments done in academia to date. We collected over 1 million hours of research data, and with it we are able to see how friendships form and evolve, and how people influence each other, in our effort to understand what makes us human. There will be some examples of what we did and what we learned, as well as some of the big-data challenges and how we approached them. I will talk about our open source project and free \u201cexperiment in a box\u201d web-service for non-programmers that allows anyone to do the same kind of data collection, from quantified-self trackers to a large scale research study.\n", "\nRethinking your use of Hadoop in light of the superpowers cloud gives you opens up the possibilities for new patterns of work that are uniquely developer-friendly for you and your organization.\nPatterns of work like tuning your cluster to the job, knowing to put your cluster away when you\u2019re not using it, and why the first priority of any analytics cluster should be downtime. Each of these, when applied, are decisions that directly impact the happiness of developers and that should be the first considerations when deciding to spin up any Hadoop cluster.\nIn this talk, you\u2019ll hear from Flip Kromer, co-founder and CTO of Infochimps, as he walks you through a series of decision trees outlining why Hadoop in the cloud can be a powerful combination, helping to make clusters cheaper and developers happier.\n", "\nData science goes far beyond computation and visualization.  The data scientist needs to explore data interactively, share computations with collaborators, provide textual and mathematical explanations of those computations and present all of this information to a wide range of audiences.  Data scientists need software tools that span these diverse activities, without compromising the centrality of code and data.\nIn this talk, I will introduce the IPython Notebook and describe how it tackles these challenges to provide a foundation for data science that is interactive, repeatable, documented and sharable.\nThe IPython project provides open source tools for interactive computing in Python.  Historically, IPython has provided an enhanced interactive shell for Python.  Over the last decade, this shell has become the de facto working environment for scientific and technical computing in Python.\nMore recently, the IPython development team has expanded its efforts to develop the IPython Notebook, a web based interactive computing environment for Python, R, shell scripts and other languages.  At its core, the IPython Notebook is a first class environment for writing and running code in a web browser.  Conveniences of a modern IDE are present: syntax highlighting, tab completion, integrated help, access to the system shell, etc. However, the Notebook goes beyond mere code, by enabling users to build documents that combine live, runnable code with visualizations, text, LaTeX formulas, images and videos.\nThe Notebook document format has been carefully designed to support collaboration.  When version controlled (git, mercurial, svn, etc.), these documents preserve a full historical record of a computation, its results and accompanying material including embedded images and visualizations.  To enhance the dissemination of results, Notebook documents can be exported to a wide range of formats, including LaTeX, PDF, Markdown and HTML (allowing, for example, easy sharing of technical content on blogs, with posts directly generated from the original source with code, prose and figures).  Finally, Notebook documents can be converted to live presentations with the click of a button: the presenter can thus engage the audience with a document that has live computations in it and not just statically pre-rendered figures and media.\nIn addition to its core features, the Notebook has powerful parallel computing support that scales computations from multi-core CPUs to the cloud.  Because the Notebook architecture decouples the actual computations (the Notebook server can be run anywhere, from laptops to the servers in the cloud) from the interactive user interface (web application), users can ensure that computations are run in close proximity to the data.\nIn this talk I will introduce the IPython Notebook and describe  its usage in the context of data science.  In particular I will describe how it is ideally suited for the challenges of big data and how its usage dramatically improves the daily work of the data scientist.\n", "\nMailChimp is an email service provider (ESP) that sends over three billion newsletters a month for two million active users. Their business model is predicated on successful delivery of clients\u2019 newsletters to the inboxes of recipients. To maintain their reputation and delivery rate with major ISPs like Gmail, Yahoo, and Hotmail, MailChimp must prevent any spam or spam-like email from being sent over its system.\nFor small-scale ESPs, it is common practice to employ a team of compliance officers to make sure the content leaving the company is not spam, however now that MailChimp is one of the largest ESPs globally, this is no longer possible. No human could check three billion newsletters manually.\nTo that effect, MailChimp has created the Email Genome Project (EGP), a predictive analytics system capable of identifying the performance of email marketing campaigns before they leave the company\u2019s system. EGP is built on a massive horizontally scalable data store populated with the billions of send and engagement records MailChimp has gathered over the past ten years. The system uses a NoSQL solution in RAM to augment its disk storage and provide lightening fast predictions to the application. Bad actors are identified in real-time and are purged from the system before they can do damage. As an added benefit, MailChimp uses the email graph data in EGP to provide good users with targeted interest data about their subscribers.\n\n\nThis talk will cover:\n\t\nThe business need for MailChimp to create the Email Genome Project\nThe storage technologies and analytic techniques used in the predictive modeling system\nThe revenue and scale benefits of transitioning to an automated, data-driven compliance process\nHow the email graph data in the system is used to provide real-time analytics to users concerning their subscribers\u2019 engagement and interests\n\n", "\nThe complexity of decision sciences in a global marketplace is a daunting challenge multi-national brands and organizations have to grapple with.  The much talked about information explosion has been uneven geographically.  Availability of actionable information and policy surrounding use of this data varies considerably across continents and emerging economies.  As practitioners, our ability to develop and deploy information based strategies across markets has been inconsistent, and will continue to be so in the near future.  In this key note, we will explore some of the challenges of big data operating in a truly global context.\n", "\nRecently we have begun to see the emergence of a new  online data challenge\u2014that of the \u201cBroad data\u201d that emerges from millions and millions of raw datasets available on the World Wide Web. For broad data the new challenges that emerge include Web-scale data search and discovery, rapid and potentially ad hoc integration of datasets, visualization and analysis of only-partially modeled datasets, and issues relating to the policies for data use, reuse and combination.  In this talk, we present the broad data challenge and discuss  potential starting points for solutions.  We illustrate these approaches using data from a \u201cmeta-catalog\u201d of over 1,000,000 open datasets that have been collected from about two hundred governments from around the world.\n", "\nMobile is already changing everything about the way we access information, and we\u2019ve only begun to experience the impact of the smart devices revolution. Mobile analytics is also very different from web product analytics. With a variety of screen sizes, operating systems, and many other features, the \u201cmobile platform\u201d is in fact a multitude of distinct platforms. How do we recognize and even benefit from this richness and heterogeneity to create amazing products our users will love? \nData science for consumer internet products relies on our ability to effectively analyze and understand ubiquitous computing in terms of a holistic product experience, as individuals consume and create data on mobile and desktop devices in their day-to-day lives.  I\u2019ll talk about mobile data science challenges  \u2014 from product development to data-driven decision making.\n", "\nATTENDEES OF THIS TUTORIAL:\nPlease download the following software in advance of this tutorial (BEFORE you arrive on site):\n\nCompute Engine\u2019s gcutil \u2013 https://developers.google.com/compute/docs/gcutil/\n\n\nCloud Storage\u2019s gsutil \u2013 https://developers.google.com/storage/docs/gsutil_install\n\n\nBigQuery bq \u2013 https://developers.google.com/bigquery/docs/cli_tool#installation\n\n\nApp Engine SDK \u2013 https://developers.google.com/appengine/downloads\n\n\nPython 2.6+ (if not already installed) \u2013 http://www.python.org/getit/releases/2.7/\n\nAll users must have a Gmail account.\nWhen data volume and velocity become massive, processing and analysis solutions require specialized technologies for different parts of the data pipeline. Google\u2019s Cloud Platform is designed to help you focus on building applications, not infrastructure. We\u2019ll demonstrate how to build end to end Big Data applications \u2013 from data collection, to analysis, to reporting and visualization.\nThe agenda will include:\n\nUsing App Engine to collect data from users or remote sensors\n\n\nUsing App Engine and Compute Engine to crunch and transform both structured and unstructured data\n\nWill demonstrate how to quickly use MapReduce\n\n\nUsing BigQuery for exploring large data sets with ad hoc queries\n\nLoading data in from App Engine or Compute Engine\nRunning SQL-like queries on the data\n\n\nVisualizing and reporting results of analysis\n\nUsing custom code and the Visualization APIs\nUsing Google Spreadsheets\n\nProgramming experience required.  We\u2019ll be using Python for this tutorial, but prior Python experience is not required.\n", "\nSome of the most complex challenges in data management exist where you may least suspect: inside video games. In the past decade the audience for games has exploded from 200M active users to 1.5B gamers worldwide. And these gamers are playing on multiple platforms \u2013 high definition consoles, PCs, social media, mobile, online \u2013 and they expect a seamless experience that connects them all. Consumers are becoming accustomed to features that allow them to play against their friends, track their progress, and even be able to turn off their console game and pick up where they left off on another platform like a smartphone.\nMuch like other forms of media, playing a video game used to be linear \u2013 you insert the disc and play the game. Now, new content can be purchased and downloaded to augment the experience, consumers can connect and play against friends online and you can track your progress and scores live over time. But these rich experiences mean an explosion in the amount of data that can really be a double-edged sword. To keep at the pace of consumer demand for online connected gaming experiences, EA\u2019s data scientists are building a new technology infrastructure that will improve the consumer experience and help the company analyze the hundreds of terabytes of consumer data that flow through the system each day.\nIn this talk, EA CTO Rajat Taneja will dive in to the challenges and complexities facing the gaming industry, how to harness the power of data and share examples of how technologies like machine learning and predictive analytics have been put in place to improve the customer experience.\n", "\nAttendees wishing to follow along on their own computers should have a working installation of the IPython HTML Notebook and pandas 0.10.1, such as can be obtained by installing either of the free EPDFree (http://www.enthought.com) or Anaconda CE (http://continuum.io/anacondace.html) Python distributions.\nPlease make sure to download all installations BEFORE you arrive onsite.\nThis tutorial will be a hands-on introduction to the\nessential tools for working with structured data in\nPython,  pandas and NumPy. We\u2019ll look at some of the basic mechanics of the libraries\nthen work through some real world examples to illustrate how to load,\nclean, and wrangle data into the form needed to produce some summaries\nand visualizations.\n", "\nBirds of a Feather (BoF) sessions provide networking opportunities for attendees interested in the same projects and topics to meet in a casual setting. BoFs topics are created by attendees and can be about individual projects or broader topics (best practices, open data, standards).\nBoFs at Strata will happen during lunch on Wednesday, February 27 and Thursday, February 28, where lunch is served.\nStop by the BoF signup board near Registration to check out the topics or start your own.\n", "\nDealing with the flood of data that confronts researchers is the fundamental challenge of 21st century research. In many areas of research, the relentless growth of data sets has led to the adoption of increasingly automated and unsupervised methods of classification. In many cases, this has led to degradation in classification quality, with machine learning and computer vision unable to replicate the successes of human pattern recognition. Web-scale citizen science has provided a temporary solution to this problem however the solution is a short-term one. In this presentation I will outline a potential strategy for combining a large web community and significant compute resources to create a scalable, intelligent classification engine.\n", "\nIt\u2019s quickly becoming apparent that MapReduce is not enough when it comes to delivering interactive, low-latency data applications on Hadoop.  Yet, never before has there been more demand from companies to discover and explore data for real-time insights.  AMPLab\u2019s open source data analysis projects, Spark and Shark, deliver iterative queries up to 100x faster than Hadoop MapReduce.  Hear how companies are using Spark-based data platforms for fast, interactive analysis on big data.\n", "\nSciDB, Rasdaman, and Precog are all efforts to combine the data management capabilities of the relational world with the complex analytical capabilities of the statistical world. If successful, these efforts promise to revolutionize data science and data analysis. In this talk, Precog CTO John A. De Goes discusses the market needs that are giving birth to the \u201cscientific database\u201d, what these systems have to offer that is currently lacking in either the data management or statistical worlds, and how scientific databases will co-exist and co-evolve with Hadoop and other leading big data platforms.\n", "\nIt is highly important to understand one\u2019s audience for effective communication, especially on the internet where brands spend large amounts of time cultivating their image.  Traditionally done with demographic data, the new way to discover who is consuming content is through understanding clicked on topics and predilections.  If we take into account url reader\u2019s attitudes and expectations, we can create grouping around opinions.  These groupings relate users along meta data to create personas that effectively describe the use-cases for brands.  Using these user defined topic centric groups, we can understand the web as a graph and gain valuable insight into community discovery and sociological phenomena which characterize group evolution.\nIn this talk I will describe how to create meaningful clusters of users of social media.  Using bitly data which spans all social networks and more, we have a unique view into what people click on in the internet.  We reveal topics and keywords that gain the most attention.  Looking along lines of topics, keywords and domains from urls consumed by users that click on bitly links, we can discover the composition of domains and users focus to create distinct and meaningful groups for social communities.\n", "\nThe majority of data we consume today are presented in lists, for example search results, news feeds, and recommendations.  The makers of these lists get to decide what\u2019s at the top, and while that can be useful for quick filtering, it is very limiting for understanding context or for thinking strategically.\nPublic health officials confronting epidemics and generals planning battles interact with geographic maps to explore all the possible plans of attack.   However, for decision makers facing conceptual complexity geography isn\u2019t necessarily the right organizing principle, and so new types of maps are needed to enable them to explore multiple dimensions.  This could be a business strategist trying to outmaneuver their competitor, or a technologist understanding how new scientific advances could change their field.\nThis talk explores the use of network graphs as maps of rich datasets.  We\u2019ll look at how unstructured data can be represented as nodes and edges using similarity metrics, how learnings from cartography inform their design and interactivity, and the toolkits available to make your own.   In the process we\u2019ll see what these maps can teach us about the evolution of big data itself.\n", "\nTopics that concern the mass market ebb and flow over time.  By now the frenzy over the United States Presidential Election has faded and we are knee deep in the next celebrity/government scandal perpetrated by the Rombama administration or the latest outbreak of Beiber fever.  Throughout the dynamic of what occupies consumer mindshare two topics seem to remain constant and compete righteously for the top spot.  We\u2019re speaking of course of the constant battle between Zombies and Vampires.  Whether they are caused by the latest virus discovered in darkest Appalachia or have my Pretty Pony inspired sparkle powers when exposed to sunlight, these two populations have take a bite into our collective psyche and are not letting go.  Argus Insights has been tracking these two populations through our social media analytics platform and offer the public service of not only tracking the spread of zombies and vampires across the world but also diving into what drives the warring factions of zompire lover and haters to unpack the real battle between red and blue states (red being the oxygenated blood craved by vampires and blue being the lifeless blood that lies stagnant in the veins of zombies).\nArgus Insights has developed a way to not only track the spread and adoption of trends (zombies and vampires is just a compelling public health case study) but also diving deep into the conversations of consumers within social media to understand the full dimensions of the attitudinal segments with these two populations.  The presentation will unpack the methodology used to not only identify and track these two affinity groups but also how within the groups, new attitudinal segments are both identified and tracked using a novel clustering approach.\n", "\nMapping was one of the first data visualization techniques, and still remains one of the most powerful. Mapping data grounds the data in a connection to the real world, and gives us a context for understanding that world.\nThe last seven years has seen an explosion of mapping on the web, as various map apis put the power of mapping in the hands of developers. However, getting large amounts of data onto a map was restricted by limited browsers and the difficulty of setting up tile renderers. This in turn limited the ability of users to interact with large data sets as every request had to round trip to the server.\nNew browser capabilities are starting to allow for rich interaction with large datasets. This session will explore dynamic visualizations over large data sets, and look at both the capabilities and challenges of setting up a browser based, user centric approach to the interactive visualizations.\n", "\nNerds crash the gates of a venerable American institution, shoving aside its so-called wise men and replacing them with a radical new data-driven order.  We\u2019ve seen it in sports, and now in The Victory Lab- which Politico has described as \u201cMoneyball for politics\u201d- Sasha Issenberg tells the hidden story of the analytical revolution upending the way  political campaigns are run in the 21st century. The Victory Lab follows the renegade academics and maverick operatives rocking the war room and re-engineering a high-stakes industry previously run on little more than gut instinct and outdated assumptions.  Armed with insights from behavioral psychology and randomized experiments that treat voters as unwitting guinea pigs -and reams of new individual-level data fed into microtargeting algorithms-the smartest campaigns now believe they know who you will vote for even before you do.  The Victory Lab presents a secret history of modern American politics, pulling back the curtain on the tactics and strategies used by some of the era\u2019s most important figures-including Barack Obama and Mitt Romney-with iconoclastic insights into human decision-making, marketing and how analytics can put any business on the road to victory.\n", "\nThis panel will share insights on how K-16 education can benefit from developments in Big Data ecosystems. As learning and teaching goes \u201cdeeply digital\u201d with platforms like Khan Academy and expands broadly into new postsecondary online MOOC environments that straddle the high school to college boundaries, students and teachers have greater opportunities for high-quality content and data-driven learning progress than ever before.  How can we convert early momentum to make a lasting contribution to the nation\u2019s approach to education?  Panelists from MIT, Khan Academy and Gooru will survey notable hurdles, unique challenges and critical issues in education for making effective use of big data and frame how the Strata community could profitably contribute. We seek the Strata community\u2019s collaboration in building the interdisciplinary field of Learning Analytics in the year ahead: come and hear how!\nThis session is sponsored by inBloom\n", "\nReal-time video analytics rank among the key applications driving the proliferation and impact of big data. Ooyala\u2019s leading video analytics platform tracks viewer engagement in real time, and its insights help media companies create deeply personalized viewing experiences. During this session, Ooyala CTO, Sean Knapp and DataStax CEO, Billy Bosworth will discuss why Ooyala selected DataStax as the big data platform powering their business.\nThe session follow the process of evaluating different big data platforms based on varying use cases and business requirements, explain how big data professionals can choose the right technology to transform their business, and how open source technology is helping close the skills shortage within the big data industry.\nThis session is sponsored by Datastax\n", "\nTo address SLA and capacity limitations in NetApp\u2019s phone-home processing, NetApp kicked off a program to implement a Hadoop-based solution on NetApp\u2019s E-Series storage.  The solution is built to handle 2-5 TB/week of incoming data, and address multiple use cases \u2013 real-time data access, aggressive processing SLAs for transactions, standard reporting, data mining, data augmentation and distribution.  This session will discuss the history of the problem, the technical architecture of the solution, and how Pentaho is used to address the challenges these disparate use cases pose for Hadoop alone.  Takeaways will include recommendations for success with Pentaho in a Hadoop-based big data solution.\nThis session is sponsored by Pentaho\n", "\nMonday, February 25, 6:30pm\nMission City Ballroom\n\nEnlighten Us, But Make it Quick\nWe\u2019re thrilled to host Ignite at Strata Conference on Monday, February 25. We\u2019ll be putting a special focus on how data is collected, analyzed, and interpreted to understand and shape the world around us, from the quirky to the sublime to the downright creepy.\nIgnite delivers thought-provoking ideas in bite-sized, five-minute chunks. Join us for a fun, high-energy series of talks by Strata attendees and the Bay Area Ignite community\u2014all aspiring to live up to the Ignite motto, \u201cEnlighten us, but make it quick.\u201d\n", "\nAs big data makes inroads into all aspects of society, how governments\nregard the technology will be critical for its success. If the past is\na guide, the state will embrace big data for its own uses (both good\nand ill). It will recognize that its authority is threatened and lash\nout. And government will try to place big data under the yoke of\nregulatory control. The talk will build on ideas outlined in the new\nbook \u201cBig Data: A Revolution That Will Transform How We Live, Work,\nand Think\u201d (with Viktor Mayer-Sch\u00f6nberger) to explain where the choke\npoints are\u2014and how to keep big data free from governments\u2019 grip.\n", "\n\nSponsored by:\n\n\n\n\n\n\nOnce again at Strata, we\u2019ll be inviting the best of the best to demonstrate their innovations at Startup Showcase. \nOur team of investors, entrepreneurs, and industry analysts will select ten leading big data startups to present their technologies and tell their stories live at Strata Conference on Tuesday, February 26. At the end of the showcase, our judges will select three companies among the finalists\u2014those whose technology, team, and offering stand apart from the rest.\nJudges\n\n\nRenee DiResta\nOATV\n\n\n\nTim Guleri\nSierra Ventures\n\n\n\nMike Dauber\nBattery Ventures\n\n\n\nJohn Jarve\nMenlo Ventures\n\n\n\nBrady Forrest\nKhosla Ventures\n\n\n\nDeepak Jeevankumar\nGeneral Catalyst Partners\n\n\n\nRob Coneybeer\nShasta Ventures\n\n\n\nMichael Baum\nRembrandt Partners\n\n\n\nMichael Callahan\nGreylock\n\n\n\nJosh Breinlinger\nSigma Partners\n\n\n\nJu-kay Kwek\nGoogle\n\n\n\nRachel PikeDraper Fisher Jurvetson\n\n\nStartup Showcase Winners\n\nFirst Place: import\u2022io\nRunners-up: Rest Devices and Splice SQL Engine\nAudience Pick: SiSense Prism\n\nStartup Showcase Finalist Companies\n\nBeyondCore\nimport\u2022io\nJethroData\nOLSET\nRest Devices\nVertascale\nSimularity\nSiSense Prism\nSplice SQL Engine\nStopped.at\nwise.io\n\n", "\nHadoop is great for analyzing data at rest.  But what if your business problem requires the ability to analyze and respond in real-time and without a human in the loop.  Getting faster, more relevant insight to take immediate action is really the name of the game with many big data industry use problems found in Telcos, Life Sciences, and Financial Services.  The traditional \u201dstore and analyze\u201d approach typically used in data mining is not naturally designed for these problems where the temporal dimension differentiates real-time data from conventional big data.  As a result, new technologies such as stream computing are needed that can continuously analyze data in motion to support real-time decision-making.  To emphasis these points, we will discuss:\n\nStream computing & real-time analytics:  data mining, time-series, geospatial, machine data, statistics, and others   \nThree use case studies that illustrate the use of real-time analytics:   \n\n\t\nIllness detection \u2013 detecting infections in pre-mature babies up to 24 hrs sooner than a human \nTraffic congestion for bus routes \u2013 automatically re-routing buses and notifying riders of schedule delays \nPatient care \u2013 analyzing brain signals for real-time detection and alert of seizures\n\n\n Best practices / lessons learned from implementing real-time big data analytics\n\n", "\nData visualization is often an after-thought in a world of collecting and analyzing data.  And yet, without clear and compelling communication, analysis will never drive insights and action.\nThe best analyses show us something that we aren\u2019t expecting and when it comes to spotting surprises, \u201ceven the best statisticians often set their calculations aside for a while and let their eyes take the lead.\u201d (Stephen Few, 2009)\nThis session explores applications of Shneiderman\u2019s famous mantra for visual data analysis (overview first, zoom and filter, then details-on-demand) as a framework in the context of three complex analytical applications at Wells Fargo:  (1) Analytics process, (2) Interactive meeting facilitation and (3) Dashboard design.\nWe will explain how an interactive process of filtering and visual representation guides not only the analytical process, but also tells a story in a way that conveys useful and actionable insights to business users. Using specific case study examples from Wells Fargo, we will describe which visualization tactics and tools have driven success and why..\n", "\nAs many enterprises deploy Hadoop into production, they are finding that it is not the volume or velocity of their data that is problematic right away, but rather the variety of the types and formats of their critical data.\nThere are too many data sources for this critical data and not enough integration. This leads to increasing infrastructure costs as companies need to run separate Hadoop, NoSQL and enterprise environments as well as increased engineering deployments to coordinate the moving parts. Enterprises are spending more on system upkeep and not keeping up with their business analysis.\nThis presentation will provide an overview on how leading companies have integrated Hadoop, NoSQL (HBase) and enterprise sources on one platform. Data is combined and processed in one simplified architecture freeing up resources to get more out of Hadoop.  Case studies and reference architectures will be reviewed to demonstrate how companies have succeeded in generating Big Data insights with less infrastructure costs.\nThis session is sponsored by MapR\n", "\nThe big data revolution is more than just terabytes or petabytes of data.  It is also the application of new paradigms, languages, and tools to these data sets.  This is a great strength of big data, but also a liability.  These tools have different data models, different utilities for reading and writing data, and different frameworks for including user code.  How can users in the same organization using different tools share data?  How can user defined functions written for one tool be used by other tools?  This talk will cover work in Apache HCatalog, Apache Pig, and Apache Hive projects that is being done to address these issues.\n", "\nNeustar is a fast growing provider of enterprise services in telecommunications, online advertising, Internet infrastructure, and advanced technology. Neustar has partnered with Hortonworks to leverage Hadoop to expand their data analysis capacity. This session describes how Hadoop has expanded their data warehouse capacity, agility for data analysis, reduced costs, and enabled new data products. We look at the challenges and opportunities in capturing 100\u2032s of TB\u2019s of compact binary network data, ad hoc analysis, integration with a scale out relational database, more agile data development, and building new products integrating multiple big data sets.\nThis session is sponsored by Hortonworks\n", "\nData mining is used more and more by communication professionals to measure brand position or the so-called \u201cpublic opinion\u201d. The more common those measurements are the more likely are attempts to skew the measurement. Those efforts are often called \u201castroturfing\u201d and might lead to wrong decisions by businesses and politicians. The objective of this talk is to show ways to detect those attempts of fake influence.\nThe talk will introduce methods such as \u201csocial reaction curves\u201d or \u201cnetwork topologies\u201d which can be used to analyze the public discussion in online and offline media. Those measurements can help to distinguish between fake and real attempts of influence. Examples of attempts to influence will be shown ranging from the woman2drive campaign in Saudi Arabia to the Occupy Wall Street Movement.\nIn a special case study about the London 2012 Olympics, Lutz Finger and Prof. Soumitra Dutta will show which sport, which athlete or which media type were dominating the 2012 Olympics. They will show that traditional ways of doing media analytics might be misleading marketers and sponsors.\nAs outlook the speaker will discuss recent advancements in automated communications algorithms such as chat bots or facebook bots. Those are becoming more sophisticated and thus harder to spot.\n", "\nJoin us at the Expo Hall Reception immediately following the first day of sessions. Have a drink or two, network with other attendees, and visit our companies who are innovating in the data space. This event is open to all sponsors, exhibitors and attendees.\n\n", "\nD3 (data-driven documents) is one of the most acclaimed data-visualization libraries and has gained considerable adoption in its short lifespan. It is a very versatile framework, which can be used to create simple charts as well as sophisticated representations like network graphs or geographic projections, and which handles interactivity and animation well. But because of its unique features it also uses a specific syntax and concepts.\nThis tutorial will allow attendees to master the essential ideas of D3 and become fully operational.\n\n\nThe outline of the tutorial is:\n\t\nIntroduction\nFundamental notions; the document model, SVG;\nSetting up D3;\nData \u2013 loading and verifying data; \nSelections \u2013 how to map data to visual elements;\nSetting attributes and style \u2013 controlling the appearance of the output; \nScales \u2013 one essential helper function in D3;\nTransitions and interactivity;\nThe concept of data joins;\nConclusion \u2013 what can be achieved with D3.\n\n", "\n2012 was particularly interesting for the variety of Big Data use-cases implemented. This session explores key patterns across horizontal and vertical use cases. The discussion will cover questions such as\n\u2022    What\u2019s the incremental power or value of Big Data over \ntraditional Data Analytics/BI?\n\u2022    How is it monetized?\n\u2022    What are the use cases?\n\u2022    How are different industry verticals using Big Data?\n   And finally\u2026\n\u2022    How to roll out your own Big Data Strategy?\nThis session is sponsored by Impetus\n", "\nThe analytics and data warehousing industries are in the midst of a major period of transformation and upheaval. Since the publication nearly a decade ago of Google\u2019s seminal MapReduce and GFS papers, we have witnessed the appearance of Apache Hadoop, followed closely by the arrival of batch-oriented SQL systems like Apache Hive, and the scramble by established SQL vendors to implement Hadoop connectors.\nThis talk addresses the recent emergence of a new generation of analytic databases inspired by Google Dremel. These databases have been designed with the goal of running real-time SQL natively on Hadoop in a manner that fully exploits the flexibility and performance of the underlying platform. Characterized by features including schema-on-read, support for semi-structured data, and pluggable storage engines, and defined by systems like Citus Data\u2019s CitusDB and Cloudera\u2019s Impala, these new systems share important architectural details that distinguish them from the previous generation of analytic databases.\nIn this talk we will discuss the unavoidable cost and performance limitations of the connector-based approach employed by many established vendors and explain the long-term significance of Apache Hive\u2019s data model along with its influence on next generation SQL-on-Hadoop databases. We will then unravel the novel architectural features common to next generation analytic database systems like CitusDB and Impala that make real-time SQL-on-Hadoop feasible. Finally, we will conclude by reviewing several important database lessons learned over the previous decades that remain relevant today.\nThis session is sponsored by Citus Data\n", "\nHadoop is the engine powering the Big Data era, an unstoppable force boasting\nmassive investments and a rich ecosystem. But this is only the beginning: Hadoop\nhas the potential to reach beyond Big Data and become the Foundation for Change,\ncatalyzing new levels of business productivity and transformation.\nHadoop will become the Foundation for Change.\nThis keynote is sponsored by EMC Greenplum\n", "\nThe excitement about Big Data is not around the data volumes or variety or velocity. The excitement stems from the results\u2014the impact on revenue, the decrease in costs, the Big gains in competitive advantage that are the result of using Hadoop and HBase across applications and industries.\nThis keynote will provide insights into how the combination of scale, efficiency, and analytic flexibility creates the power to expand the applications for Hadoop to transform companies as well as entire industries.\nThis keynote is sponsored by MapR Technologies\n", "\nMore than ever before, students are using the Internet to study, leaving behind a trail of valuable data. How can we leverage this data to improve education? Learning analytics focused on tracking student activity and performance provides strong insights, but the bigger opportunity is to \u201cclose-the-loop\u201d and use the data for the student\u2019s benefit. With Gooru (www.goorulearning.org), a free search engine for learning, we can see what web resources are the most popular, what topics students are looking to study, how much time students are spending studying specific resources and the corresponding learning outcome. With this data, we can deliver personalized learning on a scale that can honor the human right to education.\nThis keynote is sponsored by inBloom\n", "\nHadoop and SAP HANA are taking the world by storm. SAP HANA is the fastest growing commercial database in the market, being adopted by the world\u2019s top enterprises for real-time analytics and applications. Hadoop is the fastest growing open source project in the world of Big Data. Both bring critical, unique value to their customers. Come and imagine what will happen when we graft the two together.\nThis keynote is sponsored by SAP\n", "\nAlready recognized as providing the engine of computation for the data economy, Intel processors undergird many of the most demanding analytic applications in the world. And in some unexpected ways, \u2013 from helping researchers visualize energy consumption in a Texas community to helping clinicians mine for biomarkers in a mountain of genomic data to helping educators develop a personalized learning system that adapts to each child \u2013 Intel software transforms human lives by bringing intelligence to wherever big data lives.\n", "\nThere\u2019s been a huge amount of progress in recent years in developing distributed systems that are resilient to all sorts of faults. However, there\u2019s one critical category of errors that has largely been ignored: human error. The scope and potential impact of human error is massive: deployed bugs, accidentally deleting data, accidentally DDOS\u2019ng important internal services, and so on. Designing for human fault-tolerance leads to important conclusions on the fundamental ways data systems should be architected.\n", "\nThis tutorial provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop, plus its associated ecosystem. This session is intended for those who are new to Hadoop and are seeking to understand where Hadoop is appropriate and how it fits with existing systems.\nThe agenda will include:\n\nThe rationale for Hadoop\nUnderstanding the Hadoop Distributed File System (HDFS) and MapReduce\nCommon Hadoop use cases including recommendation engines, ETL, time-series analysis and more\nHow Hadoop integrates with other systems like Relational Databases and Data Warehouses\nOverview of the other components in a typical Hadoop \u201cstack\u201d such as these Apache projects: Hive, Pig, HBase, Sqoop, Flume and Oozie\n\nNo programming experience is required for this session.\n", "\nMESSAGE TO ATTENDEES:\nHi all,\nYou can find the nearly complete exercises and slides here:\nhttps://s3.amazonaws.com/thinkbig-academy/Strata2013/RealTimeSearchAndAnalytics-master.zip\nPlease note that there are still a couple of exercises and a few slides in the deck that I am putting the final touches on, but this will give you a good idea of what we will be presenting. Please make sure to re-download this zip file the day prior to the tutorial.\nSome important notes:\nA few of the exercises include installing and working on a local installation of Solr (your computer). We will guide you through the installation process. I would suggest using a Linux distribution such as Ubuntu. I will be using Mac OS X. Windows users can setup a VM or use Cygwin.\nAnother exercise will use an Amazon EC2 cluster, I will provide the connection details on the day of the tutorial.\nFurther details are in the READMEs of the exercises themselves.\nI look forward to seeing you all!\nRyan\nMore and more clients are interested in understanding how they can make use of big data. One typical use case is how to run advanced ad hoc queries over massive sets of data receive results in real time. There are an increasingly large number of products in the market available for all types of data types and requirements, including products such as DataStax Enterprise or Apache Solr/Lucene. We will break down and describe the distributed search landscape and show you how to use these interesting technologies with a hands on tutorial.\nBig Data -> Distributed Search \u2013 applications in industry (10 minutes)\n\nSearch applications for structured data\nSearch applications for unstructured data\nGeo-indexed search\nWhy distributed search? What happens as index size grows with data?\n\nExample Use Case (40 minutes)\n\n\nUse Case: Log Data\nRequirements\n\t\nPetabytes of semi-structured log data\nBillions of parsed Solr documents\nApache Solr\nSchema\nBackups\nDisaster Recovery\nDuplicates\nJoins\n\nTechnology Landscape (10-15 minutes)\n\n\nSolr/Lucene out of the box\n\t\nIntegrated Solr Solutions\nDatastax Enterprise (DSE) 2.0\nLily\nLucid Imagination\nKitenga\nKatta\n\n\n\nNon-Solr Solutions\n\t\nRiak\nMongoDB\nAmazon cloud search\nGoogle BigQuery\n\n"], "2014": ["\nTools to make small- and large-scale data analysis faster and more accessible\ncontinue to evolve at a rapid pace. On one end of the accessibility spectrum,\nprogrammatic tools for data preparation and large scale data analytics are\nbeing propelled forward by a variety of open source, academic, and commercial\ninterests. On the other, self-service or visual analytics and business\nintelligence tools have enabled advanced data workflows for many data-driven\nprofessionals formerly disenfranchised by the hitherto technical expertise\nrequired. However, there is still work to be done. In this talk, I\u2019ll take a\ncritical look at a variety of state of the art tools for each of the major\nareas of data analysis: preparation, exploration, modeling, and\ncollaboration. Where relevant, I\u2019ll illustrate shortcomings in our current\ntools, discuss ongoing initiatives to improve the state of the art and\nopportunities for the future.\n", "\nLectureTools (http://www.lecturetools.com) is a web application created by the author that provides more opportunities for students to participate in class.  Integrated with most Learning Management Systems independent research has shown that LectureTools both increases attentiveness and engagement in large classes.  Instructors can create a wide range of questions for their students including multiple choice, reorder list, association, free response, and image-based.  Students can\n1)    Type notes synchronized with the lecture slides; \n2)    Pose questions and view answers; \n3)    Draw on lecture slides; \n4)    Respond to instructor questions; \n5)    Print lecture slides and notes for off-line review.\nThis presentation discusses research using this tool that explores how best to mine the input from students in real-time to provide opportunities for adaptive learning.  Simple efforts to provide \u201clecture clouds\u201d of words typed have been expanded to intelligently guide students to additional resources as a result of their notes.  This presentation is hands-on and provides an opportunity for the audience to participate as students to experience mining of their notes and the feedback provided.\n", "\nShrikanth Shankar, Qubole\u2019s VP of Engineering, shares his best practices for building high-performance, scalable queries and deploying User Defined Functions (UDFs) to Big Data applications in Apache Hive. For data analysts and data scientists in the trenches, this is a key session to attend.  Attendees will have access to one of the best experts on Hive in the world and will learn:\n\u2022    How to write effective Hive queries\n\u2022    How to optimize current queries by up to x5 faster\n\u2022    How to ensure aggregated, accurate results\n\u2022    How to structure complex HQL queries\n\u2022    How to combine multiple languages within a query\n\u2022    Various types of UDFs supported in Hive\n\u2022    How to discover and use existing UDFs\n\u2022    Apache Hive\u2019s UDF framework and best practices\n\u2022    How to develop, deploy, and use custom UDFs\n\u2022    Overview, including examples of well-written UDFs\n", "\nLinkedIn has a unique data collection: the 238M+ members who use LinkedIn are also the most valuable entities in our corpus, which consists of people, companies, jobs, and a rich content ecosystem. Our members use LinkedIn to satisfy a diverse set of navigational and exploratory information needs, which we address by leveraging semi-structured and social content to understanding their query intent and deliver a personalized search experience.\nAs a result, we\u2019ve built a system quite different from those used for web or enterprise search.  In this talk, we will discuss how we have addressed the unique scalability, performance, and search quality challenges in order to deliver billions of deeply personalized searches to our members. Although many of the challenges we face are unique to LinkedIn, we hope that the ideas we share will prove useful to other folks thinking about entity-oriented search or working with large-scale social network data.\n", "\nApache Mesos is an open source cluster manager that provides efficient resource isolation for distributed frameworks\u2014similar to Google\u2019s \u201cBorg\u201d and \u201cOmega\u201d projects for warehouse scale computing. It is based on isolation features in the modern kernel: \u201ccgroups\u201d in Linux, \u201czones\u201d in Solaris.\nGoogle\u2019s \u201cOmega\u201d research paper shows that while 80% of the jobs on a given cluster may be batch (e.g., MapReduce), 55-60% of cluster resources go toward services. The batch jobs on a cluster are the easy part\u2014services are much more complex to schedule efficiently. However by mixing workloads, the overall problem of scheduling resources can be greatly improved.\nGiven the use of Mesos as the kernel for a \u201cdata center OS\u201d, two additional open source components Chronos (like Unix \u201ccron\u201d) and Marathon (like Unix \u201cinit.d\u201d) serve as the building blocks for creating distributed, fault-tolerant, highly-available apps at scale.\nThis talk will examine case studies of Mesos uses in production at scale: ranging from Twitter (100% on prem) to Airbnb (100% cloud), plus MediaCrossing, Categorize, HubSpot, etc. How have these organizations leveraged Mesos to build better, more scalable and efficient distributed apps? Lessons from the Mesos developer community show that one can port an existing framework with a wrapper in approximately 100 line of code. Moreover, an important lesson from Spark is that based on \u201cdata center OS\u201d building blocks one can rewrite a distributed system much like Hadoop to be 100x faster within a relatively small amount of source code.\nThese case studies illustrate the obvious benefits over prior approaches based on virtualization: scalability, elasticity, fault-tolerance, high availability, improved utilization rates, etc. Less obvious outcomes also include: reduced time for engineers to ramp-up new services at scale; reduced latency between batch and services, enabling new high-ROI use cases; and enabling dev/test apps to run on a production cluster without disrupting operations.\n", "\nA new generation of data processing systems, including web search,\nGoogle\u2019s Knowledge Graph, IBM\u2019s Watson, and several different\nrecommendation systems, combine rich databases with software driven by\nmachine learning.  The spectacular successes of these trained systems\nhave been among the most notable in all of computing and have\ngenerated excitement in health care, finance, energy, and general\nbusiness.  But building them can be challenging even for computer\nscientists with PhD-level training. If these systems are to have a\ntruly broad impact, building them must become easier. This talk\ndescribes our recent thoughts on one crucial pain point in the\nconstruction of trained systems feature engineering. For street-art\nlovers, this talk will argue that current systems require artists,\nlike Banksy, but we need them to be usable by the Mr. Brainwashes of\nthe world.\n", "\nKeynote by James Burke, science and technology historian, futurist, and author.\n", "\nAs traffic volumes in cities around the world are constantly growing we are faced with the challenge to track and control car movements in a more detailed and intelligent way to beat the traffic. Real-time information on traffic including automotive sensors and crowd-sourced data feeds are an interesting new source of data. However, to utilize this data to its full extent and turn it into valuable information, intelligent methods for analyzing and predicting traffic are needed.\nPivotal\u2019s Data Science Team has developed several innovative methods to analyze this traffic flow information harvested from real-time and in-car data sources including GPS. These methods by themselves are highly useful for predicting future traffic conditions and dissecting traffic data. We will describe how we created these algorithms and show different interesting results from their application. This example demonstrates how deeper insights into a problem can be found by combining different machine learning methods.\nThe methods developed by our team enable more intelligent routing systems through a more detailed velocity prediction based on a number of influencing factors. This is highly valuable for planning routes to far-away destinations and also useful on inner-city routes where traffic can be influenced by a lot of different factors. However, we recently tapped another valuable source of data that could enrich traffic prediction models even further.\nLocal transport authorities already make a lot of traffic and travel disruption freely available. These reports form the basis of traffic updates across a wide range of media. Currently however the reports are limited to acknowledging the start of a disruption, and then providing updates as the situation develops. In the smart city of the future these disruption reports will also predict the duration and severity of the disruptions, enabling route guidance systems to make better decisions.\nWe will also demonstrate a traffic disruption model that can predict the duration of recently begun incidents, learning the distinct traffic and disruption patterns of a major global city. The disruption prediction model incorporates historical traffic count data, previous incident reports and local weather conditions and uses an interesting variety of machine learning methods running on a massively parallel analytics database system.\nWe will conclude by outlining how the crowd-sourced real-time data could be matched to traffic disruption and open government data, to push the envelope in traffic analysis and prediction even further.\n", "\nHadoop presents as an enabling technology to better understand customer preferences and behaviors, but organizations often struggle with time-consuming data preparation and analytics processes.   edo Interactive \u2013 a leader in providing card-linked offers to financial services and retailers \u2013 shares how they drive agile, improved decision-making by complementing native Hadoop technologies with analytical databases and ETL optimization and data visualization solutions from vendors such as Pentaho.\nThis session is sponsored by Pentaho\n", "\nThe runaway success of predictive modelling, most notably visible in Kaggle competitions, has seen data scientists of all approaches try their hand at building models with an ever growing corpus of data.  However, making use of these models in an operational or influential fashion is difficult.  It is often the case that important features of a model have difficult to interpret physical meanings or, more commonly, are not correlated with any change a business can make to its product or service.  Without a strong link between model features and business actions predictive models can look great but fall short of delivering.\nThis talk will provide a sharp contrast between the \u201ctextbook best\u201d and the \u201cmost successful\u201d model in a fun and non-traditional (to data science anyway) business setting, that of North Side Chicago bars.  I will describe the problems facing these businesses, the data science used to solve their problems (with particular attention paid to building the most accurate model vs. the model that makes the most money), and relate the collective success of each competitive bar to the classic \u201cprisoner\u2019s dilemma.\u201d From this base, I will build into a much larger problem facing internet search, that of understanding user intent and satisfaction.  I will again provide a short cast study in Bing search drawing from comparisons built previously in the Chicago bar examples.\nAs a physicist turned data scientist I have found the lesson of balancing the trade-off between accuracy and performance a crucial one in moving out of tutorials, books, and examples and into real data and problems where actionable insights are paramount.  This talk will provide a balance of technical details in modeling with a broader appreciation for the role of modeling in serving results to clients and partners.\nOutline:\n\nIs the most accurate model always the best one?\nWhy it\u2019s tough to be a bar owner in Chicago (how data science can help out a bar)\nA rising tide lifts all boats (how cooperating with your competition is good for you)\nCollaborative filtering for Corona (building a successful model)\nWhat is success in search?\nPredicting user intent (defining a data science problem in Bing search)\nWhat do you mean when you say \u201cTom Cruise\u201d? (using data science to assign correct search intent)\n\n", "\nBig Data without analytics is just data, but how do you perform the analytics? For many, this is a sequential process: process your big data and then move it to where you can perform analytics.  But what happens when you can perform analytics wherever your big data sits?  This notion of In-Hadoop analytics is changing the game for the possibilities of Hadoop.\nThis keynote is sponsored by IBM\n", "\nHadoop is emerging from the shadows of proof-of-concept projects to become a catalyst for a big data revolution.  By combining open source modus operandi, commodity hardware economics, and new data analytics opportunities, Hadoop is being viewed by its more ardent advocates as the leading platform that will usher in the most significant information architecture advances in a generation. However, there are still some skeptics.  Vendors and loyalists of more traditional information management products and services see Hadoop as more of a feature than a new product category, i.e., a useful and complementary extension to earlier enterprise data platforms, but not a disruptive shift.\nThe reality is almost certainly somewhere between these polarized perspectives, although \u201cactual results may vary\u201d depending on the industry domain and business data intensity of the organization seeking to leverage Hadoop and big data solutions in general.\nIn this presentation, Sr. Director and Solution Lead for Savvis\u2019 big data solution, Milan Vaclavik and Chief Architect, Consumer Brands, John Martin provide a straightforward approach for understanding how Hadoop and related big data market dynamics fit into the broader IT market landscape. They will discuss why Hadoop alone is not a panacea for achieving information insight success and provide a robust framework for understanding the past, present, and future directions of Hadoop and big data market dynamics.\nPresenters will share considerations such as:\n\nThe strategic significance of infrastructure core services (compute, storage, network, and comprehensive security) required for robust big data solutions\nThe strategic significance of Hadoop 2.0, Hadoop/NoSQL convergence, and the critical need for effective modeling, query formulation, and data analysis capabilities as Hadoop becomes an enterprise platform for big data.\nGuidance on how an enterprise grade service-centric approach can help organizations of all sizes optimize the promise of big data dynamics while avoiding related pitfalls.\n\nThis session is sponsored by Savvis\n", "\nIn the beginning only the phones were smart.  Or so it seemed. Sensors, devices, systems, buildings. cities, and networks, especially the networks, were merely awash in data. But emerging from this primordial soup are prototypical structures of data-driven intelligence \u2013 aggregators, gateways, reservoirs, queries, models, graphs, and triggers. Linked in from data edge to data center, these elements are recomposing data analysis with novel distributed architectures. Intel is well known for accelerating  such architectural shifts in the industry with new computing platforms at ever smaller scales of hardware. Less known are our current efforts to build end-to-end reference architectures for distributed analytics using open source software such as Hadoop, Lustre, and GraphBuilder. In this session, I will illustrate these architectures with real-world examples of city governments, retail banks, food manufacturers, pharmaceutical companies, and Intel itself applying intelligence wherever data lives.\nThis session is sponsored by Intel\n", "\nAt LinkedIn, we\u2019re constantly looking for ways to deliver more insights and recommendations to our users. The only way to learn what users find useful is to test things, so we constantly work on making tests cheaper and more effective.\nIn this talk, we\u2019ll review our experimentation framework for the home page. We\u2019ll talk about the three key systems that make this possible:\n- The framework for experiment design, deployment, and analysis. (Xin Fu)\n- The system for adding content to the stream. (Joseph Adler)\n- The ranking system for finding and prioritizing the best content. (Bee-Jung Chen)\nWe\u2019ll provide some real examples of how we use this system, and talk about some of the lessons that we\u2019ve learned.\n", "\nThe ability to instrument and interrogate data as it moves through a processing pipeline is fundamental to effective machine learning at scale.  Applied in this capacity, information visualization technologies drive product innovation, shorten iteration cycles, reduce uncertainty, and ultimately improve the performance of predictive models. It can be challenging, however, to understand where in a workflow to employ data visualization, and, once committed to doing so, developing revealing visualizations that suggest clear next steps can be similarly daunting.\nIn this talk we\u2019ll describe the role that information visualization technologies play in the LinkedIn data science ecosystem, and explore best practices for understanding the structure of large-scale data in a production environment.  From hypothesis generation and feature development to model evaluation and tooling, visualization is at the heart of LinkedIn\u2019s machine learning workflows, enabling our data scientists to reason and communicate more effectively.  Broken down into clear, structured insights based on proven technology and workflow patterns, this talk will help you understand how to apply information visualization to the analytical challenges you encounter every day.\n", "\nDespite all the hype, we\u2019re failing at big data. IDC estimates that more than 90% of newly collected data is never analyzed or applied. OECD reports that the productivity growth rate for information workers has slowed dramatically over the last decade. The ability to collect, store, and process massive volumes of information isn\u2019t actually living up to the promise of delivering better decisions.\nThe problem is that human cognition doesn\u2019t scale at the rate of Hadoop clusters. The skills and systems we\u2019ve come to rely on in a small data world are straining under the flawed expectation that more complex infrastructure should justify more complicated experiences.\nIt\u2019s time to push back on this implicit cultural acceptance of end-user complexity. The move toward mobile-friendly analytics dashboards is a good start, but we can and must go further. Let\u2019s rethink the way humans are empowered to make decisions\u2014not just through the presentation of facts, but with the design of smart experiences.\nThis session will lay out a vision for using design thinking built on metadata to connect people to information in a world where complexity is inevitable and technology alone is insufficient.\nIt will challenge the audience to:\n\nRethink the role of design as being fundamental to the process of human cognition, far beyond the aesthetics of visualization\n\n\nFocus on metadata as an under-explored building block in the future of information design\n\n\nApply the principles of consumer app culture to guide an approach to enterprise decision-making in a big data context\n\n", "\nIs your organization struggling with the \u201cBig Data Analytics Platform OR Hadoop\u201d decision?  Join us to learn why and how you can use real-time big data analytics AND Hadoop together to solve your organization\u2019s big data challenges. You\u2019ll hear real-world examples on how organizations have adopted a \u201cbest-tool-for-the-job\u201d approach, using HP\u2019s HAVEn to their best advantage\u2014whether it is Hadoop for ETL and HAVEn components like HP Vertica, HP Autonomy, and more for blazing-fast analytics of all types of data or HP Vertica and Hadoop for complete data exploration and real-time analytics and more .\nThis session is sponsored by HP\n", "\nData analysts routinely report spending more time \u201cwrangling\u201d their data than performing analysis per se.  In this tutorial we focus on the ever-present yet oft-overlooked challenges of Data Transformation, including discovery, structure, content and curation.  We emphasize recent approaches that jointly emphasize interaction and inference, leveraging both human acuity and computational power.\n", "\nEffective analytics requires forethought, which seems at odds with agile software development\u2019s emphasis on velocity. However, as this talk demonstrates, the two can and do mesh nicely. Once you understand the reasons why core agile principles work, you can safely modify them for special fields like analytics. In this talk, I talk about process and and technical subjects, including emergent design, test-driven development, and other agile techniques. Both agility and analytics are necessities; I show how to bridge the friction between them.\n", "\nThe growing popularity of Hadoop has led to the availability of an increasing number of clusters worldwide, often multiply within the same organization. However, in order to leverage this computing capability, the clusters must first be primed with data. Frequently, this entails uploading existing client repositories into a remote cluster. Such a move can be challenging for the following reasons:\n\nsize: the size of the data to be transferred can be very large. Typically, enterprises do not consider adopting Big Data technologies unless they are actively experiencing pain owing to their current system being unable to handle the existing volume. At that point, their data has usually grown to significant levels and, consequently, is much more difficult to manage.\n\n\nnetworks: if the target cluster is remote, one option is to move data via wide area networks. This presents hurdles in terms of limited available throughput, bandwidth and security. Transferring large data sizes via this approach can potentially be very time consuming. A special case is if the source and destination clusters are within the same data center but belong to different organizations. This scenario requires a different set of specialized skills in order to set up a network architecture that allows data to flow.\n\n\nlack of domain knowledge & tools: there exists little understanding of the various approaches for bulk data uploads to a Hadoop cluster. In addition, widely used data transfer tools such as scp, ftp and rsync do not directly interface with HDFS and alternatives are not available. While there are tools to facilitate cluster to cluster copies, doing so across organizations and multiple hadoop versions is challenging.\n\n\nsecurity: data is particularly vulnerable during transit. Being able to safely transport high volume data across organizational boundaries and networks demands thorough understanding of security protocols and practices.\n\nIn this talk, we present a number of techniques and best practices for uploading large quantities of data to a remote Hadoop cluster. Our presentation is based on real world experience in transferring large amounts of data on behalf of various clients. Topics covered will include:\n- DistCp: this is a tool that comes bundled with hadoop for cluster to cluster copying. DistCp has many possible configuration settings and Hadoop 2.0 adds more. We will provide parameters for robust distcp performance and sketch possible pitfalls.\n- WebHDFS and HttpFS: this is a newer protocol and implementation intended to securely expose HDFS functionality. We will show how to use both to safely and rapidly transfer bulk data to a remote cluster.\n- Leveraging AWS infrastructure: we will illustrate how to take advantage of AWS infrastructure such as S3 and availability zone connectivity for moving data across geographical regions. As part of our discussion, we will present s3copy, an open source tool we have developed that facilitates copying from one S3 bucket to another.\n- Compression and Encryption: we will explore the role of preprocessing the data prior to transfer in order to reduce its size and increase security.\n- Network cross connects: this is a special case whereby the source and destination clusters are housed in the same data center but belong to different organizations. We will show the architecture necessary to achieve high bandwidth connectivity in such a situation.\n- Storage Appliance: sometimes, simply shipping a storage appliance is the speediest approach. We will offer practical guidance on selecting the right configuration and workflow in this eventuality.\nWe will conclude with a set of commonly encountered transfer scenarios and our recommendations for the right combination of technologies listed above to best attack each case.\n", "\nWhat are the essential components of a data platform? This tutorial will explain how the various parts of the Hadoop and big data ecosystem fit together in production to create a data platform supporting batch, interactive and realtime analytical workloads.\nBy tracing the flow of data from source to output, we\u2019ll explore the options and considerations for components, including:\n\nAcquisition: from internal and external data sources\nIngestion: offline and real-time processing\nStorage\nProviding data services: exposing data to applications\nAnalytics: batch and interactive\nData management: data security, lineage, metadata and quality\n\nWe\u2019ll give also advice on:\n\ntool selection\nthe function of the major Hadoop components and other big data technologies\nhardware sizing and cloud provisioning\nintegration with legacy systems\n\n", "\nNetworks are everywhere, particularly in social media.  Understanding networks can quickly reveal the key people, groups, and topics that matter most.  But the tools to collect, analyze, visualize, and gain insights into connected structures have remained complex.  Now the free and open NodeXL application makes network analysis tasks as easy as making a pie chart.  The Network Overview Discovery and Exploration add-in for Excel (2007, 2010, 2013) extends the familiar spreadsheet, enabling users to easily access networks from a range of data sources including Facebook, YouTube, Twitter, Flickr, email, message boards, Wikis, blogs, and other repositories of connections.  With simple automation tools, NodeXL users can calculate a range of network metrics, create a visualization, and generate a report highlighting key people, groups, and top URLs, hashtags, words and word pairs used in the discussion network.  Network maps have revealed many of the hidden structures of social media, defining the major differences in the shapes and structures created as people link to one another.\n", "\nMachine learning has the potential to radically improve our ability to learn from the current deluge of data. In this tutorial we will provide an introduction to modern machine learning methods, and we will show how practitioners are using machine learning to detect fraud, analyze social networks, and build personalized recommender services.\nUsing a series of case studies, we will walk you through the common tasks followed in all applied machine learning problems, from data cleaning, through model building, to predictions and finally insight.  Through this applied process we will share our intuition behind the most popular techniques and why they are best for particular applications.\nThroughout the tutorial we will show that implementing these methods and analyses is straightforward using GraphLab. GraphLab is like Hadoop for graphs in that it enables users to easily express and execute machine learning algorithms on massive graphs. We will also show how GraphLab leverages Amazon web services, advances in graph representation, asynchronous communication, and scheduling to achieve orders-of-magnitude performance gains over systems like Hadoop on real-world data.\nThis will be an active tutorial. Participants are encouraged to bring a laptop to walk through the code, execution, and commands used in the case studies. After attending this tutorial you will have the tools you need to analyze massive datasets, create scalable recommendation services, and maybe even change the world.\n", "\nApache Hadoop is rarely, if ever, used in isolation. Input data comes from other frameworks, and output results get consumed by other frameworks\u2014typically, long-running services. Scheduling many frameworks to run on the same cluster (multi-tenant) resolves problems that might otherwise lead to high costs for Big Data apps: better utilization rates, lower latency for updates, less Ops overhead, etc.\nApache Mesos is an open source cluster manager that provides efficient resource isolation for distributed frameworks, based on \u201ccgroups\u201d in Linux. It is similar to Google\u2019s \u201cBorg\u201d and \u201cOmega\u201d projects for warehouse scale computing, and has run in production at scale for over two years at Twitter. It is also a foundation for the emerging BDAS technology stack, representing a next generation beyond Hadoop.\nThis tutorial will provide hands-on experience in how to build scalable, fault-tolerant data workflows atop Mesos. Given the use of Mesos as the kernel for a data center OS, additional open source components Chronos (like Unix \u201ccron\u201d) and Marathon (like Unix \u201cinit.d\u201d) serve as the building blocks for creating distributed, fault-tolerant, highly-available apps at scale.\nWe will work with Hadoop running on a Mesos cluster, using Chronos to orchestrate Hadoop jobs and other data preparation, then using Marathon to launch a Rails + Redis application to serve the results.\nSome familiarity with shell commands on Linux is needed, and prior hands-on with running Hadoop apps would be helpful. Participants will be given access to cluster resources on Amazon AWS, and need to bring their own laptops for browser and SSH access.\nFlorian Leibert is the main author of Chronos and a contributor to Marathon. Paco Nathan is an O\u2019Reilly author (\u201cEnterprise Data Workflows with Cascading\u201d) and an expert on Mesos and Cascading use cases.\n", "\nDetails to come\u2026\n", "\nStrata Program Chairs, Roger Magoulas and Alistair Croll, welcome you to Strata Closing Keynotes\n", "\nGeoffrey Moore, Author, Speaker, Advisor\n", "\nDetails to come\u2026\n", "\nBen Fry, Principal, Fathom\n", "\nBig-data is evolving. The state of the art has gone from running large\nbatch queries over static data sets updated rarely to handling\nhigh-velocity data with low processing latency. We present a new data\nprocessing framework that is geared at processing data with a very\nhigh update frequency such as clickstreams or server web request logs.\nPredefined standing queries are answered quickly as data arrives. The\nframework is written in the Go language and thus can take advantage of\nGo\u2019s advanced concurrency primitives and extensibility. We concentrate\non programmability and provide an API that is easy to use and develop\non. We also integrate several storage mechanisms, one of which allows\nthe system to efficiently maintain aggregates of stream data in a\ncluster of PostgreSQL servers.\nDetails:\nSeveral stream processing systems have emerged in the last few years:\nTwitter\u2019s Storm and Rainbird, Google\u2019s MillWheel, etc. These systems\nreflect a real need to process high-velocity data in a way that\nprovides low-latency results. We provide a new streaming system Go\nthat concentrates on efficiency and scalability:\nParallelization:\n- Operators are defined as function closures that the system can\nexecute in parallel using data parallelism for tasks that are\nprocessing intensive\n- The system can parallelize execution even when tuple order needs to\nbe preserved\nScalability:\n- We include operators to distribute data to other nodes in a consistent manner.\n- Abstractions can be integrated with cluster management software such\nas Zookeeper.\nAlso setting this system apart from others is the storage integration.\nWe have created a data cube abstraction that closely mirrors the cube \nabstraction used in online analytical processing (OLAP). This\nabstraction allows users to define aggregate and group-by clauses and\nhave the system maintain aggregates over their streaming data. Unlike\ntraditional OLAP, our data cubes are maintained incrementally as data\narrives, allowing for low latency results.\nIn our talk, we will describe the features of our system, explain our\nsystem interface, and show some example data processing pipelines.\n", "\nIf M. Minard could visualize a complex military campaign in 1869 using just papier and a crayon, why are we stuck with static bar & pie charts?\nBig data, analytics and visualization is in its infancy.  Although it is easy to generate and gather enormous amounts of data, making sense of that data-garnering insights and delivering understanding-requires the use of sophisticated, dense, and often arcane algorithms that require data scientists to develop code and execute. Data analytics tools are complicated to install and pre-configured libraries of visuals limit the ways data can be pictured, thereby constraining understanding.  Ultimately, these limitations restrict the use big data to limited numbers of skilled and knowledgeable specialists.  This approach is not scalable and the ultimate power and value of big data can only be realized when sophisticated, elegant and easy to use capabilities are placed into the hands of the average person.\nMuch as the spreadsheet revolutionized data manipulation and analysis and placed enormous power to explore numbers at everyone\u2019s fingertips, next-generation big data visualization tools revolutionize the ability for the average person to manipulate, analyze, visualize, and explore big data.  In this briefing Justin Langseth, CEO of Zoomdata, and Gus Hunt, former CTO at CIA will highlight efforts to bring the power of sophisticated analytics and data sciences to the average user.   They will show examples of real-world user-centric design approach that: create competitive advantage by providing users and customers with a real-time data experience; enables seamless interfaces with existing systems and data through no/low-ETL implementation; and places the power to differentiate a business in users\u2019 hands by easily creating their own live infographics using the d3.js library of open source visuals.\n", "\nThere are plenty of problems in the world that could use a little insight. From poverty, to starvation, to the fair distribution of resources, to maximizing charitable donations, a little data, properly applied, goes a long way. In this session, Edgeflip and Data Science for the Social Good\u2019s Rayid Ghani, IA Ventures Scientist-in-residence and Datakind co-founder Drew Conway, and Datakind co-founder and executive director Jake Porway look at where data is making a difference today, what it promises tomorrow, and what\u2019s holding it back.\n", "\nOrganized crime is endemic in many parts of the world. When working with corrupt government officials in anti-democratic and weak states, it becomes stronger and threatens local and regional security. The proceeds from crime meanwhile are sought by Western banks, hedge funds and markets. Drew Sullivan, editor of the Organized Crime and Corruption Reporting Project, will discuss the globalization of organized crime and how big data and technology can be used to combat it.\n", "\nOur legacy information architecture is not able to cope with the realities of today\u2019s business. This is because it is not able to scale to meet our SLAs due to separation of storage and compute, economically store the volumes and types of data we currently confront, provide the agility necessary for innovation, and most importantly, provide a full 360 degree view of our customers, products, and business. In this talk Dr. Amr Awadallah will present the Enterprise Data Hub (EDH) as the new foundation for the modern information architecture.  Built with Apache Hadoop at the core, the EDH is an extremely scalable, flexible, and fault-tolerant, data processing system designed to put data at the center of your business.\n", "\nJoe Hellerstein, Founder/CEO, Trifacta, and Tutti Taygerly, VP, User Experience, Trifacta.\n", "\nData Science draws heavily on statistics, machine learning and software engineering, but these disciplines aren\u2019t much help for coming up with the right problems to solve. Thankfully, other people have already given this area much thought. Whether you are building data products, instrumenting a business, or writing reports, there are useful ideas from other disciplines that will improve your ability to frame problems, scope projects, and communicate complex results.\nWe will examine a framework for incorporating ideas from other fields (like design, argument studies, and consulting) into Data Science. In the process we will explore a number of ideas, including the four things to figure out before starting any data project and how to use common patterns of argument to refine any idea.\n", "\nThe steam engine replaced muscle with machine, laying waste to entire industries\u2014and building new ones amidst their rubble. Over the protests of Luddites, we got the weekend and the end of child labor. What at first seemed an end to jobs was a boon to productivity, and since that time the human lifespan has doubled.\nOn the other hand, technology outstrips our ability to adjust society. Machine learning, automation, software, and the rise of a service economy are concentrating wealth more than ever before. Productivity rises without a commensurate increase in quality of life or the wealth of the average citizen. It\u2019s not just blue-collar work, either: IBM\u2019s Watson, now shrunk to the size of three pizza boxes, can make better cancer diagnoses than a new medical graduate.\nIs technology creating new jobs, and ridding us of drudgery? Or is it spawning an era of rampant unemployment and class divides? That\u2019s what we\u2019ll be debating.\nThe always-popular Great Debate series returns to Strata. In this Oxford-style debate, two opposing teams take opposing positions. We poll the audience, and the teams try to sway opinions. It\u2019ll be a fast-paced, sometimes irreverent look at some of the core challenges of putting data to work.\n", "\nJoin Trifacta\u2019s founders and their customers to learn how Data Transformation is changing the way people work with data. Employing a unique combination of visualization, algorithmic suggestions and both human and machine intelligence, Data Transformation transforms raw data into clean and structured formats for analysis. By increasing data analyst productivity and giving business analysts direct access to Big Data for the first time, Trifacta\u2019s technology increases the breadth of data they work with, significantly shortens \u201ctime to insight\u201d, and enables better business decisions.\nThis session is sponsored by Trifacta\n", "\nTo harness data one must first have data. Effective collection of massive amounts of data can be a challenge in and crowdsourcing data may well be an efficient solution in some situations. The International Barcode of Life and Technical University Munich (TUM) ProteomicsDB projects are two great examples of collecting and gathering data via crowdsourcing. The former will crowdsource the collection of DNA samples around the world by enabling citizen scientists to provide insect and plant specimens in return for identification and detailed information about their organism. The aim of the consortium of institutions across 25 nations including Canada, the United States, Germany and China, is to create a database containing a DNA-based barcode for every species in the world; with a goal of 500,000 species by the end of 2015. Crowdsourcing the data via a consumer-style application is seen as key to achieving this. The TUM ProteomicsDB project focuses on crowdsourcing data from within a given scientific community, but none the less relies on crowdsourcing to fill its burgeoning data store. The project stores protein and peptide identifications from mass spectrometry-based experiments and the data assembled provides identification of proteins mapping to over 18,000 human genes representing 90% coverage of the human proteome. It currently contains more than 11,000 datasets from human cancer cell lines, tissues and body fluids and enables real-time analysis of this highly dimensional data and creates instant value by allowing to test analytical hypothesis.  The crowdsourced data stored and analyzed within ProteomicsDB can be used in basic and biomedical research for discovering therapeutic targets and developing new drugs as well as enhanced diagnosis methods. SAP is proud to be involved with driving the success of both these projects.\nThis keynote is sponsored by SAP\n", "\nLocal government is poised to reap huge benefits from open data and citizen access. Less fettered than state or federal governments, yet with access to vast amounts of information on how populations and services are working, data can pinpoint problems and make cities smarter. Join Kiran Jain, the Senior Deputy Attorney for the City of Oakland, and Shannon Spanhake, the Deputy Innovation Officer for the City and County of San Francisco, to learn how governments are changing, and being changed, by the digital age.\n", "\nEveryone knows that massive, real-time data processing is behind many of the hottest new companies in technology. But what\u2019s really going on underneath the covers? In this session, investor and technology entrepreneur Michael Abbott unboxes three startups to look at the technology, architecture, and innovations they\u2019ve harnessed to deliver their products and services.\n", "\nData science algorithms (think machine learning, clustering, outlier detection) often get conflated with the industry-standard tools and programming languages that run them. In this tutorial, John Foreman will use only spreadsheets to build models from his book Data Smart to demonstrate exactly how data science techniques work step-by-step. By building the models in spreadsheets, those who attend the tutorial will get a detailed understanding of the guts of these algorithms.\n", "\nBig data analytics drive value for companies that want to become a force in their market. Evernote, a company that is rapidly becoming one of the premier cloud-based applications, stands as an example of a young organization that has done it right. They are combining digital log data with customer data to accurately predict when users are most likely to move from a free to a paid subscription.\nIn addition to the hundreds of terabytes of online data they store to preserve their users\u2019 memories, Evernote\u2019s online application has logged record high-level activity performed via their API clients and web interfaces, growing to over 200 million recorded events per day. They are using both Hadoop and an analytic platform to deliver extreme analytic performance.\nIn this presentation, Damon Cool will discuss the analytics challenges faced by companies with rapidly scaling data, and demonstrate a working model for high-speed analytics that even an organization on a tight budget can receive actionable insight from. The presentation will also highlight Evernote\u2019s current analytics system, which was revamped in July 2012 to accommodate massive growth.\nThis session is sponsored by Actian\n", "\nWith a data-centered data center, you can make it easy for anyone to pull the data they need \u2013 and spend your time and money on building applications not plumbing. Attend this session to learn how learn how other companies have built \u2013 and benefited from \u2013 a data-centered infrastructure, and how the MarkLogic Enterprise NoSQL database platform lets you:\n\nIngest and manage all your data, documents, and semantic triples in a flexible, schema-agnostic platform \u2013 without sacrificing the ACID transactions, granular security, database management tools and other features you\u2019ve come to expect in a mature database platform\n\n\nDeliver robust, real-time search and alerting within your applications\n\n\nUse \u2013 and optimize \u2013 modern infrastructure including Hadoop and cloud to attain operational agility\n\n\nSimplify implementation of data governance requirements around security, privacy, provenance, retention, continuity, and compliance \u2013 while reducing risk, cost, and time\n\nThis session is sponsored by MarkLogic\n", "\nHBase is a distributed, scalable, big data store that is often used as the database for real time, and near real time, Big Data applications. HBase availability disruptions result in non-responsive applications, data loss etc. In this presentation, the two main areas of HBase availability concerns, namely the Region Server and the HBase master are analyzed. The use of a Paxos based co-ordination system to provide multiple active Region Servers per Region is presented. The role of the HBase Master, and a robust system for providing multiple active HBase Masters are considered here.\nThis session is sponsored by WANdisco\n", "\nData analytics is at the heart of product development at Facebook. Roughly 1000 people across the company \u2013 technical and non-technical \u2013 use the Analytics Infrastructure every day to derive insights and metrics on products, ads and for other business data analysis. Facebook\u2019s data warehouse has grown rapidly over the years, and poses unique scalability challenges (the warehouse is currently more than 250 PB in size, grew 3x in the last year and crunches more than 10 PB of data a day). The Analytics Infrastructure team at Facebook has been continuously evolving the system to handle the growing needs, both on storage and compute.\nThis talk will briefly outline the evolution of the analytics software stack in the last year and delve deeper into the data management and compute challenges and solutions at this scale. Specifically, the talk will focus on the evolution of Hive for storage and compute efficiency; and discuss the key ideas, observations and experiences in applying it to the Facebook data warehouse. In-depth topics that will be covered include storage format evolution for enhanced data compression, increasing Hadoop cluster utilization and managing multi-tenancy in the clusters.\n", "\nThe Hadoop 2.0 revolution is in full force! Organizations, companies, users are all gearing up for the major move that is from Hadoop 1.0 to Hadoop 2.0. In this talk, we will discuss what Hadoop 2.0 is about, what YARN is, how YARN changes Hadoop to be all-in-one data processing platform, what features HDFS2 unlocks and what it means to move to Hadoop 2.0. We\u2019ll discuss this major migration from 1.0 to 2.0 from various perspectives \u2013 admins, frameworks, end users & data processing platforms. We\u2019ll cover what it means for existing clusters to upgrade, how existing applications can move to Hadoop 2.0 at the same time making use of all the the great stuff that is unlocked by Hadoop 2.0 \u2013 better utilization, performance, scalability, reliability and more powerful programming models.\n", "\nCrowdsourcing marketplaces like oDesk or Amazon\u2019s Mechanical Turk give us access to people all over the world that can solve various tasks, like virtual personal assistants, image labelers, or people that can clean up gnarly datasets. Humans can solve tasks that artificial intelligence is not yet able to solve, or needs help solving, without having to resort to complex machine learning or statistics. But humans are quirky: give them bad instructions, allow them to get bored, or make them do too repetitive a task, and they will start making mistakes. In this talk, I\u2019ll explain how to effectively benefit from crowd workers to solve your most challenging tasks, using examples from the wild and from our work at Locu.\nMachine learning and crowdsourcing are at the core of most of the problems we solve at Locu.  When possible, we automate tasks with the help of trained regressions and classifiers.  However, it\u2019s not always possible to build machine-only decision-making tools, and we often need to marry machines and crowds.  In this talk, I\u2019ll highlight:\n\nHow we trained a classifier that makes judgements better than the crowd that trained it with tens of thousands of data points.\nHow to apply lessons from other fields like user interface design and cognitive science to your crowd-powered workflows.\nA hierarchical crowd of several hundred crowd workers that we\u2019ve built to be self-regulating, self-training, and offer our workers a chance at upward mobility.\n\n", "\nApache HBase is a distributed, column-oriented, key-value store for Apache Hadoop (via integration with HDFS). HBase scales to 100s of nodes easily, providing real-time, random access to petabytes of data.\nIn this tutorial, you will learn the basic elements of building a real-time application that uses HBase as a persistent data store. You\u2019ll learn:\n-The background of HBase as a datastore\n-Setting up HBase on a *nix machine\n-How to use the client libraries using hands-on exercises\n-Data modeling and schema design basics\n-Internals and design assumptions\nRequirements: Make sure to come with your laptops (Mac / Linux or access to an EC2 instance) and if possible, download HBase 0.96.0 tarball from the Apache website (http://hbase.apache.org) so we can get to work right away. The tutorial includes hands-on exercises.\n", "\nIPython started in 2001 simply as a better interactive Python shell. Over the last decade it has grown into a powerful set of interlocking components that maximize developer productivity while working interactively with code and data. These components include:\n\nThe IPython Kernel: a standalone process that executes user\u2019s code, provides features for namespace manipulation/introspection and returns output in various formats using a well specified message protocol. This is the foundation of the IPython architecture and any of the following user interfaces can connect to a kernel to leverage its capabilities.\nThe IPython Shell: an enhanced interactive shell for Python. This provides a terminal-based user interface to the Kernel for interactive work and is the traditional IPython command line experience.\nThe IPython Qt Console: provides the look and feel of a terminal, but adds support for inline plotting/figures, graphical tooltips, persistent sessions and can be embedded in other Qt applications.\nThe IPython Notebook: a web based interactive computing environment that can execute code on server-side IPython Kernels and allows the user to author documents that contain live code and its output, text, JavaScript/HTML, equations, figures and videos. These documents, which tell the complete story of a computation, can be shared online, version controlled and converted to different formats.\nNBViewer: a website for sharing and viewing IPython Notebook online\nNBConvert: a tool for converting IPython Notebooks to different formats including static HTML, LaTeX, slideshows, PDF, etc.\nIPython.parallel: A high-performance, low-latency system for parallel computing that supports the control of a cluster of IPython Kernels running on a large cluster or in the cloud.  This allows parallel computations to be done interactively and reproducibly from any of the IPython user interfaces, including the Notebook.\n\nIn this hands-on, in-depth tutorial, we will briefly describe IPython\u2019s architecture and will then show how to use the above components for a highly productive interactive computing workflow in Python.\nAn outline of the tutorial follows:\n\nCore IPython: Interactive use with the terminal-based Shell and Qt Console. IPython basics: magic commands, shell aliases, system shell access, the history system, variable caching, object introspection tools. Development workflow: combining the interpreter session with python files via the %run command, timing, profiling, debugging.\nThe IPython Notebook: Interactive usage of the notebook: The IPython display protocol: defining custom display methods for your own objects (HTML, Images, JavaScript). Integration with plotting and visualization libraries. Building and using interactive widgets that communicate between Python and JavaScript using JSON data.\nNBConvert: Converting notebooks to other formats for sharing, blogging and publication. Command line and programmatic conversions.\nConfiguring IPython: How to configure and customize IPython\u2019s various components using configuration files, extensions and command line options. IPython configuration profiles.\nParallelism with IPython: Description of the basic architecture and a few examples of its usage. This tutorial will not have time to go into much detail on this topic.\n\nThis tutorial will be very hands-on, with students encouraged to install IPython on their laptops and follow along with examples and exercises. It will be presented by two IPython core developers and project leaders.\n", "\nIn our fast-paced Big Data world, the need to quickly extract and apply meaningful insights derived from terabytes of data is rapidly making the role of data scientist essential across almost every industry.\nMany believe the perfect data scientist would be an expert in a wide variety of technical fields ranging from mathematics and statistics to data engineering and visualization, but would also have a solid grounding in fields like business, behavioral economics and customer behavior.  It\u2019s a tall order, and it\u2019s still rare to find expertise in such a wide variety of disciplines in a single individual. \nBut as more academic programs focus on developing data scientists, many organizations are left wondering how to best position their organizations to take advantage of this emerging data sciences competency.\nIn this panel discussion, experts from a variety of industries will share their first-hand experiences building and deploying teams of data scientists.  They\u2019ll discuss the different approaches they have tried \u2013 what worked well and what didn\u2019t \u2013 and share practical advice on finding and hiring data the right data scientists, building multi-faceted teams with complementary skills, positioning an organization to best take advantage of data scientists\u2019 skills, and balancing structure, culture, and mentorship to enable data science teams to succeed.\n", "\nSkytree Adviser is an expert system for machine learning. It not only chooses appropriate models, but it also evaluates them with unique algorithms for detecting miss-specifications, outliers, and other anomalies. It was developed through iterative testing on two different populations: novices (marketers, executives, students\u2026) and experts (Ph.D. statisticians, computer scientists, analysts, ...). Novices have been able to fit specialized models, such as ordinal logistic regression or spectral clustering, and experts have discovered anomalies in their model-fitting process that they had not noticed when using standard ML platforms or statistics packages. This unusual range of target users was made possible by the development of a uniquely simple interface (called a \u201cbutton tree\u201d) and a rich output format (an HTML document containing an introduction, tables, dynamic graphics, technical references and numerous links to tutorial Web sites). This tutorial will feature real data examples to illustrate the unique detective work that Adviser provides for discovering patterns in data that most analysts and programs tend to miss. Because of the flexibility of the Adviser design, participants ranging from Excel users to advanced analysts can benefit from this tutorial.\n", "\nThe cloud provides an easy onramp to building and deploying Big Data solutions, particularly the latest technologies that favor scale-out architectures. Transitioning from initial deployment to a large-scale, highly performant operation without breaking the bank may not be easy.\nUnderstanding the benefits, weaknesses, and performance characteristics of public cloud and bare metal cloud deployments and their cost impacts can help you make the right decisions. The goal is to provide some insight into how to select the correct deployment strategy based on your Big Data application\u2019s needs. This insight is based on our extensive testing while creating Big Data solutions on our bare metal cloud platform as well as various other virtualized deployment options.  We will focus on real world deployment strategies and focus on the economic impacts of various deployment strategies at scale.\nThis session is sponsored by Softlayer\n", "\nLife is made of data, and the right equipment can track everything to build a rich picture of me. But life consists of millions of tiny, intuitive decisions that we make every day without even being aware of them. Cognitive science tells us that when we\u2019re having a conversation with another human, only about 5% of the meaning we extract from that conversation actually comes from the words that are spoken. The other 95% is based on body language, facial expressions, tone of voice \u2013 even smell. We process these stimuli so quickly that we\u2019re not even conscious we\u2019re doing it \u2013 so, what we consider our intuition may in fact be evidence-based decision making, just at hyper-speed.\nA lot of the vision pieces out there these days paint a world where decisions are made for us based on data. On the one hand, this is appealing \u2013 machines taking on tasks and simple choices we can\u2019t be bothered to make. On the other hand, it\u2019s disorienting \u2013 what is the logic underlying these decisions? The rapid choices we make are part of what make us uniquely human. They determine our experiences, and the fact that we don\u2019t understand quite how we make them should beg the question of whether we are ready to teach machines to make them too. No array of sensors, no matter how clever, can equal our human choices, and this creates an unprecedented design challenge in having jurisdiction over what\u2019s measured and how it\u2019s interpreted.\nThe answer is for designers to keep their focus on the things that make us uniquely human, and to allow for those things to shine through the interactions, services and systems that we build with big data. If people are afraid of big data, perhaps it\u2019s because we know we\u2019re not machines, and we shouldn\u2019t aspire to be. The role of design in this is not primarily the creation of artefacts, digital or otherwise \u2013 it is to make a translation between the abstract and the human, and to make connections between humans and the things they need, want and love. Sometimes that means exposing the mechanics, sometimes that means hiding the complexity.\nShelley Evenson, Executive Director, Organizational Evolution at Fjord, will outline the key tenets of designing for big data: simplicity, show (don\u2019t tell) and bringing context to data. She will also discuss how you can overcome issues such as the difficulty in selling simple solutions, how to avoid resorting to wireframes and how you can make data accessible so people can explore and play with it to see emerging patterns, instead of relying on key numbers.\n", "\nComplex distributed systems and services present new problems for traditional monitoring and debugging tools. The systems are significantly larger, produce significantly more data, and have significantly more interconnected and interdependent components which must be considered. As existing monitoring solutions are no longer able to keep pace, it is vital to develop new tools and architectures to enable developers and analysts to understand and scale the large systems of today. The Twitter Observability stack is one such tool set which operates at a very large scale, monitoring the distributed system which is Twitter.\nThis talk will cover the end to end architecture of the stack, starting from instrumenting services and applications, through the storage technologies for managing millions of time series, and finally to the analytic and monitoring tools enabling users to use data to build and maintain large scale distributed systems. Learnings will be shared on how to make many classes of users happy, including developers and operations staff, in addition to lessons on how to build and scale monitoring stacks for your application, whether big or small.\nOutline\n\nStarting with good data \u2013 what you measure is one of the most important aspects.\nThe importance of low-level and high-level aggregate data in distributed systems, including the importance of data dependencies.\nLessons learned and architecture of collecting and moving data from the applications through the system.\nStorage and how to manage explosive, continual growth of time series and trace datasets.\nHow to delight users with powerful tools to query and visualize their data\nAutomatic monitoring and alerting: how to scale automated monitoring and prevent failures before they happen.\n\n", "\nThis session introduces the Sidekick Pattern: using small amounts of carefully curated data to increase the value of big, messy data sets. Common sources for data sidekicks include surveys, small experiments, crowdsourcing, and public APIs.  In the first year of data science at Jawbone, we have used the Sidekick Pattern many times to augment data streams from the UP fitness tracker: movement, sleep, workouts, and food.\nWith the right tools, data sidekicks are easy to create, and they can accelerate analysis, solve cold start problems, and simplify complicated data pipelines.  As a result, data sidekicks are a valuable technique for enabling rapid prototyping, securing organizational buy-in, and bringing new data products to market faster.\nThe session is geared towards practical answers to the following questions:\n\nWhat makes a good data sidekick?\nWhat are the advantages and disadvantages of using data sidekicks?\nHow can I recognize opportunities to use the sidekick pattern?\nWhat free/low-cost tools can accelerate the sidekick pattern?\nWhat are common mistakes when applying the sidekick pattern?\n\n", "\nHappy accidents can influence one\u2019s creative process. Ian Timourian will discuss his exploration of the algorithms and techniques utilized by the famous poet Gertrude Stein through visualization. This talk touches on important topics such as the intersection of art and data visualization, novel techniques for the visualization of n-grams, methods for turning poetry into music, the joy (and challenges!) of data exploration, mashups, generative art, and more.\n", "\nFarrah Bostic, Founder, The Difference Engine.\n", "\nHow do we know how many people have been killed in Syria?  If violence\nis escalating or decreasing?  The hard answer is we don\u2019t.  But through\ncareful application of machine learning and other statistical\ntechniques, we can quantify what we do, and don\u2019t, know. In this talk\nMegan will present how the Human Rights Data Analysis Group uses random\nforests, multiple systems estimation, and various Python and R packages\nto estimate conflict casualties.\n", "\nApache Hive is the de-facto standard for SQL-in-Hadoop today, with more enterprises relying on this open source project than on any alternative. Enterprises have asked for Hive to become more real-time and interactive\u201a and the Hive community has responded.\nPlease join Arun Murthy, Owen O\u2019Malley and Alan Gates to learn more about Stinger and improvements to Apache Hive, how much Hive has grown in the last 12 months, and how much further it will soon go.\nAlan, Arun & Owen will cover how Hive:\nIncreases performance and scale by simplifying tasks using Apache Tez (Arun)\nDecreases file size and effectiveness with the ORC file (Owen)\nExpanding OLAP functionality, data type conformance, and adding ACID compliant updates (Alan)\nThis session is sponsored by Hortonworks\n", "\nDetails to come\u2026\n", "\nRosanne Haggerty, President & CEO, Community Solutions\n", "\n3-Hours: This workshop provides a detailed discussion of the new features of Apache Hadoop 2.0. We will discuss how YARN turns Hadoop from a single use system for batch data processing into a multi-use platform for storing and processing data in many ways other than batch. We will also discuss the details of the new HDFS improvements like High Availability, Federation, and Snapshots.\nApache Hadoop 2.0 is not just a major release number, but represents a generational shift in the architecture of Apache Hadoop. With YARN, Apache Hadoop is recast as a significantly more powerful platform \u2013 one that takes Hadoop beyond merely batch applications to taking its position as a \u2018data operating system\u2019.\nIn this presentation, we will discuss the details of YARN and provide an overview of how you might develop your own YARN implementation. We will also discuss the components of HDFS High Availability, how to protect your enterprise data with HDFS Snapshots, and how Federation can be used to utilize your cluster resources more effectively. We will also include a brief discussion on migrating from Hadoop 1.x to 2.0.\nAttendees should be familiar with the basic components of Hadoop 1.x, and should bring pen and paper for taking notes.\n", "\nWhile the first big data systems made a new class of applications possible, organizations must now compete on the speed and sophistication with which they can draw value from data. Future data processing platforms will need to not just scale cost-effectively; but to allow ever more real-time analysis, and to support both simple queries and today\u2019s most sophisticated analytics algorithms. Through the Spark project at Apache and Berkeley, we\u2019ve brought six years research to enable real-time and complex analytics within the Hadoop stack.\n", "\nStrata Program Chairs, Roger Magoulas and Alistair Croll, welcome you to the first day of keynotes.\n", "\nBuilding systems for gathering and storing large amounts of data is a problem that engineers can wrap their heads around. We have many good tools at our disposal now as the result of the work of many engineering organizations solving this for themselves. There are still hurdles, especially as data warehouses grow into the exabyte scale, but the technical issues are largely understood.\nWhat to do with the wealth of data that we\u2019re storing, on the other hand, is more nebulous. Tools are being constructed to slice and dice and dashboard your data, but there is still a level of dissatisfaction with what is available. Data analysis tools built by engineers are enormously powerful, but difficult to use. Pretty dashboards tend to be more limited in ways that you can deal with your data, and let\u2019s face it, a truly well-designed interface for a data analysis tool is a rare thing. In this presentation I will discuss my dissatisfaction with the design of dashboarding tools, how I approached the decision to build something new, and what my goals are for StatusWolf.\nI will talk about the motivations behind this project, the process of finding and dealing with the disparate data sources being used, the design philosophy and iteration of the interface and user experience, and the technical aspects of collecting, aggregating, analyzing and presenting data for humans to understand. Most of all I will discuss what I\u2019ve learned about data presentation and analytics through this process, and how my background in art and design informed my engineering brain.\n", "\n2014 is expected to be the year when Hadoop goes mainstream. A majority of companies are expected to move Hadoop from small-scale test environments into full-fledged production deployments. So what does it take to put Hadoop in production? How do you keep your Hadoop operational costs low?\nOur guest Mike Gualtieri, principal analyst at Forrester Research, Inc., will facilitate a panel of production Hadoop users \u2013 including enterprise companies such as Cisco, a leading Telco as well as a prominent web 2.0 company \u2013 to discuss the challenges and best practices for deploying Hadoop in production. Join us for an engaging conversation on tips and tricks in deploying Hadoop in production.\nThis session is sponsored by MapR Technologies\n", "\nThis five-minute keynote will provide a quick overview of some of the more surprising things Hadoop is capable of in 5 minutes or less.\nThis keynote is sponsored by MapR Technologies\n", "\nTo seize the future data must be harnessed in actionable time. Based on a real deployment see how you can achieve instant results with infinite storage. This example filters large amounts of cold data in Hadoop, analyzes it in Real-Time in the SAP HANA platform and visualizes it with SAP Lumira. Apache Hadoop is often seen as synonymous with Big Data and has found a place alongside established databases and data warehouses in today\u2019s enterprise. The SAP HANA platform uniformly amplifies the value of Big Data across this data fabric including working with data sets that are stored in a variety of places including Hadoop.  This session will demonstrate how solutions from SAP and our Hadoop partners can help your organization seize the future, harness and gain unprecedented insight from Big Data.\nThis session is sponsored by SAP\n", "\nHow do you keep up with the velocity and variety of data streaming in from\nthe operational systems that power your business? What about getting\nanalytics on your data even before you persist and replicate it? In this\ntalk, Comcast, one of the world\u2019s leading media, entertainment and\ncommunications companies, describes their blueprint for collecting,\norganizing and deriving real-time operational intelligence from big data,\ndriving superior customer experience and delivering a critical 360 degree\nview of their next-generation media service.\nThis session is sponsored by Splunk\n", "\nHadoop has proven itself and the Big Data market demands ever more powerful and real-time analytics.  \u201cIn-Hadoop\u201d Analytics is the next major step towards that ambitious goal. New open source technologies like Spark and Shark, paired with text and machine learning analytics will change how you manage and analyze your data. This session covers how these advancements will change your enterprise.\nThis session is sponsored by IBM\n", "\nData warehouses arose from the need to gather and standardize enterprise information for decision support. Over the past few decades the relational database management system (RDBMS) has been the technical basis for these architectures, with various tradeoffs. However, recent increases in the amount and diversity of data available for analysis, and the availability of new tools to manage this data, have caused many enterprises to rethink their data warehousing strategies. In this talk, we\u2019ll explore how Apache Hadoop has rapidly evolved to become the new foundation for enterprise analytics \u2013 the enterprise data hub \u2013 and learn about the state-of-the-art in deploying a modern data warehouse on top of the Hadoop stack.\nThis session is sponsored by Cloudera\n", "\nComputational devices and systems are constantly telling their stories in the form of logs: logins and logouts, network connections and disconnections, sensor readings, customer interactions, database activities, configuration changes, and application errors. These logs can often tell us important information about the operational health, security, and usage patterns of our systems.\nHowever, the scale and variety of raw log data can pose a formidable challenge. Logs come in many different formats, and these formats themselves evolve over time.  For large log volumes, interactive single-machine processing is not an option.  Random fluctuations and transient disruptions can make static rules and thresholds prone to \u201cfalse positive\u201d alerts.\nIn this talk we will discuss a few ways in which machine learning techniques can be combined with human guidance in order to understand what the logs are telling us:\n\ndetection of system-wide changes in behavior \n\u201clearning by example\u201d to identify events \npartially supervised discovery of log structure\ninferring log relevance\ngraph mining of logs \ntime-series modeling of log metrics\n\nWe\u2019ll also give examples of interesting findings uncovered by these applying these approaches to large volumes of real data in a production log-management service.\n", "\nA new generation of businesses is betting that data-driven products can give them the edge. From efficient marketplaces to frictionless transactions, from better customer interaction to prescient predictions, from platforms that can mine vast troves of data instantly to catalogs of information available for sale, the startup ecosystem loves Big Data.\n\n\n\nStartups that put data first aren\u2019t just able to adjust faster and find their product and market better. They\u2019re also reconsidering aging industries and ailing infrastructure, upending incumbents and breaking down barriers to entry.\n\nStartup Showcase returns to Strata in 2014. It\u2019s your chance to hone your pitch, meet investors, and bounce your big idea off the industry\u2019s movers and shakers.\nOur team of investors, entrepreneurs, and industry analysts will select ten leading big data startups from all the submissions we receive. These ten firms will have a chance to present their technologies and tell their stories live at the Startup Showcase on Tuesday, February 11; then our judges will choose the best of the best, which we\u2019ll announce from the stage on February 12.\nInterested in highlighting your big data startup, with its disruptive, mind-blowing new technology? Submit a proposal for the showcase.\n", "\nNewSQL has followed quickly on the heels of NoSQL \u2013 providing scale-out of NoSQL along with SQL and ACID guarantees. We\u2019ll discuss NewSQL with customer examples and contrast it with SQL on Hadoop implementations. Real-time analytics is emerging as a competitive differentiator for multiple businesses, and NewSQL databases are increasingly providing it. We\u2019ll discuss distributed joins \u2013 the workhorse of analytics with a database architecture deep dive. Finally we\u2019ll round off with operations \u2013 the big problem with big data.\nThis session is sponsored by Clustrix\n", "\nMany organizations are using Apache Hadoop software to store and process data cost-effectively, but extracting knowledge from the data remains a challenge. Advanced techniques like graph analytics are capable of yielding powerful insights into unexpected and unknown relationships but technology adoption is hindered by scarce expertise, laborious workflows, and platforms incapable of constructing and analyzing large-scale graphs. In this talk, Dr. Ted Willke, General Manager of Intel\u2019s Graph Analytics Operation, will discuss Intel\u2019s efforts to deliver a graph cluster solution that is as easy to work with as it is powerful. Ted will present large-scale graph analytics case studies, describe Intel\u2019s open-source based graph cluster platform, and demonstrate how easy it is to program a commercial-quality data workflow.\n", "\nUnderstanding user sentiment, improving user engagement, and maximizing ROI for the advertising dollar spent without harming user experiences are all crucial to Yahoo!\u2019s business.  In order to effectively perform these tasks, we ingest hundreds of TB of advertising data every day on Hadoop clusters of thousands of machines. Many of the algorithms we use to measure user engagement can be modeled as multiway self-join queries that are very expensive to compute on very large datasets.\nThe challenge we face is how to effectively query this vast amount of information and come up with interesting insights. Over the course of the last year, we have been developing a new data platform for user affinity analysis using Shark and Spark.\nIn this talk, we discuss our use cases, and the advanced streaming algorithms we have implemented on top of these platforms, and the general architecture to provide interactive, real-time insightful analytics to our data scientists. The deployment of these new systems, along with the novel algorithms (min hashing, mod hashing, other sketches) can reduce the runtime of such analytics from hours to seconds.\n", "\nThere is an exponential growth in data that is being collected and stored. This has created an unprecedented demand for processing and analyzing massive amounts of data. Furthermore, analysts and data scientists want results fast to enable explorative data analysis, while more and more applications require data processing to happen in near real time.\nIn this talk, we present BlinkDB, which uses a radically different approach where queries are always processed in near real time, regardless of the size of the underlying dataset. This is enabled by not looking at all the data, but rather operating on statistical samples of the underlying datasets. More precisely, BlinkDB gives the user the ability to trade between the accuracy of the results and the time it takes to compute queries. The challenge is to ensure that query results are still meaningful, even though only a subset of the data has been processed. Here we leverage recent advances in statistical machine learning and query processing. Using statistical bootstrapping, we can resample the data in parallel to compute confidence intervals that tell the quality of the sampled results. To compute the sampled data in parallel, we build on the Shark distributed query engine, which can compute tens of thousands of queries per second.\nBlinkDB is being integrated in Shark and in Facebook Presto and is also in the process of being deployed at a number of companies. This talk will feature an overview of the BlinkDB architecture and its design philosophy. We will also cover how the audience can leverage this new technology to gain insights in real-time using a variety of real-world use cases from our early adopters.\n", "\nThis tutorial, the first of a two-part big data analysis training series, will present three new cutting edge components of the Berkeley Data Analytics Stack (BDAS), as well as provide a brief introduction to the stack as a whole. Currently under development in the UC Berkeley AMPLab, BDAS currently contains eight components that are tightly integrated with each other and with popular components of the Hadoop ecosystem. We will start by covering Spark (a high-speed cluster computing system engine), Spark Streaming (the real-time processing system built on Spark), and Shark (the SQL component on top of Spark). Then we will dive into three newly released components of BDAS. First, BlinkDB, a distributed query that provides ultra-low latency results via approximate, error bounded results. Second, MLbase, a platform for implementing and consuming machine learning algorithms at scale. Finally, Tachyon is a fault tolerant distributed in-memory file system enabling reliable file sharing at memory-speed across cluster frameworks, such as Spark and Hadoop MapReduce.\n", "\nNetflix is a data-driven company. While \u201cdata-driven\u201d is often no more than a lofty buzzword, we\u2019ll discuss how we make it a reality.\nWe\u2019ll dive into the technologies we use and the philosophies underpinning how we get things done.\nWe\u2019ll cover our transition to a \u201ccloud-native\u201d data infrastructure (at massive scale), and how this has been a huge enabler for us and is more broadly the \u201cright\u201d way to go.\nWe\u2019ll talk about how we heavily leverage open source software and how and why we are contributing back (e.g. \u201cGenie\u201d \u2013 our Hadoop platform-as-a-service, and \u201cLipstick\u201d \u2013 our Pig visualization and monitoring tool).\nWe\u2019ll cover our unconventional approach to development, which focuses on enablement / eliminating process. This includes self-service promotion to production (no QA team and no DBAs \u201capproving\u201d your changes) coupled with a rich ecosystem of tools and automation.\nThe goal of this presentation is for you to walk away with your head spinning on new and better ways to get the most out of the data at your company.\n", "\nThe maturation and development of open source technologies has made it easier than ever for companies to derive insights from vast quantities of data. In this session, we will cover how to build a real-time analytics stack using Kafka, Storm, and Druid.\nAnalytics pipelines running purely on Hadoop can suffer from hours of data lag. Initial attempts to solve this problem often lead to inflexible solutions, where the queries must be known ahead of time, or fragile solutions where the integrity of the data cannot be assured. Combining Hadoop with Kafka, Storm, and Druid can guarantee system availability, maintain data integrity, and support fast and flexible queries.\nIn the described system, Kafka provides a fast message bus and is the delivery point for machine-generated event streams. Storm and Hadoop work together to load data into Druid. Storm handles near-real-time data and Hadoop handles historical data and data corrections. Druid provides flexible, highly available, low-latency queries.\nThis talk is based on our real-world experiences building out such a stack for online advertising analytics at Metamarkets.\n", "\nMost are familiar with A/B testing of websites \u2013 a scientific method used to evaluate the effect of one factor by conducting a controlled experiment using a treatment site (with the new/changed factor) and a control site. A/B testing is just one type of experimental design \u2013 approaches used to test multiple factors and their interactions. In this session, we will walk through case studies to help participants understand how broadly they can apply experimental design principles throughout their projects, architectures and enterprises.\nThe term \u201cDesign of Experiments\u201d refers to a systematic approach to understanding causality using data collection along with applied statistical techniques. Properly designed experiments control the environment of data collection, but this does not necessarily mean a carefully constructed lab in a clean building somewhere. In fact, most of us participate in designed experiments every day by simply browsing the web.\nApplications of experimental design have come a long way in the last century. Its early industrial applications started in Agriculture in the 1930s. In the last several decades, it became widely popular in the manufacturing industry with the practice of Six Sigma, and has recently been used in website optimization.\nIn this session, we\u2019ll review the \u201cdesign\u201d and the \u201cexperiment\u201d side of Design of Experiments, from systematic data collection to basic statistical applications including experiments with multiple factors and discuss its future beyond the web. We will discuss examples ranging from understanding the performance impact of system improvements to understanding customer sensitivity to electricity prices.\n", "\nThis tutorial will provide hands-on training for BlinkDB, MLbase, Spark, and Shark, components of the Berkeley Data Analytics Stack (BDAS). We will provide each audience member access to an EC2 cluster pre-loaded with real-world datasets, and walk them through hands-on exercises analyzing these data using the aforementioned technologies. The exercises will cover brand new components, including BlinkDB, a distributed query that provides ultra-low latency results via approximate, error bounded results. Another new component covered  is MLbase, a platform for implementing and consuming machine learning algorithms at scale.\nAdditonally, we will learn to use more mature components of the stack including the Spark and Shark command line interfaces for ad-hoc analysis that take advantage of Spark\u2019s in-memory caching primitives to speed up queries by an order of magnitude. The lessons will include Spark Streaming, the real-time component of Spark.\n", "\nData products are the driving force behind new multi-billion dollar companies and a lot of the things we do today on a day to day basis have machine learning algorithms behind them. But unfortunately, even though data science is a concept invented in the 21st century, in practice the state of data science is more similar to software engineering in late 20th century.\nThe pioneers of data science did a great job of making it very accessible and fairly easy to pick up, but since it\u2019s beginning circa 2005, not much effort has been made to bring it up to par with modern software engineering practices. Machine learning is software. As such, it should follow standard software engineering practices,, however, the current tools of the trade are not modular, maintainable or reusable.  In this tutorial we will learn to work with Scalding, a Scala DSL which provides both the simplicity of languages like Apache Pig, and the power of a functional fully JVM language.\n", "\nApache Hadoop has become an attractive platform for exabyte-capacity data storage. In the enterprise data warehouse (EDW) area in particular, Hadoop now increasingly serves as complementary technology for cost-efficient data loading and cleaning, supporting the EDW\u2019s role in enabling interactive analysis and reporting on relational data. However, thanks to recent advances in the Hadoop ecosystem that expand the range of EDW-equivalent analytic capabilities entirely in open source software, it is now also possible for Hadoop to serve as a virtual EDW for native Big Data (stored in HDFS). Thus, costly processes for moving that data into the traditional EDW just for the purpose of analysis are no longer required.\nIn this session, attendees will get an architect-level view of this solution (comprising HDFS, Cloudera Impala, and the Parquet columnar storage format) and explore an example configuration and benchmark numbers that demonstrate how it offers a high level of performance, functionality, and ability to handle a multi-user workload, while retaining Hadoop\u2019s traditional strengths of flexibility and ease of scaling.\n", "\nAre you looking to deliver business value from Big Data & Hadoop? Would you like to help your organization find new markets, new customer segments & new product categories with Big Data & Hadoop? Would you like to quickly uncover rich & powerful business insights from your Hadoop deployment? If yes, then this session is for you.\nAttend this session to learn how leading organizations across the world are using a Data Discovery platform in conjunction with Hadoop to uncover  rich & powerful business insights. This session will answer questions like what a Data Discovery platform is and how a Data Discovery platform makes it easy, fast & powerful to perform Big Data Analytics & Discovery and uncover game changing insights.\n", "\nIPython with its notebook interface is an interactive programming environment that is particularly well suited for data exploration, modelling and sharing of analysis results notably via nbviewer.ipython.org.\nScikit-learn a versatile Machine Learning library for Python that blends well with the NumPy and SciPy ecosystem and is used by a growing user-base of both academic researchers and data scientists and engineers in the tech industry.\nThe two projects offer together a productive environment for building and evaluating predictive models from data. In particular IPython distributed computing capabilities make it possible to offload computational intensive Machine Learning tasks to clusters of tens or hundreds of nodes without breaking the interactive experience.\nThe goal of the presentation is to showcase how to setup an ad hoc data modelling environment using a cluster provisioned in a public cloud and use it perform common predictive modelling operations such as:\n\ncross-validated model assessment and automated search for the best parameters for common feature extraction and machine learning algorithms,\nparallel training of out-of-core text classification models for sentiment analysis,\nparallel training of large randomized ensembles of decision trees (a.k.a. Random Forests).\n\n", "\nScikit-learn is a versatile Machine Learning library for Python that blends well with the NumPy and SciPy ecosystem and is used by a growing user-base of both academic researchers and data scientists and engineers in the tech industry.\nIPython with its notebook interface is an interactive programming environment that is particularly well suited for data exploration, modelling and sharing of analysis results notably via nbviewer.ipython.org.\nThe objective of this tutorial is to get acquainted both with Machine Learning concepts in general and the pydata ecosystem in particular.\nThe session will cover the following topics:\n- how to extract a Machine Learning friendly representation of raw data (feature extraction),\n- how to train various machine learning models such as Logistic Regression, Support Vector Machines and randomized ensembles of decision trees,\n- how to evaluate the predictive accuracy of a model and detect overfitting,\n- how to automatically tune the model parameters from data.\n", "\nLearn how AWS thinks about big data and how we and our customers have approached managing large datasets using services such as Amazon S3, Amazon Elastic MapReduce, Amazon DynamoDB, and Amazon Redshift. We\u2019ll discuss the key elements of good big data architecture, including elastically growing your resources as you need them, using the right tools for the job, and paying only for what you use. We\u2019ll provide a holistic view of how you can leverage AWS tools to support your structured and unstructured data stack through specific examples from diverse customers.\n", "\nWe optimize ads, but not our mood. We know more about our tweets than our own bodies. That\u2019s all about to change. As wearables transform the \u2018quantified self\u2019 from a niche to a mainstream market, they are generating vast amounts of data about our health, habits, and lifestyles.\nWith sleep data gathered from hundreds of thousands of UP wrist bands, we can push the boundaries of understanding our bodies beyond what was possible with traditional studies alone. We can now understand not only whether men sleep longer than women, but also how that changes with age. We know that Americans lost sleep over the Boston Marathon bombing, but not over the birth of the royal baby. We can study how jetlag affects your body, at scale. We can find out how air quality affects your movement and your health. These insights are key to encouraging people to get more sleep, rewarding them when they do, and improving the quality of their lives along the way.\nJoin us if you\u2019d like to hear more about sleep, health, and a world where an ecosystem of sensors is changing what we know about ourselves.\n", "\nDespite all the recent advancements in the operations management field, data center management today still largely remains as a black art. Administrators have limited visibility into their data center operations today and yet they have to make important operations management decisions every day. A typical data center generates about a Billion data points every day. A lot of insight could be gathered from this data but due to the large volume and scale, on-premise software solutions only collect limited subset of this data. This limits them to a very narrow view of the data center.  We at CloudPhysics have taken a different approach to this problem. We created an analytics platform in the cloud, that provides the ability to query, slice and dice and mashup the data with multiple data-sources. This approach not only yields incredible insights but also solves many of the teething operational management issues that have not been solved before. In this talk we give an overview of the data center metadata and provide details on how CloudPhysics handles this data at scale using its platform.\n", "\nData Governance adds value to enterprise information assets by connecting uniquely human intelligence to data triage and mark-up challenges \u2013 making the data work for a variety of use cases by assuring its interoperability.  Yet as data volumes increase exponentially in volume and variety, Data Governance must do more.  Without sacrificing that golden core of specialized human acumen, it must become more automated, more context-aware, and more intelligent.  Some of that burden will rest on the data stewards, and even more will be required of Data Governance\u2019s supporting technology stack.\nTaking lessons from such varied sources as municipal building codes and game theory, Data Governance SME Rachel Haines and information strategist Scott Lee will discuss their vision for the bright future of Big Data Governance.   Such a future\u2026\n\nIn which highly advertent, intelligent processes will constantly be at work to improve the enterprise\u2019s semantic metadata assets\n\n\nWhere context is king and all data leverage will be sensed, tagged, and understood as part of a conversation and locale\n\n\nThat enables governance and on-boarding of third party, poly-structured, external, and proprietary data as easily and thoroughly as simple internal, structured enterprise data\n\n", "\nRecognizing that the fossil fuel based economy of the present must give way to a renewable energy based economy of the future, the Harvard Clean Energy Project set out to discover and design new molecular materials for the next generation of organic solar cells. These carbon-based photovoltaics offer a path to a simple, cost-effective, and high-volume production of renewable energy devices with exceptionally versatile features. They could in particular bring electricity to the estimated 2.5 billion people around the world living in rural areas without access to the power grid. The project is sponsored by the White House Materials Genome Initiative and part of the Global Climate and Energy Project.\nBy harnessing the immense computing power of the IBM World Community Grid (a distributed volunteer computing platform), the research team at Harvard University performs quantum chemical calculations on millions of organic material candidates. The obtained electronic properties are used to determine which compounds are most promising for high-performance materials.\nCurrently, Harvard\u2019s Clean Energy Project has studied 2.3 million compounds with 24 million conformers in 150 million density functional theory calculations. It thus represents the most extensive first-principles quantum chemical investigation ever conducted. Each computational characterization of a molecular motif produces about 20-40 megabytes (MB) of data, and the project collects approximately 750 gigabytes (GB) of data each day. So far, the data archive has grown to about 400 terabytes (TB). To store the results of this massive investigation, the scientists at Harvard have built large data storage arrays called \u201cJabba\u201d, based on a design by Backblaze Inc. Each array utilizes 45 3TB hard drives from HGST, a Western Digital Company. Harvard has designed their Jabba arrays with built-in redundancies (RAID and tape backup) to ensure the integrity of the valuable research data. The key to the arrays\u2019 performance is the use of reliable, high-capacity, and low-power storage from HGST. Harvard has filled over 150 HGST drives to this point and has recently commissioned Jabba 5 and 6 to increase its capacity to 700TB. The project may well accumulate a petabyte of results by the time it winds down.\nThe virtual high-throughput approach of the Clean Energy Project allows the study of material candidates on an unprecedented scale. It eclipses the possibilities of experiment or traditional computational modeling by 4-5 orders of magnitude. The necessary parameter space for high-performance materials is very narrow and the search for suitable candidates correspondingly difficult. The presented large-scale screening, however, still provides 1000 candidates with the prerequisites for a power conversion efficiency of 11+% and 35000 candidates of 10+%. The most promising compounds are forwarded to experimentalist partners. The results are also used for the cheminformatics analysis of structure-property relationships and thus provide the foundation for the rational design of new leads. In June 2013, the data became available in an open and free reference database for the community.\nIn this session, Alan Aspura-Guzik, Professor of Chemistry and Chemical Biology at Harvard University and the hands-on lead/practitioner for the Harvard Clean Energy Project will inform attendees about the tools and techniques they acquired during this project around data mining, analysis, machine learning, drug discovery, and pattern recognition. He will outline best practices and lessons learned from this big data project that will benefit mankind aiding the quest for clean energy solutions, bringing electricity to billions around the world, and improving their quality of life.\n", "\nBeing \u201cdata-driven\u201d is about more than just storing lots of data and generating reports. As with many other types of projects, the most crucial part of any data-oriented project is choosing an appropriate problem or opportunity on which to focus in the first place. In this tutorial, you will learn how to apply design thinking to identify problems and opportunities where data can be used as part of a solution. We will go through a series of small-group exercises where we focus on defining problems, considering current solutions, creating new approaches, and building prototypes. Participants will leave armed with a new perspective on how to use data as a resource within their own organizations.\n", "\nHow do we optimize social change? How do we connect users to the change they want to see?\nDuring this talk, you will learn how Change.org uses machine learning and recommendation algorithms in order to optimize user engagement, revenue, and impact. More specifically, I will cover:\n\nhow we built one of the most sophisticated machine-learning based email targeting tools ind the industry using finely tuned feature engineering and distributed random forests\naggregating multiple data sources (collaborative filtering-based systems, social recommendations, topics a user follows, geographic information) to construct a single feed for a user\nhow we deal with the cold start problem\nthe different collaborative filtering techniques we use and how we scale them\nhow we enrich these techniques with supervised topic generation\n\n", "\nRecent years have seen an explosion of technologies for managing, processing and analyzing graphs. While the most well known users of graph technologies have been social web properties such as Facebook and LinkedIn, a quiet revolution has been steadily spreading across other industries. In this last 18 months, more than 30 of the Global 2000, and many times as many startups, have quietly been working to apply graphs to a wide array of business-critical use cases.\nFor example: one of the world\u2019s top parcel delivery carriers wasn\u2019t going to be able to handle Christmas volumes last year because of numerous challenges stemming from online ordering. The solution? Replace the legacy routing system with a graph database, which now routes 5M packages per day in real time: faster and more efficiently than its relational cousins ever could. One of the top investment banks now onboards traders using an identity & access management system based on graphs. Media metadata turns out to be best represented as a graph; and consumers respond well to the opportunity to visually navigate the graph (such as is done by the app Discovr Music). Similar trends are developing in telecommunications, healthcare, human resources, gaming, and many more.\nWe are entering an era of connected data: where those companies that can master the connections between their data \u2013 the lines and patterns linking the dots, and not just the dots \u2013 will outperform the companies that fail to recognize connectedness.\nThis session will discuss two major topics: the emergence of graphs and graph databases outside of the \u201cclassic\u201d social use cases, and the factors driving this trend. We will draw from both open source community projects, and real-world early commercial adopters in verticals like finance, telecom, healthcare, life sciences, and more. You will leave this session with an appreciation for and understanding of how one can reap the benefits of applying graph technologies outside of managing the social graph.\n", "\nThe web, through powerful libraries such as d3.js, is transforming how we work with data. However, the browser is a long way from where most of our code/data actually lives, namely, on the server side. To leverage the power of JavaScript/HTML/CSS, users must develop custom web applications for each problem. This is much too painful for exploratory data science, where the end goal is a moving target.\nIn this talk I will describe how the IPython Notebook solves this problem by reducing the distance between a user and their code/data. This is accomplished by a new widget architecture that makes it easy to leverage the power of both JavaScript/HTML and Python in exploratory data science.\nThe IPython Notebook is an open-source, web-based interactive computing environment for Python, R, shell scripts, Julia and other languages. At its core, the Notebook is an environment for writing and running code in an interactive and exploratory manner. On top of this foundation, it adds a document based workflow: Notebook documents contain live code, descriptive text, mathematical equations, images, videos and arbitrary HTML. These documents provide a complete and reproducible record of a computation and can be shared with others, version controlled and converted to a wide range of of static formats (HTML, PDF, slides, etc.).\nAfter reviewing the basics of the IPython Notebook, I will describe its underlying architecture and the recent extensions to support interactive JavaScript/HTML widgets. The IPython architecture consists of  separate server-side processes, called Kernels, that run user\u2019s code (in Python, R, Julia, etc.) and returns output to the browser over a JSON based message protocol (over WebSockets and ZeroMQ). This message protocol now includes the ability to synchronize the state of JavaScript objects living in the browser to computations and data living in the Kernel. The result is that data scientists doing exploratory work can easily leverage both modern web technologies, such as d3.js, and the computational capabilities of languages such as Python, R and Julia in a single integrated environment. Examples of applications of these capabilities include:\n\nParameters in Kernel side computations can be coupled to UI controls (sliders, drop down menus) in the browser.\nBrowser side visualizations can be backed by Kernel side data structures (Data Frames, networks/graphs, time series) and their states can be synchronized based on user interactions.\nKernel side data structures can be edited and created using browser side UIs.\n\nI will describe the widget architecture in detail and then show examples of how it can be used in exploratory work. The talk will be of interest to developers interested in using the architecture to build widgets and for working data scientists who could use those widgets. I will also briefly mention how other backend languages, such as Julia and R can leverage this same architecture.\n", "\nStrata Program Chairs, Alistair Croll and Roger Magoulas, welcome you to the second day of keynotes.\n", "\nUsing big data effectively almost always involves large amounts of cleaning and processing.  Proper categorization and attribute labels are essential.  In many cases some of the steps can only be done manually making crowdsourcing a crucial tool for data scientists.\nThis talk will describe micotasking, where it fits in the crowdsourcing landscape, and how data scientists and developers can most effectively tap into the crowd to collect and process their data sets. Several real world cases will be used to illustrate the possibilities, including tweet analysis, social profile mining and pre-processing satellite imagery for big data queries. In this talk I will also take a stab at predicting where the state of the art will be a year from now.\n\nCollecting large data sets using the crowd\nAugmenting, labeling and categorizing using microtasking\nConducting big data experiments using the crowd \nTraining machine learning models using results from the crowd\nReal-world examples of tweet and sentiment analysis, satellite imagery, and social profile mining\n\n", "\nStatistical machine learning techniques tend to fail when faced with an adaptive adversary attempting to evade detection in the data.  Humans do an excellent job of correctly spotting adaptive adversaries given a good way to digest the data.  On the other hand, humans are glacially slow and error-prone when it comes to moving through very large volumes of data, a task best left to the machines.\nFighting complex fraud and cyber-security threats requires a symbiosis between the computers and teams of human analysts.  The computers use algorithmic analysis, heuristics, and/or statistical characterization to find interesting \u2018simple\u2019 patterns in the data.  These candidate events are then queued for in-depth human analysis in rich, expressive, interactive analysis environments.\nIn this talk, we\u2019ll take a look at case studies of three different systems, using a partnership of automation and human analysis on large scale data to find the clandestine human behavior that these datasets hold, including a discussion of the backend systems architecture and a demo of the interactive analysis environment.\nThe backend systems architecture is a mix of open source technologies, like Cassandra, Lucene, and Hadoop, and some new components that bind them all together.\nThe interactive analysis environment allows seamless pivoting between semantic, geospatial, and temporal analysis with a powerful GUI interface that\u2019s usable by non-data scientists.\nThe systems are real systems currently in use by commercial banks, pharmaceutical companies, and governments.\n", "\nAs the realm of data science expands we are examining both more data and more diverse types of data to give us insights. This double whammy of data volume and data diversity has led many to explore new data storage and querying techniques. CitusDB helps solve this problem by enabling you to use PostgreSQL\u2019s rich ecosystem to build a highly scalable data platform that supports all your data.\nPostgreSQL allows users to easily build and share extensions that can enable new data types, new SQL operators, and new storage formats. Since new extension mechanisms became available in 2011 PostgreSQL has been augmented for a large and variable number of purposes. We\u2019ll look at three different ways PostgreSQL has been extended and discuss how CitusDB leverages these referencing real world examples:\n\nStoring and querying semi-structured data using new data types such as hstore \nCustom operations such as distinct approximations to allow faster and more scalable queries \nUse of new storage techniques such as columnar stores to allow fast querying of massive datasets\n\nCitusDB integrates with PostgreSQL to bring you benefits of performance and scalability while retaining the reliability, functionality, and extensibility of PostgreSQL. We\u2019ll look at how this integration happens and how CitusDB can help you build a scalable data platform using commodity hardware.\nThis session is sponsored by Citrus Data\n", "\nSmart meters may be the most visible element of the so-called smart grid, but how smart can the grid be if the plants producing the energy are dumb?\nTo ensure the integrity of the grid, every stage of our electrical power infrastructure \u2013 including generation, transmission and distribution \u2013 has to get \u201dsmart.\u201d  With power consumption growing an average 3-5% per year globally, the average age of a power transformer in the US is 30-40 years, and many are facing end-of-life issues.\nSophisticated sensors connected to software platforms that continuously gather, visualize and analyze data in real time to produce actionable insights are critical to preserving our aging energy assets and keeping the electricity flowing.\nThe challenge is making sense of the continuous flood of data these online sensors generate. For example, a single infrared camera produces 76,400 pixels of data at a rate of 60 frames/second \u2013 300 megabytes of data every second!\nThis presentation will explore the following issues:\n\u2022 How can utilities get the information needed to continuously monitor the health of transformers and other energy infrastructure assets in order to keep the grid running strong?\n\u2022 How can power companies manage the influx of data in order to interpret information quickly and separate the noise from the necessary?\n\u2022 How do operators get critical information back to the people and systems to control it \u2026 without stressing and breaking the existing communications infrastructure?\n\u2022 How do you store the flood of data so it can be extracted, analyzed and acted upon?\n\u2022 Given cyber security challenges, how do you keep all this data secure?\n\u2022 How can data flows be integrated into controls and be \u201cautomated?\u201d\n\u2022 What are the barriers to adopting new M2M technologies for the energy grid and how can they be overcome?\n", "\nIn the world of ever growing data volumes, how do you extract insight, trends and meaning from all that data in Hadoop? Getting relevant information in seconds (instead of hours or days) from big data requires a different approach. \nJoin us to learn about how to reveal insights in your Big data and redefine how your organization solves complex problems.\n\n\nThis session will showcase how to:\n\t\nreveal insights in your big data and redefine how your organization solves complex problems. \nprepare, explore and model multiple scenarios using in-memory analytics, \nmodel tasks interactively and in real time, \nask what-if questions on all the data, \ninstantly add or drop variables into a model and see their influence,\nassess predictive power, \nunderstand your model fit with model diagnostics on the fly\nuse a scalable recommendation system to help improve customer experience \nand more\u2026\n\nThis session is sponsored by SAS\n", "\nRamona, Pierson, CEO, Declara\n", "\nMobile devices, sensors and GPS are driving the demand to handle big data in both batch and real time. This presentation discusses how we used complex event processing (CEP) and MapReduce based technologies to track and process data from a soccer match as part of the annual DEBS event processing challenge. In 2013, the challenge included a data set generated by a real soccer match in which sensors were placed in the soccer ball and players\u2019 shoes. This session will review how we used CEP to implement DESB challenge and achieved throughput in excess of 100,000 events/sec. It also will examine how we extended the solution to conduct batch processing using business activity monitoring (BAM) using the same framework, enabling users to obtain both instant analytics as well as more detailed batch processing based results.\nThis session is sponsored by WSO2\n", "\nCombine your best algorithms and smartest data architecture, and what do you get? Without humans, you have an expensive, high tech brick.  Humans generate data, which is used by and for humans to achieve human goals. If you want your data department to earn its keep by showing real value, you must build your social systems as meticulously as you build your pipeline.\nAs \u201cbig data\u201d prepares to enter the \u201ctrough of disillusionment\u201d in the Gartner hype curve, we all shoulder the burden of driving the industry forward and delivering on the promise of data, now. In this session, you will learn how to:\n\nHire for a balanced team of different types of data professionals who you can actually find.\nTrain data team members and data consumers in analytic rigor and cognitive biases.\nBuild a new organizational decision-making lifecycle, from goals, to business questions, to data analysis, to interpretation, to business strategy.\nLead organizations to identify and focus on metrics that truly make a difference.\nCreate data products that drive value for end-users and whole organizations.\n\n", "\nAlthough the Department of Defense is no stranger to the problem of data analytics at scale, it is encountering many of the same fundamental challenges that businesses face, given the modern explosion of data acquisition, storage, and processing technologies.\nXDATA is a $25 million/year DARPA program that seeks to develop computational techniques and software tools for analyzing large volumes of data, both semi-structured (e.g., tabular, relational, categorical) and unstructured (e.g., text documents, message traffic).  The central challenges we are addressing include developing scalable algorithms for processing imperfect data in distributed data stores, and creating effective human-computer interaction tools for facilitating rapidly customizable visual reasoning for diverse missions.\nNovel aspects of the XDATA program include the embracing of open-source technologies, which is relatively rare in the Defense sector, and a focus on minimizing design-to-testing time of new software by using near continual feedback from users.\nIn this talk, the Program Manager for XDATA will talk about the origins of the program, including personal experiences in Afghanistan which led to a recognition within the Department of Defense that a broader, coherent strategy was needed for tackling the computational challenges of large, heterogeneous datasets.  The discussion will then introduce some of the libraries and teams comprising the XDATA effort, and highlight the innovations being developed, ranging from novel machine learning and graph algorithms, to fundamental improvements in data processing and distributed computation, to many powerful libraries and techniques for visualization of large data.\nThe multi-year program is still in its first year, and participating teams have just completed an intensive summer workshop where they focused on solving data science \u201cchallenge problems\u201d on several datasets of interest to both businesses and defense.  The talk will also showcase some interesting results from these explorations.\nWe hope to engage with the Strata audience to identify opportunities and best practices that will enable cultural shifts in government, business, and non-profit organizations to occur, and then to facilitate the transition to a new community around Department of Defense problems that is inherently open and collaborative.\n", "\nThe United States Patent and Trademark Office wanted a simple, lightweight, yet modern and rich discovery interface for Chinese patent data.  This is the story of the Global Patent Search Network, the next generation multilingual search platform for the USPTO.  GPSN, http://gpsn.uspto.gov, was the first public application deployed in the cloud, and allowed a very small development team to build a discovery interface across millions of patents.\nThis case study will cover:\n\u2022    How we leveraged Amazon Web Services platform for data ingestion, auto scaling, and deployment at a very low price compared to traditional data centers.\n\u2022    We will cover some of the innovative methods for converting XML formatted data to usable information.\n\u2022    Parsing through 5 TB of raw TIFF image data and converting them to modern web friendly format.\n\u2022    Challenges in building a modern Single Page Application that provides a dynamic, rich user experience.\n\u2022    How we built \u201cdata sharing\u201d features into the application to allow third party systems to build additional functionality on top of GPSN.\n", "\nVisualization is a weak link in big data tools: shoving 1MM rows into standard charts breaks their visual design and kills interactivity. In our mission to scale charts, we built the Superconductor language. It automatically compiles declarative visualizations into GPU code (WebCL+WebGL). This talk will explore how we\u2019re redesigning and optimizing core charts like heat maps and line graphs.\nThe Superconductor compiler can be downloaded at http://www.sc-lang.com and we\u2019re launching http://www.graphistry.com to provide accelerated charts built on top of it.\n", "\nDetails to come\u2026\n", "\nSpreadsheets are used extensively in industry: they are the number one tool for financial analysis and are also prevalent in other domains, such as logistics and planning. Their flexibility and immediate feedback make them easy to use for non-programmers. But they are as easy to build, as they are difficult to analyze, maintain and check. Felienne\u2019s research aims at developing methods to support spreadsheet users to understand, update and improve spreadsheets. Inspiration was taken from classic software engineering, as this field is specialized in the analysis of data and calculations. In this talk Felienne will summarize her recently completed PhD research on the topic of spreadsheet structure visualization, spreadsheet smells and clone detection, as well as presenting a sneak peek into the future of spreadsheet research as Delft University.\n", "\nRodney Mullen, professional skateboarder, company owner, and inventor.\n", "\nThis session will address the exciting possibilities of bringing dramatic improvements in various industry verticals using big data analytics especially real-time analytics over high-volume data in motion. \nThis talk will also showcase a real-time streaming analytics platform from Impetus that enables high-volume, high-speed data ingest and supports rapid deployment of predictive models over real-time data including PMML compatible analytical models exported from standard tools like R and SAS.\nThis session is sponsored by Impetus Technologies\n", "\nGraph analytics have applications beyond large web scale organizations. Many computing problems can be efficiently expressed and processed as a graph and can lead to useful insights that drive product and business decisions\nWhile you can express graph algorithms as SQL queries in Hive or Hadoop MapReduce programs, an API designed specifically for graph processing makes writing many iterative graph computations (such as page rank, connected components, label propagation, graph-based clustering, etc.) easy to express in simpler and easier to understand code.   Apache Giraph provides such a native graph processing API, runs on existing Hadoop infrastructure and can directly access HDFS and/or Hive tables.\nThis talk describes our efforts at Facebook to scale Apache Giraph to very large graphs of up to one trillion edges and how we run Apache Giraph in production.  We will also talk about several algorithms that we have implemented and their use cases.\n", "\nIn February 2013, there will be as many mobile phones as people on planet earth. Today, more people have access to mobiles than water or electricity. Many companies are racing to reach these \u201cnext billions\u201d whether it is Facebook (zero) and internet.org or Google and Loon or others.\nWe have been working to reach the same group over the last 5 years via increasingly ubiquitous \u201cdumb\u201d or simple handset phones and have learned much about user experience and partnership building. Which days of the week are best for polling?  Which times of day?  How much do phone penetration and literacy matter?  How about multiple languages?  How much does offering an incentive help? Who is best to partner with as we scale?\nWe\u2019ve also learned from our stumbles.  How do we make sure that messages are not lost in transmission?  And when does cloud work and when does it not?\n", "\nBirds of a Feather (BoF) sessions provide networking opportunities for attendees interested in the same projects and topics to meet in a casual setting. BoFs topics are created by attendees and can be about individual projects or broader topics (best practices, open data, standards).\nBoFs at Strata will happen during lunch on Wednesday, February 12 and Thursday, February 13, where lunch is served.\nStop by the BoF signup board near Registration to check out the topics or start your own.\n", "\nAgile development is a methodology based on short work cycles called sprints.  At the core of this practice is the fundamental principle of collaboration between cross-functional teams.  It promotes adaptive planning, quick feedback loops, and a rapid response to change.  Since its inception in early 2001, it has significantly changed the core philosophy of software engineering and product management.  In this talk, we\u2019ll describe how Alpine and its customers are applying the same philosophy in the data science discipline through collaboration, code-free, and a simple approach to creating powerful advanced analytic workflows on big data.\nThis session is sponsored by Alpine Data Labs\n", "\nBirds of a Feather (BoF) sessions provide networking opportunities for attendees interested in the same projects and topics to meet in a casual setting. BoFs topics are created by attendees and can be about individual projects or broader topics (best practices, open data, standards).\nBoFs at Strata will happen during lunch on Wednesday, February 12 and Thursday, February 13, where lunch is served.\nStop by the BoF signup board near Registration to check out the topics or start your own.\n", "\nMany companies have adopted a strategy to move most of their data processing onto Hadoop to solve for storage and processing at scale. Technologies, such as Impala, raises the bar for query performance onto a large scale platform. But still, there is a wide gap between having the infrastructure to cope with various types and large volumes of data and actually obtain insight in a timely and intuitive manner.\nAttending this session will provide two things:\n1. Insight into how others have gotten new insights across previously siloed data sources \u2013 based on real world examples and real world data!\n2. Ideas on how you can easily visualize various types of data and bridge the gap from storage and processing to insight.\nWe will show how using so called \u201cmicro-aggregate delegation\u201d enables users to see results immediately, achieving instantaneous analysis of arbitrarily large amounts of raw data. We will also explain the \u201cDeath Star Join\u201d approach that allows for joining of micro-aggregate streams from disparate Hadoop, NoSQL, and legacy sources together \u2013 while they are in-flight!\nLast but not least, we will demonstrate how touch and gesture comes in as intuitive ways of changing what you see on the spot, and how these capabilities help modify your view with your thought process \u2013 a natural way of understanding data!\n", "\nApache Accumulo is a sorted, distributed key/value store that is largely based on Google\u2019s Bigtable design. Accumulo includes some novel improvements on Bigtable, such as cell-level access controls and a server-side programming mechanism (the Iterator Framework) that can modify key/value pairs at various points in the data management process.\nCell-level security controls enable organizations to apply fine-grained access controls to each key/value pair in Accumulo.  This capability was originally developed by the NSA to enable integration of datasets with different levels of security classification.  More recently, Sqrrl has worked with customers in healthcare, telecommunications, and financial services companies to leverage cell-level security for more effective Big Data information integration and regulatory compliance in those industries.\nIn this talk, Adam Fuchs, the CTO of Sqrrl and co-founder of the Accumulo project will discuss some of the lessons learned for properly architecting, applying, and managing cell-level security labels in customer environments.\n", "\nFreebase is a database that holds over a billion facts on more than 40 million real-world-entities. Working with these 88 GB of uncompressed data brings its own challenges. To explore this dataset, we\u2019ll have on stage representatives of 3 Google teams: Freebase, Big Data, and Maps.\nThe gender gap is a big social issue that needs to be quantified. When looking at Freebase data, women notable enough to be in this database seem underrepresented compared to men. Digging deeper, we can see differences between geographic locations, professions, and ages.\nWe\u2019ll look into the technical how-to: You\u2019ll learn what the freebase entities represent. We\u2019ll show you how we were able to query this massive dataset in seconds. And how to explore all these insights in an interactive map.\n", "\nThe measure of success for a data scientist is not number of insights, but impact on co-workers\u2019 behavior. Moving from insight to action requires an art underutilized by the data science community: storytelling.\nI will cover techniques including the Fogg model, loss aversion, and minimum viable stories, using examples of my failures and successes in driving behavioral change with data.\n", "\nD3 (data-driven documents) is one of the most acclaimed data-visualization libraries and has gained considerable adoption in its short lifespan. It is a very versatile framework, which can be used to create simple charts as well as sophisticated representations like network graphs or geographic projections, and which handles interactivity and animation well. But because of its unique features it also uses a specific syntax and concepts.\nThis tutorial will allow attendees to master the essential ideas of D3 and become fully operational.\nThe outline of the tutorial is:\n- Introduction\n- Fundamental notions; the document model, SVG;\n- Setting up D3\n- Data\n- Selections \u2013 how to map data to visual elements;\n- Setting attributes and style \u2013 controlling the appearance of the output;\n- Scales \u2013 one essential helper function in D3;\n- Transitions and interactivity;\n- The concept of data joins;\n- Conclusion \u2013 what can be achieved with D3.\n", "\nThere are three main time sinks in any data science task:\n1. Figuring out what you want to do.\n2. Turning a vague goal into a precise set of tasks (i.e. programming).\n3. Actually crunching the numbers.\nA well-design domain specific language (or DSL) tightly coupled to the problem domain can make all three pieces faster. In this talk, I\u2019ll discuss two DSLs built in R: ggvis for visualisation and dplyr for data manipulation. These build on my previous packages ggplot2 and plyr, improving both expressivity and speed.\nData visualisation and manipulation are key parts of the data science process. \nggvis makes it easy to declaratively describe interactive web graphics. It combines a declarative syntax based on ggplot2 with shiny\u2019s reactive programming model and vega\u2019s declarative JS rendering system. dplyr implements the most important verbs of data manipulation in a datastore-agnostic fashion, so you can think about and compute with your data in the same way regarldess of whether you\u2019re working with a local in-memory data frame or a remote on-disk database.\n", "\nYesterday\u2019s technology can\u2019t handle today\u2019s big data. This is\ntrue not only of our storage and computing tools, but also of\nour visualization tools. Our visual bandwidth is simply not\nenough to take in today\u2019s high-dimensional big data.\nWe present two new approaches for leveraging additional\nhuman senses to make sense of big data: data music videos\nand data gastronomification. These multisensory approaches \nradically change the data analysis approach; they encourage\nimmersive and ambient data consumption, allowing us to ask\ndrastically different questions of our data.\nWe have developed some open-source tools for building and\nscaling systems for realtime data analysis with data music\nvideos and data gastronomification. We\u2019ll discuss the theory\nbehind these two data analysis methods, and then we\u2019ll present\ncase studies on how our tools are used to enable business\nanalytics and instill a data-driven culture.\n", "\nThe wonderful thing about big data vendors is that there\u2019s so many of them. Not just the traditional big software players, but a vibrant ecosystem of startups offering innovative data solutions.\nUnfortunately, the terrible thing about big data vendors is that there\u2019s so many of them. Anybody implementing data solutions has to navigate this landscape. Who will you choose, and how will your choice affect your future options?\nIn this talk we\u2019ll present a roadmap to the big data vendor landscape. We\u2019ll touch on topics including:\n\nWhy having a big data platform is important\nUnderstand the business plans of the startups and how likely they are to be around next year\nHow to choose between building and buying in an open source world\nThe tradeoff between quick results and limiting future options\n\nAlong the way we\u2019ll bring in real world examples from experience creating big data solutions, and be prepared to answer specific questions from the audience about buying decisions.\n", "\nJoin us at the Opening Reception for a drink or two and to network with other attendees and visit our companies innovating in the data space. This event is open to all sponsors, exhibitors and attendees.\n\n", "\nBooth Crawl is back at Strata, immediately after the afternoon sessions on Wednesday. \nQuench your thirst with vendor-hosted libations and snacks while you check out all the cool stuff in the Expo Hall. It\u2019s also a great time to meet and mingle with fellow attendees and Strata speakers and authors.\n", "\nThe Industrial Internet is all about optimizing an industry\u2019s assets and operations.  Advanced analytics, data, and software enable our devices to connect and be better understood in new ways.  At GE, we are developing the next generation of software and analytic capabilities to enable our industries to benefit from the Industrial Internet.  In this presentation, I will introduce the concepts and technologies behind the Industrial Internet, describe how Big Data is a critical component, survey how industry is approaching Big Data, and describe several of our existing efforts to research and develop technologies for the Industrial Internet utilizing Big Data.   To fully realize the value out of massive industrial data, we need to think about the ecosystem of big distributed storage, compute, and knowledge to achieve re-usable, repeatable, and intelligent Big Data products.\n", "\nProbabilistic programming languages are systems in which stochastic modeling primitives are \u201cfirst class\u201d citizens.  At a minimum, these environments provide a clean separation of probabilistic modeling, which captures our assumptions about the world and the hypothesis space, and inference, i.e. learning which hypotheses provide the best explanation of the observed data.  Though still a subject of intense academic research, these systems have recently emerged in various open source projects and are now available for widespread evaluation and use.\nThis talk will provide a high-level introduction to the state of the probabilistic programming field, and give a feel for what it\u2019s like to use these environments.\nQuestions that will be answered include:\n\nWhat are the advantages of probabilistic programming over other approaches?\n\n\nHow should one choose from the available probabilistic programming systems? What are the relative strengths and weaknesses of the various options?\n\n\nWhat do probabilistic programs \u201clook like\u201d? How are these programs different from more traditional solutions?\n\n\nAre these systems ready for scaled production use? What further developments are most needed to accelerate their adoption in industry?\n\n", "\nDavid Epstein, Senior Writer, Sports Illustrated; author, The Sports Gene: Inside the Science of Extraordinary Athletic Performance\n", "\nDavid Epstein, Senior Writer, Sports Illustrated; author, The Sports Gene: Inside the Science of Extraordinary Athletic Performance\n", "\nWhen failure becomes invisible, the difference between failure and success may also become invisible.\nWe each want to dissect and apply the lessons gained from the life stories of diet gurus, celebrity CEOs, and superstar athletes. We\u2019d all like to deconstruct success and reconstruct it in our own lives, but looking to the successful for clues about how to better live your life is, at best, an incomplete strategy and, at worst, a giant waste of time.\nDavid McRaney will tell the story of how the Department of War Math in World War II helped bring to light the psychology of how we miss what is important when it comes to failure, and how the modern understanding of the psychology of luck provides the best game plan for getting the best out of life.\n", "\nImplementing and consuming Machine Learning techniques at scale are difficult tasks for ML Developers and End Users. MLbase (www.mlbase.org) is an open-source platform under active development addressing the issues of both groups. MLbase consists of three components\u2014MLlib, MLI and ML Optimizer. MLlib is a low-level distributed ML library written against the Spark, MLI is an API / platform for feature extraction and algorithm development that introduces high-level ML programming abstractions, and ML Optimizer is a layer aiming to simplify ML problems for End Users by automating the tasks of feature and model selection. In this talk we will describe the high-level functionality of each of these layers, and demonstrate its scalability and ease-of-use via real-world examples involving classification, regression, clustering and collaborative filtering.\n", "\nThis tutorial is geared around understanding the basics of how Apache Cassandra stores and access time series data. We\u2019ll start with an overview of how Cassandra works and how that can be a perfect fit for time series. There will be coding as a part of the hands-on tutorial. The goal will be to take a example application and code through the different aspects of working with this unique data pattern. The final section will cover many of the projects in and out of the Cassandra ecosystem that can be leveraged for time series collection and retrieval. Attendees should leave with a comprehensive view of the many options to make considered choices in their time series projects.\nHour 1: Core concepts\n\nIntroduction to Apache Cassandra\nWhy Cassandra is used for storing time series data\nData models for time series\nDeployment models for collection\n\nHour 2: Hands on session with example application \n\nBuilding the right data model\nCoding the right way for your application\nDeploying and performance tuning\n\nHour 3: Using pre-built tools and projects. \n\nIngesting data with Apache Flume\nGraphing data with open-source visualization tools\nWorking with KairosDB, an OpenTSDB clone on Apache Cassandra\n\n", "\nMany companies collect data, but what value does this information truly generate? Businesses are often reluctant to invest in new technologies, due to a fear of the unknown or hearing horror stories of not getting the expected ROI from Big Data implementations. This talk analyzes the challenges faced by cloud service providers and illustrates how traditional warehouses and Hadoop work hand-in-hand to produce much-needed business value. Data driven decision making starts from understanding business problems and applying the most appropriate technology stack to solve them. In this session, we will share our success story of marrying the world of traditional warehouses and new generation big data technologies to get the best of both worlds by creating win-win experiences. We will focus on the transformational story of metric-driven decision making at Rackspace.\nThis session is sponsored by Rackspace\n"]}